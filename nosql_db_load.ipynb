{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# from pymongo import MongoClient\n",
    "# import json\n",
    "\n",
    "# # Connect to local MongoDB\n",
    "# client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# # Choose the database and collection\n",
    "# db = client[\"myDatabase\"]\n",
    "# collection = db[\"myCollection\"]\n",
    "\n",
    "# # Load JSON data from file\n",
    "# with open(\"D:/Code/RAG/pytorch-rag/data.json\", \"r\") as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Insert into collection\n",
    "# if isinstance(data, list):\n",
    "#     collection.insert_many(data)\n",
    "# else:\n",
    "#     collection.insert_one(data)\n",
    "\n",
    "# print(\"Data inserted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "# Connect to local MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"myDatabase\"]\n",
    "collection = db[\"myCollection\"]\n",
    "\n",
    "# Clear the collection if you want a fresh insert\n",
    "collection.delete_many({})\n",
    "\n",
    "# Load JSON data from file\n",
    "with open(\"doc_info.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Insert documents into MongoDB\n",
    "if isinstance(data, dict) and \"documents\" in data:\n",
    "    collection.insert_many(data[\"documents\"])\n",
    "    print(\"Documents inserted successfully!\")\n",
    "else:\n",
    "    print(\"Unexpected JSON structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('67f0968102ab5481fe017980'), 'title': 'Buffer', 'page_text': 'Buffer [LINK_1]  class torch.nn.parameter.Buffer( data=None , * , persistent=True ) [source]  [source]  [LINK_2]  A kind of Tensor that should not be considered a model\\nparameter. For example, BatchNorm’s running_mean is not a parameter, but is part of the module’s state.  Buffers are Tensor subclasses, that have a\\nvery special property when used with Module s – when they’re\\nassigned as Module attributes they are automatically added to the list of\\nits buffers, and will appear e.g. in buffers() iterator.\\nAssigning a Tensor doesn’t have such effect. One can still assign a Tensor as explicitly by using\\nthe register_buffer() function.  Parameters  data ( Tensor ) – buffer tensor.  persistent ( bool  ,  optional ) – whether the buffer is part of the module’s state_dict . Default: True\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.Buffer.html#buffer\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.Buffer.html#torch.nn.parameter.Buffer', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.parameter.Buffer.html#buffer', 'https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html#Buffer', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parameter.py#L225', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Buffer.html#torch.nn.parameter.Buffer', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html', 'https://pytorch.org/docs/stable/nn.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017981'), 'title': 'Parameter', 'page_text': 'Parameter [LINK_1]  class torch.nn.parameter.Parameter( data=None , requires_grad=True ) [source]  [source]  [LINK_2]  A kind of Tensor that is to be considered a module parameter.  Parameters are Tensor subclasses, that have a\\nvery special property when used with Module s - when they’re\\nassigned as Module attributes they are automatically added to the list of\\nits parameters, and will appear e.g. in parameters() iterator.\\nAssigning a Tensor doesn’t have such effect. This is because one might\\nwant to cache some temporary state, like last hidden state of the RNN, in\\nthe model. If there was no such class as Parameter , these\\ntemporaries would get registered too.  Parameters  data ( Tensor ) – parameter tensor.  requires_grad ( bool  ,  optional ) – if the parameter requires gradient. Note that\\nthe torch.no_grad() context does NOT affect the default behavior of\\nParameter creation–the Parameter will still have requires_grad=True in no_grad mode. See Locally disabling gradient computation for more\\ndetails. Default: True\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter', 'https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html#Parameter', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parameter.py#L19', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Buffer.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017982'), 'title': 'UninitializedParameter', 'page_text': 'UninitializedParameter [LINK_1]  class torch.nn.parameter.UninitializedParameter( requires_grad=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A parameter that is not initialized.  Uninitialized Parameters are a special case of torch.nn.Parameter where the shape of the data is still unknown.  Unlike a torch.nn.Parameter , uninitialized parameters\\nhold no data and attempting to access some properties, like their shape,\\nwill throw a runtime error. The only operations that can be performed on a uninitialized\\nparameter are changing its datatype, moving it to a different device and\\nconverting it to a regular torch.nn.Parameter .  The default device or dtype to use when the parameter is materialized can be set\\nduring construction using e.g. device=\\'cuda\\' .  cls_to_become [source]  [LINK_3]  alias of Parameter\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#uninitializedparameter\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#uninitializedparameter', 'https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html#UninitializedParameter', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parameter.py#L181', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parameter.py#L19', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017983'), 'title': 'UninitializedBuffer', 'page_text': 'UninitializedBuffer [LINK_1]  class torch.nn.parameter.UninitializedBuffer( requires_grad=False , device=None , dtype=None , persistent=True ) [source]  [source]  [LINK_2]  A buffer that is not initialized.  Uninitialized Buffer is a a special case of torch.Tensor where the shape of the data is still unknown.  Unlike a torch.Tensor , uninitialized parameters\\nhold no data and attempting to access some properties, like their shape,\\nwill throw a runtime error. The only operations that can be performed on a uninitialized\\nparameter are changing its datatype, moving it to a different device and\\nconverting it to a regular torch.Tensor .  The default device or dtype to use when the buffer is materialized can be set\\nduring construction using e.g. device=\\'cuda\\' .\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html#uninitializedbuffer\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html#uninitializedbuffer', 'https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html#UninitializedBuffer', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parameter.py#L254', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017984'), 'title': 'Module', 'page_text': 'Module [LINK_1]  class torch.nn.Module( *args , **kwargs ) [source]  [source]  [LINK_2]  Base class for all neural network modules.  Your models should also subclass this class.  Modules can also contain other Modules, allowing them to be nested in\\na tree structure. You can assign the submodules as regular attributes:  importtorch.nnasnnimporttorch.nn.functionalasFclassModel(nn.Module):def__init__(self)->None:super().__init__()self.conv1=nn.Conv2d(1,20,5)self.conv2=nn.Conv2d(20,20,5)defforward(self,x):x=F.relu(self.conv1(x))returnF.relu(self.conv2(x))  Submodules assigned in this way will be registered, and will also have their\\nparameters converted when you call to() , etc.  Note  As per the example above, an __init__() call to the parent class\\nmust be made before assignment on the child.  Variables  training ( bool ) – Boolean represents whether this module is in training or\\nevaluation mode.  add_module( name , module ) [source]  [source]  [LINK_3]  Add a child module to the current module.  The module can be accessed as an attribute using the given name.  Parameters  name ( str ) – name of the child module. The child module can be\\naccessed from this module using the given name  module ( Module ) – child module to be added to the module.  apply( fn ) [source]  [source]  [LINK_4]  Apply fn recursively to every submodule (as returned by .children() ) as well as self.  Typical use includes initializing the parameters of a model\\n(see also torch.nn.init ).  Parameters  fn ( Module -> None) – function to be applied to each submodule  Returns  self  Return type  Module  Example:  >>>@torch.no_grad()>>>definit_weights(m):>>>print(m)>>>iftype(m)==nn.Linear:>>>m.weight.fill_(1.0)>>>print(m.weight)>>>net=nn.Sequential(nn.Linear(2,2),nn.Linear(2,2))>>>net.apply(init_weights)Linear(in_features=2, out_features=2, bias=True)Parameter containing:tensor([[1., 1.],[1., 1.]], requires_grad=True)Linear(in_features=2, out_features=2, bias=True)Parameter containing:tensor([[1., 1.],[1., 1.]], requires_grad=True)Sequential((0): Linear(in_features=2, out_features=2, bias=True)(1): Linear(in_features=2, out_features=2, bias=True))  bfloat16() [source]  [source]  [LINK_5]  Casts all floating point parameters and buffers to bfloat16 datatype.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  buffers( recurse=True ) [source]  [source]  [LINK_6]  Return an iterator over module buffers.  Parameters  recurse ( bool ) – if True, then yields buffers of this module\\nand all submodules. Otherwise, yields only buffers that\\nare direct members of this module.  Yields  torch.Tensor – module buffer  Return type  Iterator [ Tensor ]  Example:  >>>forbufinmodel.buffers():>>>print(type(buf),buf.size())<class \\'torch.Tensor\\'> (20L,)<class \\'torch.Tensor\\'> (20L, 1L, 5L, 5L)  children() [source]  [source]  [LINK_7]  Return an iterator over immediate children modules.  Yields  Module – a child module  Return type  Iterator [ Module ]  compile( *args , **kwargs ) [source]  [source]  [LINK_8]  Compile this Module’s forward using torch.compile() .  This Module’s __call__ method is compiled and all arguments are passed as-is\\nto torch.compile() .  See torch.compile() for details on the arguments for this function.  cpu() [source]  [source]  [LINK_9]  Move all model parameters and buffers to the CPU.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  cuda( device=None ) [source]  [source]  [LINK_10]  Move all model parameters and buffers to the GPU.  This also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.  Note  This method modifies the module in-place.  Parameters  device ( int  ,  optional ) – if specified, all parameters will be\\ncopied to that device  Returns  self  Return type  Module  double() [source]  [source]  [LINK_11]  Casts all floating point parameters and buffers to double datatype.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  eval() [source]  [source]  [LINK_12]  Set the module in evaluation mode.  This has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g. Dropout , BatchNorm ,\\netc.  This is equivalent with self.train(False) .  See Locally disabling gradient computation for a comparison between .eval() and several similar mechanisms that may be confused with it.  Returns  self  Return type  Module  extra_repr() [source]  [source]  [LINK_13]  Return the extra representation of the module.  To print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.  Return type  str  float() [source]  [source]  [LINK_14]  Casts all floating point parameters and buffers to float datatype.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  forward( *input ) [source]  [LINK_15]  Define the computation performed at every call.  Should be overridden by all subclasses.  Note  Although the recipe for forward pass needs to be defined within\\nthis function, one should call the Module instance afterwards\\ninstead of this since the former takes care of running the\\nregistered hooks while the latter silently ignores them.  get_buffer( target ) [source]  [source]  [LINK_16]  Return the buffer given by target if it exists, otherwise throw an error.  See the docstring for get_submodule for a more detailed\\nexplanation of this method’s functionality as well as how to\\ncorrectly specify target .  Parameters  target ( str ) – The fully-qualified string name of the buffer\\nto look for. (See get_submodule for how to specify a\\nfully-qualified string.)  Returns  The buffer referenced by target  Return type  torch.Tensor  Raises  AttributeError – If the target string references an invalid\\n    path or resolves to something that is not a\\n    buffer  get_extra_state() [source]  [source]  [LINK_17]  Return any extra state to include in the module’s state_dict.  Implement this and a corresponding set_extra_state() for your module\\nif you need to store extra state. This function is called when building the\\nmodule’s state_dict() .  Note that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.  Returns  Any extra state to store in the module’s state_dict  Return type  object  get_parameter( target ) [source]  [source]  [LINK_18]  Return the parameter given by target if it exists, otherwise throw an error.  See the docstring for get_submodule for a more detailed\\nexplanation of this method’s functionality as well as how to\\ncorrectly specify target .  Parameters  target ( str ) – The fully-qualified string name of the Parameter\\nto look for. (See get_submodule for how to specify a\\nfully-qualified string.)  Returns  The Parameter referenced by target  Return type  torch.nn.Parameter  Raises  AttributeError – If the target string references an invalid\\n    path or resolves to something that is not an nn.Parameter  get_submodule( target ) [source]  [source]  [LINK_19]  Return the submodule given by target if it exists, otherwise throw an error.  For example, let’s say you have an nn.Module  A that\\nlooks like this:  A(\\n    (net_b): Module(\\n        (net_c): Module(\\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n        )\\n        (linear): Linear(in_features=100, out_features=200, bias=True)\\n    )\\n)  (The diagram shows an nn.Module  A . A which has a nested\\nsubmodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .)  To check whether or not we have the linear submodule, we\\nwould call get_submodule(\"net_b.linear\") . To check whether\\nwe have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") .  The runtime of get_submodule is bounded by the degree\\nof module nesting in target . A query against named_modules achieves the same result, but it is O(N) in\\nthe number of transitive modules. So, for a simple check to see\\nif some submodule exists, get_submodule should always be\\nused.  Parameters  target ( str ) – The fully-qualified string name of the submodule\\nto look for. (See above example for how to specify a\\nfully-qualified string.)  Returns  The submodule referenced by target  Return type  torch.nn.Module  Raises  AttributeError – If the target string references an invalid\\n    path or resolves to something that is not an nn.Module  half() [source]  [source]  [LINK_20]  Casts all floating point parameters and buffers to half datatype.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  ipu( device=None ) [source]  [source]  [LINK_21]  Move all model parameters and buffers to the IPU.  This also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.  Note  This method modifies the module in-place.  Parameters  device ( int  ,  optional ) – if specified, all parameters will be\\ncopied to that device  Returns  self  Return type  Module  load_state_dict( state_dict , strict=True , assign=False ) [source]  [source]  [LINK_22]  Copy parameters and buffers from state_dict into this module and its descendants.  If strict is True , then\\nthe keys of state_dict must exactly match the keys returned\\nby this module’s state_dict() function.  Warning  If assign is True the optimizer must be created after\\nthe call to load_state_dict unless get_swap_module_params_on_conversion() is True .  Parameters  state_dict ( dict ) – a dict containing parameters and\\npersistent buffers.  strict ( bool  ,  optional ) – whether to strictly enforce that the keys\\nin state_dict match the keys returned by this module’s state_dict() function. Default: True  assign ( bool  ,  optional ) – When set to False , the properties of the tensors\\nin the current module are preserved whereas setting it to True preserves\\nproperties of the Tensors in the state dict. The only\\nexception is the requires_grad field of Default:``False`  Returns  missing_keys is a list of str containing any keys that are expected  by this module but missing from the provided state_dict .  unexpected_keys is a list of str containing the keys that are not  expected by this module but present in the provided state_dict .  Return type  NamedTuple with missing_keys and unexpected_keys fields  Note  If a parameter or buffer is registered as None and its corresponding key\\nexists in state_dict , load_state_dict() will raise a RuntimeError .  modules() [source]  [source]  [LINK_23]  Return an iterator over all modules in the network.  Yields  Module – a module in the network  Return type  Iterator [ Module ]  Note  Duplicate modules are returned only once. In the following\\nexample, l will be returned only once.  Example:  >>>l=nn.Linear(2,2)>>>net=nn.Sequential(l,l)>>>foridx,minenumerate(net.modules()):...print(idx,\\'->\\',m)0 -> Sequential((0): Linear(in_features=2, out_features=2, bias=True)(1): Linear(in_features=2, out_features=2, bias=True))1 -> Linear(in_features=2, out_features=2, bias=True)  mtia( device=None ) [source]  [source]  [LINK_24]  Move all model parameters and buffers to the MTIA.  This also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.  Note  This method modifies the module in-place.  Parameters  device ( int  ,  optional ) – if specified, all parameters will be\\ncopied to that device  Returns  self  Return type  Module  named_buffers( prefix=\\'\\' , recurse=True , remove_duplicate=True ) [source]  [source]  [LINK_25]  Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters  prefix ( str ) – prefix to prepend to all buffer names.  recurse ( bool  ,  optional ) – if True, then yields buffers of this module\\nand all submodules. Otherwise, yields only buffers that\\nare direct members of this module. Defaults to True.  remove_duplicate ( bool  ,  optional ) – whether to remove the duplicated buffers in the result. Defaults to True.  Yields  (str, torch.Tensor) – Tuple containing the name and buffer  Return type  Iterator [ Tuple [ str , Tensor ]]  Example:  >>>forname,bufinself.named_buffers():>>>ifnamein[\\'running_var\\']:>>>print(buf.size())  named_children() [source]  [source]  [LINK_26]  Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields  (str, Module) – Tuple containing a name and child module  Return type  Iterator [ Tuple [ str , Module ]]  Example:  >>>forname,moduleinmodel.named_children():>>>ifnamein[\\'conv4\\',\\'conv5\\']:>>>print(module)  named_modules( memo=None , prefix=\\'\\' , remove_duplicate=True ) [source]  [source]  [LINK_27]  Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Parameters  memo ( Optional  [  Set  [  Module  ]  ] ) – a memo to store the set of modules already added to the result  prefix ( str ) – a prefix that will be added to the name of the module  remove_duplicate ( bool ) – whether to remove the duplicated module instances in the result\\nor not  Yields  (str, Module) – Tuple of name and module  Note  Duplicate modules are returned only once. In the following\\nexample, l will be returned only once.  Example:  >>>l=nn.Linear(2,2)>>>net=nn.Sequential(l,l)>>>foridx,minenumerate(net.named_modules()):...print(idx,\\'->\\',m)0 -> (\\'\\', Sequential((0): Linear(in_features=2, out_features=2, bias=True)(1): Linear(in_features=2, out_features=2, bias=True)))1 -> (\\'0\\', Linear(in_features=2, out_features=2, bias=True))  named_parameters( prefix=\\'\\' , recurse=True , remove_duplicate=True ) [source]  [source]  [LINK_28]  Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters  prefix ( str ) – prefix to prepend to all parameter names.  recurse ( bool ) – if True, then yields parameters of this module\\nand all submodules. Otherwise, yields only parameters that\\nare direct members of this module.  remove_duplicate ( bool  ,  optional ) – whether to remove the duplicated\\nparameters in the result. Defaults to True.  Yields  (str, Parameter) – Tuple containing the name and parameter  Return type  Iterator [ Tuple [ str , Parameter ]]  Example:  >>>forname,paraminself.named_parameters():>>>ifnamein[\\'bias\\']:>>>print(param.size())  parameters( recurse=True ) [source]  [source]  [LINK_29]  Return an iterator over module parameters.  This is typically passed to an optimizer.  Parameters  recurse ( bool ) – if True, then yields parameters of this module\\nand all submodules. Otherwise, yields only parameters that\\nare direct members of this module.  Yields  Parameter – module parameter  Return type  Iterator [ Parameter ]  Example:  >>>forparaminmodel.parameters():>>>print(type(param),param.size())<class \\'torch.Tensor\\'> (20L,)<class \\'torch.Tensor\\'> (20L, 1L, 5L, 5L)  register_backward_hook( hook ) [source]  [source]  [LINK_30]  Register a backward hook on the module.  This function is deprecated in favor of register_full_backward_hook() and\\nthe behavior of this function will change in future versions.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_buffer( name , tensor , persistent=True ) [source]  [source]  [LINK_31]  Add a buffer to the module.  This is typically used to register a buffer that should not to be\\nconsidered a model parameter. For example, BatchNorm’s running_mean is not a parameter, but is part of the module’s state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting persistent to False . The\\nonly difference between a persistent buffer and a non-persistent buffer\\nis that the latter will not be a part of this module’s state_dict .  Buffers can be accessed as attributes using given names.  Parameters  name ( str ) – name of the buffer. The buffer can be accessed\\nfrom this module using the given name  tensor ( Tensor  or  None ) – buffer to be registered. If None , then operations\\nthat run on buffers, such as cuda , are ignored. If None ,\\nthe buffer is not included in the module’s state_dict .  persistent ( bool ) – whether the buffer is part of this module’s state_dict .  Example:  >>>self.register_buffer(\\'running_mean\\',torch.zeros(num_features))  register_forward_hook( hook , * , prepend=False , with_kwargs=False , always_call=False ) [source]  [source]  [LINK_32]  Register a forward hook on the module.  The hook will be called every time after forward() has computed an output.  If with_kwargs is False or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won’t be\\npassed to the hooks and only to the forward . The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after forward() is called. The hook\\nshould have the following signature:  hook(module,args,output)->Noneormodifiedoutput  If with_kwargs is True , the forward hook will be passed the kwargs given to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature:  hook(module,args,kwargs,output)->Noneormodifiedoutput  Parameters  hook ( Callable ) – The user defined hook to be registered.  prepend ( bool ) – If True , the provided hook will be fired\\nbefore all existing forward hooks on this torch.nn.modules.Module . Otherwise, the provided hook will be fired after all existing forward hooks on\\nthis torch.nn.modules.Module . Note that global forward hooks registered with register_module_forward_hook() will fire before all hooks\\nregistered by this method.\\nDefault: False  with_kwargs ( bool ) – If True , the hook will be passed the\\nkwargs given to the forward function.\\nDefault: False  always_call ( bool ) – If True the hook will be run regardless of\\nwhether an exception is raised while calling the Module.\\nDefault: False  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_forward_pre_hook( hook , * , prepend=False , with_kwargs=False ) [source]  [source]  [LINK_33]  Register a forward pre-hook on the module.  The hook will be called every time before forward() is invoked.  If with_kwargs is false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won’t be\\npassed to the hooks and only to the forward . The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature:  hook(module,args)->Noneormodifiedinput  If with_kwargs is true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned. The hook should have\\nthe following signature:  hook(module,args,kwargs)->Noneoratupleofmodifiedinputandkwargs  Parameters  hook ( Callable ) – The user defined hook to be registered.  prepend ( bool ) – If true, the provided hook will be fired before\\nall existing forward_pre hooks on this torch.nn.modules.Module . Otherwise, the provided hook will be fired after all existing forward_pre hooks\\non this torch.nn.modules.Module . Note that global forward_pre hooks registered with register_module_forward_pre_hook() will fire before all\\nhooks registered by this method.\\nDefault: False  with_kwargs ( bool ) – If true, the hook will be passed the kwargs\\ngiven to the forward function.\\nDefault: False  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_full_backward_hook( hook , prepend=False ) [source]  [source]  [LINK_34]  Register a backward hook on the module.  The hook will be called every time the gradients with respect to a module\\nare computed, i.e. the hook will execute if and only if the gradients with\\nrespect to module outputs are computed. The hook should have the following\\nsignature:  hook(module,grad_input,grad_output)->tuple(Tensor)orNone  The grad_input and grad_output are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the input that will be used in place of grad_input in\\nsubsequent computations. grad_input will only correspond to the inputs given\\nas positional arguments and all kwarg arguments are ignored. Entries\\nin grad_input and grad_output will be None for all non-Tensor\\narguments.  For technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module’s forward function.  Warning  Modifying inputs or outputs inplace is not allowed when using backward hooks and\\nwill raise an error.  Parameters  hook ( Callable ) – The user-defined hook to be registered.  prepend ( bool ) – If true, the provided hook will be fired before\\nall existing backward hooks on this torch.nn.modules.Module . Otherwise, the provided hook will be fired after all existing backward hooks on\\nthis torch.nn.modules.Module . Note that global backward hooks registered with register_module_full_backward_hook() will fire before\\nall hooks registered by this method.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_full_backward_pre_hook( hook , prepend=False ) [source]  [source]  [LINK_35]  Register a backward pre-hook on the module.  The hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature:  hook(module,grad_output)->tuple[Tensor]orNone  The grad_output is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of grad_output in\\nsubsequent computations. Entries in grad_output will be None for\\nall non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module’s forward function.  Warning  Modifying inputs inplace is not allowed when using backward hooks and\\nwill raise an error.  Parameters  hook ( Callable ) – The user-defined hook to be registered.  prepend ( bool ) – If true, the provided hook will be fired before\\nall existing backward_pre hooks on this torch.nn.modules.Module . Otherwise, the provided hook will be fired after all existing backward_pre hooks\\non this torch.nn.modules.Module . Note that global backward_pre hooks registered with register_module_full_backward_pre_hook() will fire before\\nall hooks registered by this method.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_load_state_dict_post_hook( hook ) [source]  [source]  [LINK_36]  Register a post-hook to be run after module’s load_state_dict() is called.  It should have the following signature::  hook(module, incompatible_keys) -> None  The module argument is the current module that this hook is registered\\non, and the incompatible_keys argument is a NamedTuple consisting\\nof attributes missing_keys and unexpected_keys . missing_keys is a list of str containing the missing keys and unexpected_keys is a list of str containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling load_state_dict() with strict=True are affected by modifications the hook makes to missing_keys or unexpected_keys , as expected. Additions to either\\nset of keys will result in an error being thrown when strict=True , and\\nclearing out both missing and unexpected keys will avoid an error.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_load_state_dict_pre_hook( hook ) [source]  [source]  [LINK_37]  Register a pre-hook to be run before module’s load_state_dict() is called.  It should have the following signature::  hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950  Parameters  hook ( Callable ) – Callable hook that will be invoked before\\nloading the state dict.  register_module( name , module ) [source]  [source]  [LINK_38]  Alias for add_module() .  register_parameter( name , param ) [source]  [source]  [LINK_39]  Add a parameter to the module.  The parameter can be accessed as an attribute using given name.  Parameters  name ( str ) – name of the parameter. The parameter can be accessed\\nfrom this module using the given name  param ( Parameter  or  None ) – parameter to be added to the module. If None , then operations that run on parameters, such as cuda ,\\nare ignored. If None , the parameter is not included in the\\nmodule’s state_dict .  register_state_dict_post_hook( hook ) [source]  [source]  [LINK_40]  Register a post-hook for the state_dict() method.  It should have the following signature::  hook(module, state_dict, prefix, local_metadata) -> None  The registered hooks can modify the state_dict inplace.  register_state_dict_pre_hook( hook ) [source]  [source]  [LINK_41]  Register a pre-hook for the state_dict() method.  It should have the following signature::  hook(module, prefix, keep_vars) -> None  The registered hooks can be used to perform pre-processing before the state_dict call is made.  requires_grad_( requires_grad=True ) [source]  [source]  [LINK_42]  Change if autograd should record operations on parameters in this module.  This method sets the parameters’ requires_grad attributes\\nin-place.  This method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).  See Locally disabling gradient computation for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it.  Parameters  requires_grad ( bool ) – whether autograd should record operations on\\nparameters in this module. Default: True .  Returns  self  Return type  Module  set_extra_state( state ) [source]  [source]  [LINK_43]  Set extra state contained in the loaded state_dict .  This function is called from load_state_dict() to handle any extra state\\nfound within the state_dict . Implement this function and a corresponding get_extra_state() for your module if you need to store extra state within its state_dict .  Parameters  state ( dict ) – Extra state from the state_dict  set_submodule( target , module ) [source]  [source]  [LINK_44]  Set the submodule given by target if it exists, otherwise throw an error.  For example, let’s say you have an nn.Module  A that\\nlooks like this:  A(\\n    (net_b): Module(\\n        (net_c): Module(\\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n        )\\n        (linear): Linear(in_features=100, out_features=200, bias=True)\\n    )\\n)  (The diagram shows an nn.Module  A . A has a nested\\nsubmodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .)  To overide the Conv2d with a new submodule Linear , you\\nwould call set_submodule(\"net_b.net_c.conv\",nn.Linear(33,16)) .  Parameters  target ( str ) – The fully-qualified string name of the submodule\\nto look for. (See above example for how to specify a\\nfully-qualified string.)  module ( Module ) – The module to set the submodule to.  Raises  ValueError – If the target string is empty  AttributeError – If the target string references an invalid\\n    path or resolves to something that is not an nn.Module  share_memory() [source]  [source]  [LINK_45]  See torch.Tensor.share_memory_() .  Return type  T  state_dict( * , destination: T_destination , prefix: str = \\'\\' , keep_vars: bool = False )→T_destination [source]  [source]  [LINK_46]  state_dict( * , prefix: str = \\'\\' , keep_vars: bool = False )→Dict [ str ,  Any ]  Return a dictionary containing references to the whole state of the module.  Both parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to None are not included.  Note  The returned object is a shallow copy. It contains references\\nto the module’s parameters and buffers.  Warning  Currently state_dict() also accepts positional arguments for destination , prefix and keep_vars in order. However,\\nthis is being deprecated and keyword arguments will be enforced in\\nfuture releases.  Warning  Please avoid the use of argument destination as it is not\\ndesigned for end-users.  Parameters  destination ( dict  ,  optional ) – If provided, the state of module will\\nbe updated into the dict and the same object is returned.\\nOtherwise, an OrderedDict will be created and returned.\\nDefault: None .  prefix ( str  ,  optional ) – a prefix added to parameter and buffer\\nnames to compose the keys in state_dict. Default: \\'\\' .  keep_vars ( bool  ,  optional ) – by default the Tensor s\\nreturned in the state dict are detached from autograd. If it’s\\nset to True , detaching will not be performed.\\nDefault: False .  Returns  a dictionary containing a whole state of the module  Return type  dict  Example:  >>>module.state_dict().keys()[\\'bias\\', \\'weight\\']  to( device: Optional [ Union [ str ,  device ,  int ]] = ... , dtype: Optional [ dtype ] = ... , non_blocking: bool = ... )→Self [source]  [source]  [LINK_47]  to( dtype: dtype , non_blocking: bool = ... )→Self  to( tensor: Tensor , non_blocking: bool = ... )→Self  Move and/or cast the parameters and buffers.  This can be called as  to( device=None , dtype=None , non_blocking=False ) [source]  [source]  to( dtype , non_blocking=False ) [source]  [source]  to( tensor , non_blocking=False ) [source]  [source]  to( memory_format=torch.channels_last ) [source]  [source]  Its signature is similar to torch.Tensor.to() , but only accepts\\nfloating point or complex dtype s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device , if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving CPU Tensors with\\npinned memory to CUDA devices.  See below for examples.  Note  This method modifies the module in-place.  Parameters  device ( torch.device ) – the desired device of the parameters\\nand buffers in this module  dtype ( torch.dtype ) – the desired floating point or complex dtype of\\nthe parameters and buffers in this module  tensor ( torch.Tensor ) – Tensor whose dtype and device are the desired\\ndtype and device for all parameters and buffers in this module  memory_format ( torch.memory_format ) – the desired memory\\nformat for 4D parameters and buffers in this module (keyword\\nonly argument)  Returns  self  Return type  Module  Examples:  >>>linear=nn.Linear(2,2)>>>linear.weightParameter containing:tensor([[ 0.1913, -0.3420],[-0.5113, -0.2325]])>>>linear.to(torch.double)Linear(in_features=2, out_features=2, bias=True)>>>linear.weightParameter containing:tensor([[ 0.1913, -0.3420],[-0.5113, -0.2325]], dtype=torch.float64)>>>gpu1=torch.device(\"cuda:1\")>>>linear.to(gpu1,dtype=torch.half,non_blocking=True)Linear(in_features=2, out_features=2, bias=True)>>>linear.weightParameter containing:tensor([[ 0.1914, -0.3420],[-0.5112, -0.2324]], dtype=torch.float16, device=\\'cuda:1\\')>>>cpu=torch.device(\"cpu\")>>>linear.to(cpu)Linear(in_features=2, out_features=2, bias=True)>>>linear.weightParameter containing:tensor([[ 0.1914, -0.3420],[-0.5112, -0.2324]], dtype=torch.float16)>>>linear=nn.Linear(2,2,bias=None).to(torch.cdouble)>>>linear.weightParameter containing:tensor([[ 0.3741+0.j,  0.2382+0.j],[ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)>>>linear(torch.ones(3,2,dtype=torch.cdouble))tensor([[0.6122+0.j, 0.1150+0.j],[0.6122+0.j, 0.1150+0.j],[0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)  to_empty( * , device , recurse=True ) [source]  [source]  [LINK_48]  Move the parameters and buffers to the specified device without copying storage.  Parameters  device ( torch.device ) – The desired device of the parameters\\nand buffers in this module.  recurse ( bool ) – Whether parameters and buffers of submodules should\\nbe recursively moved to the specified device.  Returns  self  Return type  Module  train( mode=True ) [source]  [source]  [LINK_49]  Set the module in training mode.  This has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. Dropout , BatchNorm ,\\netc.  Parameters  mode ( bool ) – whether to set training mode ( True ) or evaluation\\nmode ( False ). Default: True .  Returns  self  Return type  Module  type( dst_type ) [source]  [source]  [LINK_50]  Casts all parameters and buffers to dst_type .  Note  This method modifies the module in-place.  Parameters  dst_type ( type  or  string ) – the desired type  Returns  self  Return type  Module  xpu( device=None ) [source]  [source]  [LINK_51]  Move all model parameters and buffers to the XPU.  This also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.  Note  This method modifies the module in-place.  Parameters  device ( int  ,  optional ) – if specified, all parameters will be\\ncopied to that device  Returns  self  Return type  Module  zero_grad( set_to_none=True ) [source]  [source]  [LINK_52]  Reset gradients of all model parameters.  See similar function under torch.optim.Optimizer for more context.  Parameters  set_to_none ( bool ) – instead of setting to zero, set the grads to None.\\nSee torch.optim.Optimizer.zero_grad() for details.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.add_module\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.bfloat16\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.children\\n [LINK_8]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.compile\\n [LINK_9]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cpu\\n [LINK_10]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda\\n [LINK_11]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.double\\n [LINK_12]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval\\n [LINK_13]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.extra_repr\\n [LINK_14]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.float\\n [LINK_15]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward\\n [LINK_16]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_buffer\\n [LINK_17]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_extra_state\\n [LINK_18]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_parameter\\n [LINK_19]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_submodule\\n [LINK_20]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half\\n [LINK_21]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.ipu\\n [LINK_22]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict\\n [LINK_23]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.modules\\n [LINK_24]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.mtia\\n [LINK_25]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers\\n [LINK_26]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_children\\n [LINK_27]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_modules\\n [LINK_28]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters\\n [LINK_29]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters\\n [LINK_30]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_backward_hook\\n [LINK_31]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\\n [LINK_32]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook\\n [LINK_33]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook\\n [LINK_34]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook\\n [LINK_35]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook\\n [LINK_36]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_post_hook\\n [LINK_37]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_pre_hook\\n [LINK_38]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_module\\n [LINK_39]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_parameter\\n [LINK_40]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_post_hook\\n [LINK_41]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_pre_hook\\n [LINK_42]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.requires_grad_\\n [LINK_43]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.set_extra_state\\n [LINK_44]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.set_submodule\\n [LINK_45]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.share_memory\\n [LINK_46]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict\\n [LINK_47]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to\\n [LINK_48]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to_empty\\n [LINK_49]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train\\n [LINK_50]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.type\\n [LINK_51]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.xpu\\n [LINK_52]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.zero_grad', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L402', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.add_module', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L634', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.add_module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.apply', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L995', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply', 'https://pytorch.org/docs/stable/nn.init.html#nn-init-doc', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.bfloat16', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1170', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.bfloat16', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.buffers', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2665', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Iterator', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.children', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2719', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.children', 'https://docs.python.org/3/library/typing.html#typing.Iterator', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.compile', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2982', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.compile', 'https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile', 'https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile', 'https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.cpu', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1112', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cpu', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.cuda', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1036', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.double', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1148', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.double', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2846', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train', 'https://pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.extra_repr', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2922', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.extra_repr', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.float', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1137', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.float', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L386', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.get_buffer', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L826', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_buffer', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/exceptions.html#AttributeError', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.get_extra_state', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L862', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_extra_state', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.set_extra_state', 'https://docs.python.org/3/library/functions.html#object', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.get_parameter', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L790', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_parameter', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/exceptions.html#AttributeError', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.get_submodule', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L666', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_submodule', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/exceptions.html#AttributeError', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.half', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1159', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.ipu', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1055', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.ipu', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.load_state_dict', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2473', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict', 'https://pytorch.org/docs/stable/future_mod.html#torch.__future__.get_swap_module_params_on_conversion', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.modules', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2748', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.modules', 'https://docs.python.org/3/library/typing.html#typing.Iterator', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.mtia', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1093', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.mtia', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.named_buffers', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2688', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Iterator', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.named_children', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2728', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_children', 'https://docs.python.org/3/library/typing.html#typing.Iterator', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.named_modules', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2775', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_modules', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/typing.html#typing.Set', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.named_parameters', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2633', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Iterator', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.parameters', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2608', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Iterator', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_backward_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1394', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_backward_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_buffer', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L522', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_forward_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1646', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1580', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_full_backward_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1420', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_full_backward_pre_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1345', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_load_state_dict_post_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2263', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_post_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_load_state_dict_pre_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2251', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_pre_hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_module', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L662', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_module', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.add_module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_parameter', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L584', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_parameter', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_state_dict_post_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2061', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_post_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_state_dict_pre_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2085', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_pre_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.requires_grad_', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2864', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.requires_grad_', 'https://pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.set_extra_state', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L883', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.set_extra_state', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_extra_state', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.set_submodule', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L731', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.set_submodule', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/exceptions.html#ValueError', 'https://docs.python.org/3/library/exceptions.html#AttributeError', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.share_memory', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2915', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.share_memory', 'https://pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.state_dict', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2142', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Dict', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1216', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1216', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1216', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1216', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1216', 'https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to_empty', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1181', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to_empty', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2824', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.type', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1123', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.type', 'https://docs.python.org/3/library/functions.html#type', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.xpu', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L1074', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.xpu', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.zero_grad', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L2887', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.zero_grad', 'https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad', 'https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017985'), 'title': 'Sequential', 'page_text': 'Sequential [LINK_1]  class torch.nn.Sequential( *args: Module ) [source]  [source]  [LINK_2]  class torch.nn.Sequential( arg: OrderedDict [ str ,  Module ] )  A sequential container.  Modules will be added to it in the order they are passed in the\\nconstructor. Alternatively, an OrderedDict of modules can be\\npassed in. The forward() method of Sequential accepts any\\ninput and forwards it to the first module it contains. It then\\n“chains” outputs to inputs sequentially for each subsequent module,\\nfinally returning the output of the last module.  The value a Sequential provides over manually calling a sequence\\nof modules is that it allows treating the whole container as a\\nsingle module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are\\neach a registered submodule of the Sequential ).  What’s the difference between a Sequential and a torch.nn.ModuleList ? A ModuleList is exactly what it\\nsounds like–a list for storing Module s! On the other hand,\\nthe layers in a Sequential are connected in a cascading way.  Example:  # Using Sequential to create a small model. When `model` is run,# input will first be passed to `Conv2d(1,20,5)`. The output of# `Conv2d(1,20,5)` will be used as the input to the first# `ReLU`; the output of the first `ReLU` will become the input# for `Conv2d(20,64,5)`. Finally, the output of# `Conv2d(20,64,5)` will be used as input to the second `ReLU`model=nn.Sequential(nn.Conv2d(1,20,5),nn.ReLU(),nn.Conv2d(20,64,5),nn.ReLU())# Using Sequential with OrderedDict. This is functionally the# same as the above codemodel=nn.Sequential(OrderedDict([(\\'conv1\\',nn.Conv2d(1,20,5)),(\\'relu1\\',nn.ReLU()),(\\'conv2\\',nn.Conv2d(20,64,5)),(\\'relu2\\',nn.ReLU())]))  append( module ) [source]  [source]  [LINK_3]  Append a given module to the end.  Parameters  module ( nn.Module ) – module to append  Return type  Sequential\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#sequential\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential.append', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#sequential', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L64', 'https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential', 'https://docs.python.org/3/library/collections.html#collections.OrderedDict', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential.append', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L253', 'https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential.append', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017986'), 'title': 'ModuleList', 'page_text': 'ModuleList [LINK_1]  class torch.nn.ModuleList( modules=None ) [source]  [source]  [LINK_2]  Holds submodules in a list.  ModuleList can be indexed like a regular Python list, but\\nmodules it contains are properly registered, and will be visible by all Module methods.  Parameters  modules ( iterable  ,  optional ) – an iterable of modules to add  Example:  classMyModule(nn.Module):def__init__(self)->None:super().__init__()self.linears=nn.ModuleList([nn.Linear(10,10)foriinrange(10)])defforward(self,x):# ModuleList can act as an iterable, or be indexed using intsfori,linenumerate(self.linears):x=self.linears[i//2](x)+l(x)returnx  append( module ) [source]  [source]  [LINK_3]  Append a given module to the end of the list.  Parameters  module ( nn.Module ) – module to append  Return type  ModuleList  extend( modules ) [source]  [source]  [LINK_4]  Append modules from a Python iterable to the end of the list.  Parameters  modules ( iterable ) – iterable of modules to append  Return type  Self  insert( index , module ) [source]  [source]  [LINK_5]  Insert a given module before a given index in the list.  Parameters  index ( int ) – index to insert.  module ( nn.Module ) – module to insert\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#modulelist\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.append\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.extend\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.insert', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#modulelist', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L281', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList.append', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L416', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.append', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList.extend', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L430', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.extend', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList.insert', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L405', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.insert', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017987'), 'title': 'ModuleDict', 'page_text': 'ModuleDict [LINK_1]  class torch.nn.ModuleDict( modules=None ) [source]  [source]  [LINK_2]  Holds submodules in a dictionary.  ModuleDict can be indexed like a regular Python dictionary,\\nbut modules it contains are properly registered, and will be visible by all Module methods.  ModuleDict is an ordered dictionary that respects  the order of insertion, and  in update() , the order of the merged OrderedDict , dict (started from Python 3.6) or another ModuleDict (the argument to update() ).  Note that update() with other unordered mapping\\ntypes (e.g., Python’s plain dict before Python version 3.6) does not\\npreserve the order of the merged mapping.  Parameters  modules ( iterable  ,  optional ) – a mapping (dictionary) of (string: module)\\nor an iterable of key-value pairs of type (string, module)  Example:  classMyModule(nn.Module):def__init__(self)->None:super().__init__()self.choices=nn.ModuleDict({\\'conv\\':nn.Conv2d(10,10,3),\\'pool\\':nn.MaxPool2d(3)})self.activations=nn.ModuleDict([[\\'lrelu\\',nn.LeakyReLU()],[\\'prelu\\',nn.PReLU()]])defforward(self,x,choice,act):x=self.choices[choice](x)x=self.activations[act](x)returnx  clear() [source]  [source]  [LINK_3]  Remove all items from the ModuleDict.  items() [source]  [source]  [LINK_4]  Return an iterable of the ModuleDict key/value pairs.  Return type  Iterable [ Tuple [ str , Module ]]  keys() [source]  [source]  [LINK_5]  Return an iterable of the ModuleDict keys.  Return type  Iterable [ str ]  pop( key ) [source]  [source]  [LINK_6]  Remove key from the ModuleDict and return its module.  Parameters  key ( str ) – key to pop from the ModuleDict  Return type  Module  update( modules ) [source]  [source]  [LINK_7]  Update the ModuleDict with key-value pairs from a mapping, overwriting existing keys.  Note  If modules is an OrderedDict , a ModuleDict , or\\nan iterable of key-value pairs, the order of new elements in it is preserved.  Parameters  modules ( iterable ) – a mapping (dictionary) from string to Module ,\\nor an iterable of key-value pairs of type (string, Module )  values() [source]  [source]  [LINK_8]  Return an iterable of the ModuleDict values.  Return type  Iterable [ Module ]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#moduledict\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.clear\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.items\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.keys\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.pop\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.update\\n [LINK_8]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.values', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#moduledict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L449', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.update', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.update', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.update', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.clear', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L522', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.clear', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.items', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L541', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.items', 'https://docs.python.org/3/library/typing.html#typing.Iterable', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.keys', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L536', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.keys', 'https://docs.python.org/3/library/typing.html#typing.Iterable', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.pop', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L526', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.pop', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.update', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L551', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.update', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict', 'https://pytorch.org/docs/stable/nn.html#module-torch.nn.modules', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.values', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L546', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.values', 'https://docs.python.org/3/library/typing.html#typing.Iterable', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017988'), 'title': 'ParameterList', 'page_text': 'ParameterList [LINK_1]  class torch.nn.ParameterList( values=None ) [source]  [source]  [LINK_2]  Holds parameters in a list.  ParameterList can be used like a regular Python\\nlist, but Tensors that are Parameter are properly registered,\\nand will be visible by all Module methods.  Note that the constructor, assigning an element of the list, the append() method and the extend() method will convert any Tensor into Parameter .  Parameters  parameters ( iterable  ,  optional ) – an iterable of elements to add to the list.  Example:  classMyModule(nn.Module):def__init__(self)->None:super().__init__()self.params=nn.ParameterList([nn.Parameter(torch.randn(10,10))foriinrange(10)])defforward(self,x):# ParameterList can act as an iterable, or be indexed using intsfori,pinenumerate(self.params):x=self.params[i//2].mm(x)+p.mm(x)returnx  append( value ) [source]  [source]  [LINK_3]  Append a given value at the end of the list.  Parameters  value ( Any ) – value to append  Return type  ParameterList  extend( values ) [source]  [source]  [LINK_4]  Append values from a Python iterable to the end of the list.  Parameters  values ( iterable ) – iterable of values to append  Return type  Self\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#parameterlist\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.append\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.extend', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#parameterlist', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L591', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.append', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.extend', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList.append', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L678', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.append', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList.extend', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L689', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.extend', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017989'), 'title': 'ParameterDict', 'page_text': 'ParameterDict [LINK_1]  class torch.nn.ParameterDict( parameters=None ) [source]  [source]  [LINK_2]  Holds parameters in a dictionary.  ParameterDict can be indexed like a regular Python dictionary, but Parameters it\\ncontains are properly registered, and will be visible by all Module methods.\\nOther objects are treated as would be done by a regular Python dictionary  ParameterDict is an ordered dictionary. update() with other unordered mapping\\ntypes (e.g., Python’s plain dict ) does not preserve the order of the\\nmerged mapping. On the other hand, OrderedDict or another ParameterDict will preserve their ordering.  Note that the constructor, assigning an element of the dictionary and the update() method will convert any Tensor into Parameter .  Parameters  values ( iterable  ,  optional ) – a mapping (dictionary) of\\n(string : Any) or an iterable of key-value pairs\\nof type (string, Any)  Example:  classMyModule(nn.Module):def__init__(self)->None:super().__init__()self.params=nn.ParameterDict({\\'left\\':nn.Parameter(torch.randn(5,10)),\\'right\\':nn.Parameter(torch.randn(5,10))})defforward(self,x,choice):x=self.params[choice].mm(x)returnx  clear() [source]  [source]  [LINK_3]  Remove all items from the ParameterDict.  copy() [source]  [source]  [LINK_4]  Return a copy of this ParameterDict instance.  Return type  ParameterDict  fromkeys( keys , default=None ) [source]  [source]  [LINK_5]  Return a new ParameterDict with the keys provided.  Parameters  keys ( iterable  ,  string ) – keys to make the new ParameterDict from  default ( Parameter  ,  optional ) – value to set for all keys  Return type  ParameterDict  get( key , default=None ) [source]  [source]  [LINK_6]  Return the parameter associated with key if present. Otherwise return default if provided, None if not.  Parameters  key ( str ) – key to get from the ParameterDict  default ( Parameter  ,  optional ) – value to return if key not present  Return type  Any  items() [source]  [source]  [LINK_7]  Return an iterable of the ParameterDict key/value pairs.  Return type  Iterable [ Tuple [ str , Any ]]  keys() [source]  [source]  [LINK_8]  Return an iterable of the ParameterDict keys.  Return type  Iterable [ str ]  pop( key ) [source]  [source]  [LINK_9]  Remove key from the ParameterDict and return its parameter.  Parameters  key ( str ) – key to pop from the ParameterDict  Return type  Any  popitem() [source]  [source]  [LINK_10]  Remove and return the last inserted (key, parameter) pair from the ParameterDict.  Return type  Tuple [ str , Any ]  setdefault( key , default=None ) [source]  [source]  [LINK_11]  Set the default for a key in the Parameterdict.  If key is in the ParameterDict, return its value.\\nIf not, insert key with a parameter default and return default . default defaults to None .  Parameters  key ( str ) – key to set default for  default ( Any ) – the parameter set to the key  Return type  Any  update( parameters ) [source]  [source]  [LINK_12]  Update the ParameterDict with key-value pairs from parameters , overwriting existing keys.  Note  If parameters is an OrderedDict , a ParameterDict , or\\nan iterable of key-value pairs, the order of new elements in it is preserved.  Parameters  parameters ( iterable ) – a mapping (dictionary) from string to Parameter , or an iterable of\\nkey-value pairs of type (string, Parameter )  values() [source]  [source]  [LINK_13]  Return an iterable of the ParameterDict values.  Return type  Iterable [ Any ]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#parameterdict\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.clear\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.copy\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.fromkeys\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.get\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.items\\n [LINK_8]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.keys\\n [LINK_9]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.pop\\n [LINK_10]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.popitem\\n [LINK_11]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.setdefault\\n [LINK_12]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.update\\n [LINK_13]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.values', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#parameterdict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L735', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.update', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.update', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.clear', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L843', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.clear', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.copy', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L819', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.copy', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.fromkeys', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L876', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.fromkeys', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.get', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L867', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.get', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.items', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L891', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.items', 'https://docs.python.org/3/library/typing.html#typing.Iterable', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.keys', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L887', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.keys', 'https://docs.python.org/3/library/typing.html#typing.Iterable', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.pop', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L848', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.pop', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.popitem', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L858', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.popitem', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.setdefault', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L828', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.setdefault', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.update', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L899', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.update', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.values', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/container.py#L895', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.values', 'https://docs.python.org/3/library/typing.html#typing.Iterable', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01798a'), 'title': 'register_module_forward_pre_hook', 'page_text': 'torch.nn.modules.module.register_module_forward_pre_hook [LINK_1]  torch.nn.modules.module.register_module_forward_pre_hook( hook ) [source]  [source]  [LINK_2]  Register a forward pre-hook common to all modules.  Warning  This adds global state to the nn.module module\\nand it is only intended for debugging/profiling purposes.  The hook will be called every time before forward() is invoked.\\nIt should have the following signature:  hook(module,input)->Noneormodifiedinput  The input contains only the positional arguments given to the module.\\nKeyword arguments won’t be passed to the hooks and only to the forward .\\nThe hook can modify the input. User can either return a tuple or a\\nsingle modified value in the hook. We will wrap the value into a tuple\\nif a single value is returned(unless that value is already a tuple).  This hook has precedence over the specific module hooks registered with register_forward_pre_hook .  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch-nn-modules-module-register-module-forward-pre-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch-nn-modules-module-register-module-forward-pre-hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#register_module_forward_pre_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L212', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01798b'), 'title': 'register_module_forward_hook', 'page_text': 'torch.nn.modules.module.register_module_forward_hook [LINK_1]  torch.nn.modules.module.register_module_forward_hook( hook , * , with_kwargs=False , always_call=False ) [source]  [source]  [LINK_2]  Register a global forward hook for all the modules.  Warning  This adds global state to the nn.module module\\nand it is only intended for debugging/profiling purposes.  The hook will be called every time after forward() has computed an output.\\nIt should have the following signature:  hook(module,input,output)->Noneormodifiedoutput  The input contains only the positional arguments given to the module.\\nKeyword arguments won’t be passed to the hooks and only to the forward .\\nYou can optionally modify the output of the module by returning a new value\\nthat will replace the output from the forward() function.  Parameters  hook ( Callable ) – The user defined hook to be registered.  always_call ( bool ) – If True the hook will be run regardless of\\nwhether an exception is raised while calling the Module.\\nDefault: False  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  This hook will be executed before specific module hooks registered with register_forward_hook .\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html#torch-nn-modules-module-register-module-forward-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html#torch-nn-modules-module-register-module-forward-hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#register_module_forward_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L244', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01798c'), 'title': 'register_module_backward_hook', 'page_text': 'torch.nn.modules.module.register_module_backward_hook [LINK_1]  torch.nn.modules.module.register_module_backward_hook( hook ) [source]  [source]  [LINK_2]  Register a backward hook common to all the modules.  This function is deprecated in favor of torch.nn.modules.module.register_module_full_backward_hook() and the behavior of this function will change in future versions.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html#torch-nn-modules-module-register-module-backward-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html#torch-nn-modules-module-register-module-backward-hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#register_module_backward_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L291', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01798d'), 'title': 'register_module_full_backward_pre_hook', 'page_text': 'torch.nn.modules.module.register_module_full_backward_pre_hook [LINK_1]  torch.nn.modules.module.register_module_full_backward_pre_hook( hook ) [source]  [source]  [LINK_2]  Register a backward pre-hook common to all the modules.  Warning  This adds global state to the nn.module module\\nand it is only intended for debugging/profiling purposes.  Hooks registered using this function behave in the same way as those\\nregistered by torch.nn.Module.register_full_backward_pre_hook() .\\nRefer to its documentation for more details.  Hooks registered using this function will be called before hooks registered\\nusing torch.nn.Module.register_full_backward_pre_hook() .  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch-nn-modules-module-register-module-full-backward-pre-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch-nn-modules-module-register-module-full-backward-pre-hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#register_module_full_backward_pre_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L320', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01798e'), 'title': 'register_module_full_backward_hook', 'page_text': 'torch.nn.modules.module.register_module_full_backward_hook [LINK_1]  torch.nn.modules.module.register_module_full_backward_hook( hook ) [source]  [source]  [LINK_2]  Register a backward hook common to all the modules.  Warning  This adds global state to the nn.module module\\nand it is only intended for debugging/profiling purposes.  Hooks registered using this function behave in the same way as those\\nregistered by torch.nn.Module.register_full_backward_hook() .\\nRefer to its documentation for more details.  Hooks registered using this function will be called before hooks registered\\nusing torch.nn.Module.register_full_backward_hook() .  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch-nn-modules-module-register-module-full-backward-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch-nn-modules-module-register-module-full-backward-hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#register_module_full_backward_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L347', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01798f'), 'title': 'register_module_buffer_registration_hook', 'page_text': 'torch.nn.modules.module.register_module_buffer_registration_hook [LINK_1]  torch.nn.modules.module.register_module_buffer_registration_hook( hook ) [source]  [source]  [LINK_2]  Register a buffer registration hook common to all modules.  Warning  This adds global state to the nn.Module module  The hook will be called every time register_buffer() is invoked.\\nIt should have the following signature:  hook(module,name,buffer)->Noneornewbuffer  The hook can modify the input or return a single modified value in the hook.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch-nn-modules-module-register-module-buffer-registration-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch-nn-modules-module-register-module-buffer-registration-hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#register_module_buffer_registration_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L134', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017990'), 'title': 'register_module_module_registration_hook', 'page_text': 'torch.nn.modules.module.register_module_module_registration_hook [LINK_1]  torch.nn.modules.module.register_module_module_registration_hook( hook ) [source]  [source]  [LINK_2]  Register a module registration hook common to all modules.  Warning  This adds global state to the nn.Module module  The hook will be called every time register_module() is invoked.\\nIt should have the following signature:  hook(module,name,submodule)->Noneornewsubmodule  The hook can modify the input or return a single modified value in the hook.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch-nn-modules-module-register-module-module-registration-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch-nn-modules-module-register-module-module-registration-hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#register_module_module_registration_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L160', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017991'), 'title': 'register_module_parameter_registration_hook', 'page_text': 'torch.nn.modules.module.register_module_parameter_registration_hook [LINK_1]  torch.nn.modules.module.register_module_parameter_registration_hook( hook ) [source]  [source]  [LINK_2]  Register a parameter registration hook common to all modules.  Warning  This adds global state to the nn.Module module  The hook will be called every time register_parameter() is invoked.\\nIt should have the following signature:  hook(module,name,param)->Noneornewparameter  The hook can modify the input or return a single modified value in the hook.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch-nn-modules-module-register-module-parameter-registration-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch-nn-modules-module-register-module-parameter-registration-hook', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#register_module_parameter_registration_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/module.py#L186', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017992'), 'title': 'nn.Conv1d', 'page_text': 'Conv1d [LINK_1]  class torch.nn.Conv1d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 1D convolution over an input signal composed of several input\\nplanes.  In the simplest case, the output value of the layer with input size(  N  ,  C  in  ,  L  )  (N, C_{\\\\text{in}}, L)(N,Cin\\u200b,L)and output(  N  ,  C  out  ,  L  out  )  (N, C_{\\\\text{out}}, L_{\\\\text{out}})(N,Cout\\u200b,Lout\\u200b)can be\\nprecisely described as:  out  (  N  i  ,  C  out  j  )  =  bias  (  C  out  j  )  +  ∑  k  =  0  C  i  n  −  1  weight  (  C  out  j  ,  k  )  ⋆  input  (  N  i  ,  k  )  \\\\text{out}(N_i, C_{\\\\text{out}_j}) = \\\\text{bias}(C_{\\\\text{out}_j}) +\\n\\\\sum_{k = 0}^{C_{in} - 1} \\\\text{weight}(C_{\\\\text{out}_j}, k)\\n\\\\star \\\\text{input}(N_i, k)out(Ni\\u200b,Coutj\\u200b\\u200b)=bias(Coutj\\u200b\\u200b)+k=0∑Cin\\u200b−1\\u200bweight(Coutj\\u200b\\u200b,k)⋆input(Ni\\u200b,k)  where⋆  \\\\star⋆is the valid cross-correlation operator,N  NNis a batch size,C  CCdenotes a number of channels,L  LLis a length of signal sequence.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation, a single\\nnumber or a one-element tuple.  padding controls the amount of padding applied to the input. It\\ncan be either a string {‘valid’, ‘same’} or a tuple of ints giving the\\namount of implicit padding applied on both sides.  dilation controls the spacing between the kernel points; also\\nknown as the à trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  Note  When groups == in_channels and out_channels == K * in_channels ,\\nwhere K is a positive integer, this operation is also known as a “depthwise convolution”.  In other words, for an input of size(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b),\\na depthwise convolution with a depthwise multiplier K can be performed with the arguments(  C  in  =  C  in  ,  C  out  =  C  in  ×  K  ,  .  .  .  ,  groups  =  C  in  )  (C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})(Cin\\u200b=Cin\\u200b,Cout\\u200b=Cin\\u200b×K,...,groups=Cin\\u200b).  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Note  padding=\\'valid\\' is the same as no padding. padding=\\'same\\' pads\\nthe input so the output has the shape as the input. However, this mode\\ndoesn’t support any stride values other than 1.  Note  This module supports complex data types i.e. complex32,complex64,complex128 .  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  ,  tuple  or  str  ,  optional ) – Padding added to both sides of\\nthe input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel\\nelements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the\\noutput. Default: True  padding_mode ( str  ,  optional ) – \\'zeros\\' , \\'reflect\\' , \\'replicate\\' or \\'circular\\' . Default: \\'zeros\\'  Shape:  Input:(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b)or(  C  i  n  ,  L  i  n  )  (C_{in}, L_{in})(Cin\\u200b,Lin\\u200b)  Output:(  N  ,  C  o  u  t  ,  L  o  u  t  )  (N, C_{out}, L_{out})(N,Cout\\u200b,Lout\\u200b)or(  C  o  u  t  ,  L  o  u  t  )  (C_{out}, L_{out})(Cout\\u200b,Lout\\u200b), where  L  o  u  t  =  ⌊  L  i  n  +  2  ×  padding  −  dilation  ×  (  kernel_size  −  1  )  −  1  stride  +  1  ⌋  L_{out} = \\\\left\\\\lfloor\\\\frac{L_{in} + 2 \\\\times \\\\text{padding} - \\\\text{dilation}\\n          \\\\times (\\\\text{kernel\\\\_size} - 1) - 1}{\\\\text{stride}} + 1\\\\right\\\\rfloorLout\\u200b=⌊strideLin\\u200b+2×padding−dilation×(kernel_size−1)−1\\u200b+1⌋  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  out_channels  ,  in_channels  groups  ,  kernel_size  )  (\\\\text{out\\\\_channels},\\n\\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}}, \\\\text{kernel\\\\_size})(out_channels,groupsin_channels\\u200b,kernel_size).\\nThe values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  kernel_size  k = \\\\frac{groups}{C_\\\\text{in} * \\\\text{kernel\\\\_size}}k=Cin\\u200b∗kernel_sizegroups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape\\n(out_channels). If bias is True , then the values of these weights are\\nsampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  kernel_size  k = \\\\frac{groups}{C_\\\\text{in} * \\\\text{kernel\\\\_size}}k=Cin\\u200b∗kernel_sizegroups\\u200b  Examples:  >>>m=nn.Conv1d(16,33,3,stride=2)>>>input=torch.randn(20,16,50)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L214', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d', 'https://en.wikipedia.org/wiki/Cross-correlation', 'https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017993'), 'title': 'nn.Conv2d', 'page_text': 'Conv2d [LINK_1]  class torch.nn.Conv2d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 2D convolution over an input signal composed of several input\\nplanes.  In the simplest case, the output value of the layer with input size(  N  ,  C  in  ,  H  ,  W  )  (N, C_{\\\\text{in}}, H, W)(N,Cin\\u200b,H,W)and output(  N  ,  C  out  ,  H  out  ,  W  out  )  (N, C_{\\\\text{out}}, H_{\\\\text{out}}, W_{\\\\text{out}})(N,Cout\\u200b,Hout\\u200b,Wout\\u200b)can be precisely described as:  out  (  N  i  ,  C  out  j  )  =  bias  (  C  out  j  )  +  ∑  k  =  0  C  in  −  1  weight  (  C  out  j  ,  k  )  ⋆  input  (  N  i  ,  k  )  \\\\text{out}(N_i, C_{\\\\text{out}_j}) = \\\\text{bias}(C_{\\\\text{out}_j}) +\\n\\\\sum_{k = 0}^{C_{\\\\text{in}} - 1} \\\\text{weight}(C_{\\\\text{out}_j}, k) \\\\star \\\\text{input}(N_i, k)out(Ni\\u200b,Coutj\\u200b\\u200b)=bias(Coutj\\u200b\\u200b)+k=0∑Cin\\u200b−1\\u200bweight(Coutj\\u200b\\u200b,k)⋆input(Ni\\u200b,k)  where⋆  \\\\star⋆is the valid 2D cross-correlation operator,N  NNis a batch size,C  CCdenotes a number of channels,H  HHis a height of input planes in pixels, andW  WWis\\nwidth in pixels.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation, a single\\nnumber or a tuple.  padding controls the amount of padding applied to the input. It\\ncan be either a string {‘valid’, ‘same’} or an int / a tuple of ints giving the\\namount of implicit padding applied on both sides.  dilation controls the spacing between the kernel points; also\\nknown as the à trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  The parameters kernel_size , stride , padding , dilation can either be:  a single int – in which case the same value is used for the height and width dimension  a tuple of two ints – in which case, the first int is used for the height dimension,\\nand the second int for the width dimension  Note  When groups == in_channels and out_channels == K * in_channels ,\\nwhere K is a positive integer, this operation is also known as a “depthwise convolution”.  In other words, for an input of size(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b),\\na depthwise convolution with a depthwise multiplier K can be performed with the arguments(  C  in  =  C  in  ,  C  out  =  C  in  ×  K  ,  .  .  .  ,  groups  =  C  in  )  (C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})(Cin\\u200b=Cin\\u200b,Cout\\u200b=Cin\\u200b×K,...,groups=Cin\\u200b).  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Note  padding=\\'valid\\' is the same as no padding. padding=\\'same\\' pads\\nthe input so the output has the shape as the input. However, this mode\\ndoesn’t support any stride values other than 1.  Note  This module supports complex data types i.e. complex32,complex64,complex128 .  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  ,  tuple  or  str  ,  optional ) – Padding added to all four sides of\\nthe input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the\\noutput. Default: True  padding_mode ( str  ,  optional ) – \\'zeros\\' , \\'reflect\\' , \\'replicate\\' or \\'circular\\' . Default: \\'zeros\\'  Shape:  Input:(  N  ,  C  i  n  ,  H  i  n  ,  W  i  n  )  (N, C_{in}, H_{in}, W_{in})(N,Cin\\u200b,Hin\\u200b,Win\\u200b)or(  C  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in}, H_{in}, W_{in})(Cin\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, H_{out}, W_{out})(N,Cout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C_{out}, H_{out}, W_{out})(Cout\\u200b,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  0  ]  −  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  −  1  stride  [  0  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0]\\n          \\\\times (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[0]Hin\\u200b+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  1  ]  −  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  −  1  stride  [  1  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1]\\n          \\\\times (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[1]Win\\u200b+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1\\u200b+1⌋  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  out_channels  ,  in_channels  groups  ,  (\\\\text{out\\\\_channels}, \\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}},(out_channels,groupsin_channels\\u200b,kernel_size[0]  ,  kernel_size[1]  )  \\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]})kernel_size[0],kernel_size[1]).\\nThe values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  ∏  i  =  0  1  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}k=Cin\\u200b∗∏i=01\\u200bkernel_size[i]groups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape\\n(out_channels). If bias is True ,\\nthen the values of these weights are\\nsampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  ∏  i  =  0  1  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}k=Cin\\u200b∗∏i=01\\u200bkernel_size[i]groups\\u200b  Examples  >>># With square kernels and equal stride>>>m=nn.Conv2d(16,33,3,stride=2)>>># non-square kernels and unequal stride and with padding>>>m=nn.Conv2d(16,33,(3,5),stride=(2,1),padding=(4,2))>>># non-square kernels and unequal stride and with padding and dilation>>>m=nn.Conv2d(16,33,(3,5),stride=(2,1),padding=(4,2),dilation=(3,1))>>>input=torch.randn(20,16,50,100)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L378', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d', 'https://en.wikipedia.org/wiki/Cross-correlation', 'https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017994'), 'title': 'nn.Conv3d', 'page_text': 'Conv3d [LINK_1]  class torch.nn.Conv3d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 3D convolution over an input signal composed of several input\\nplanes.  In the simplest case, the output value of the layer with input size(  N  ,  C  i  n  ,  D  ,  H  ,  W  )  (N, C_{in}, D, H, W)(N,Cin\\u200b,D,H,W)and output(  N  ,  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b)can be precisely described as:  o  u  t  (  N  i  ,  C  o  u  t  j  )  =  b  i  a  s  (  C  o  u  t  j  )  +  ∑  k  =  0  C  i  n  −  1  w  e  i  g  h  t  (  C  o  u  t  j  ,  k  )  ⋆  i  n  p  u  t  (  N  i  ,  k  )  out(N_i, C_{out_j}) = bias(C_{out_j}) +\\n                        \\\\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\\\star input(N_i, k)out(Ni\\u200b,Coutj\\u200b\\u200b)=bias(Coutj\\u200b\\u200b)+k=0∑Cin\\u200b−1\\u200bweight(Coutj\\u200b\\u200b,k)⋆input(Ni\\u200b,k)  where⋆  \\\\star⋆is the valid 3D cross-correlation operator  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation.  padding controls the amount of padding applied to the input. It\\ncan be either a string {‘valid’, ‘same’} or a tuple of ints giving the\\namount of implicit padding applied on both sides.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but this link has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  The parameters kernel_size , stride , padding , dilation can either be:  a single int – in which case the same value is used for the depth, height and width dimension  a tuple of three ints – in which case, the first int is used for the depth dimension,\\nthe second int for the height dimension and the third int for the width dimension  Note  When groups == in_channels and out_channels == K * in_channels ,\\nwhere K is a positive integer, this operation is also known as a “depthwise convolution”.  In other words, for an input of size(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b),\\na depthwise convolution with a depthwise multiplier K can be performed with the arguments(  C  in  =  C  in  ,  C  out  =  C  in  ×  K  ,  .  .  .  ,  groups  =  C  in  )  (C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})(Cin\\u200b=Cin\\u200b,Cout\\u200b=Cin\\u200b×K,...,groups=Cin\\u200b).  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Note  padding=\\'valid\\' is the same as no padding. padding=\\'same\\' pads\\nthe input so the output has the shape as the input. However, this mode\\ndoesn’t support any stride values other than 1.  Note  This module supports complex data types i.e. complex32,complex64,complex128 .  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  ,  tuple  or  str  ,  optional ) – Padding added to all six sides of\\nthe input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  padding_mode ( str  ,  optional ) – \\'zeros\\' , \\'reflect\\' , \\'replicate\\' or \\'circular\\' . Default: \\'zeros\\'  Shape:  Input:(  N  ,  C  i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\\u200b,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in}, D_{in}, H_{in}, W_{in})(Cin\\u200b,Din\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C_{out}, D_{out}, H_{out}, W_{out})(Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b),\\nwhere  D  o  u  t  =  ⌊  D  i  n  +  2  ×  padding  [  0  ]  −  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  −  1  stride  [  0  ]  +  1  ⌋  D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0]\\n      \\\\times (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorDout\\u200b=⌊stride[0]Din\\u200b+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1\\u200b+1⌋  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  1  ]  −  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  −  1  stride  [  1  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1]\\n      \\\\times (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[1]Hin\\u200b+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  2  ]  −  dilation  [  2  ]  ×  (  kernel_size  [  2  ]  −  1  )  −  1  stride  [  2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] - \\\\text{dilation}[2]\\n      \\\\times (\\\\text{kernel\\\\_size}[2] - 1) - 1}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[2]Win\\u200b+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1\\u200b+1⌋  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  out_channels  ,  in_channels  groups  ,  (\\\\text{out\\\\_channels}, \\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}},(out_channels,groupsin_channels\\u200b,kernel_size[0]  ,  kernel_size[1]  ,  kernel_size[2]  )  \\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]}, \\\\text{kernel\\\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]).\\nThe values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  ∏  i  =  0  2  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}k=Cin\\u200b∗∏i=02\\u200bkernel_size[i]groups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape (out_channels). If bias is True ,\\nthen the values of these weights are\\nsampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  ∏  i  =  0  2  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}k=Cin\\u200b∗∏i=02\\u200bkernel_size[i]groups\\u200b  Examples:  >>># With square kernels and equal stride>>>m=nn.Conv3d(16,33,3,stride=2)>>># non-square kernels and unequal stride and with padding>>>m=nn.Conv3d(16,33,(3,5,2),stride=(2,1,1),padding=(4,2,0))>>>input=torch.randn(20,16,10,50,100)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#conv3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#conv3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L557', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d', 'https://en.wikipedia.org/wiki/Cross-correlation', 'https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017995'), 'title': 'nn.ConvTranspose1d', 'page_text': 'ConvTranspose1d [LINK_1]  class torch.nn.ConvTranspose1d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 1D transposed convolution operator over an input image\\ncomposed of several input planes.  This module can be seen as the gradient of Conv1d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\\na deconvolution (although it is not an actual deconvolution operation as it does\\nnot compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation.  padding controls the amount of implicit zero padding on both\\nsides for dilation*(kernel_size-1)-padding number of points. See note\\nbelow for details.  output_padding controls the additional size added to one side\\nof the output shape. See note below for details.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but the link here has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  Note  The padding argument effectively adds dilation*(kernel_size-1)-padding amount of zero padding to both sizes of the input. This is set so that\\nwhen a Conv1d and a ConvTranspose1d are initialized with same parameters, they are inverses of each other in\\nregard to the input and output shapes. However, when stride>1 , Conv1d maps multiple input shapes to the same output\\nshape. output_padding is provided to resolve this ambiguity by\\neffectively increasing the calculated output shape on one side. Note\\nthat output_padding is only used to find output shape, but does\\nnot actually add zero-padding to output.  Note  In some circumstances when using the CUDA backend with CuDNN, this operator\\nmay select a nondeterministic algorithm to increase performance. If this is\\nundesirable, you can try to make the operation deterministic (potentially at\\na performance cost) by setting torch.backends.cudnn.deterministic=True .\\nPlease see the notes on Reproducibility for background.  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding\\nwill be added to both sides of the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side\\nof the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  Shape:  Input:(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b)or(  C  i  n  ,  L  i  n  )  (C_{in}, L_{in})(Cin\\u200b,Lin\\u200b)  Output:(  N  ,  C  o  u  t  ,  L  o  u  t  )  (N, C_{out}, L_{out})(N,Cout\\u200b,Lout\\u200b)or(  C  o  u  t  ,  L  o  u  t  )  (C_{out}, L_{out})(Cout\\u200b,Lout\\u200b), where  L  o  u  t  =  (  L  i  n  −  1  )  ×  stride  −  2  ×  padding  +  dilation  ×  (  kernel_size  −  1  )  +  output_padding  +  1  L_{out} = (L_{in} - 1) \\\\times \\\\text{stride} - 2 \\\\times \\\\text{padding} + \\\\text{dilation}\\n          \\\\times (\\\\text{kernel\\\\_size} - 1) + \\\\text{output\\\\_padding} + 1Lout\\u200b=(Lin\\u200b−1)×stride−2×padding+dilation×(kernel_size−1)+output_padding+1  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  in_channels  ,  out_channels  groups  ,  (\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},(in_channels,groupsout_channels\\u200b,kernel_size  )  \\\\text{kernel\\\\_size})kernel_size).\\nThe values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  kernel_size  k = \\\\frac{groups}{C_\\\\text{out} * \\\\text{kernel\\\\_size}}k=Cout\\u200b∗kernel_sizegroups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape (out_channels).\\nIf bias is True , then the values of these weights are\\nsampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  kernel_size  k = \\\\frac{groups}{C_\\\\text{out} * \\\\text{kernel\\\\_size}}k=Cout\\u200b∗kernel_sizegroups\\u200b\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#convtranspose1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#convtranspose1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L822', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf', 'https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017996'), 'title': 'nn.ConvTranspose2d', 'page_text': 'ConvTranspose2d [LINK_1]  class torch.nn.ConvTranspose2d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 2D transposed convolution operator over an input image\\ncomposed of several input planes.  This module can be seen as the gradient of Conv2d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\\na deconvolution (although it is not an actual deconvolution operation as it does\\nnot compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation.  padding controls the amount of implicit zero padding on both\\nsides for dilation*(kernel_size-1)-padding number of points. See note\\nbelow for details.  output_padding controls the additional size added to one side\\nof the output shape. See note below for details.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but the link here has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  The parameters kernel_size , stride , padding , output_padding can either be:  a single int – in which case the same value is used for the height and width dimensions  a tuple of two ints – in which case, the first int is used for the height dimension,\\nand the second int for the width dimension  Note  The padding argument effectively adds dilation*(kernel_size-1)-padding amount of zero padding to both sizes of the input. This is set so that\\nwhen a Conv2d and a ConvTranspose2d are initialized with same parameters, they are inverses of each other in\\nregard to the input and output shapes. However, when stride>1 , Conv2d maps multiple input shapes to the same output\\nshape. output_padding is provided to resolve this ambiguity by\\neffectively increasing the calculated output shape on one side. Note\\nthat output_padding is only used to find output shape, but does\\nnot actually add zero-padding to output.  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding\\nwill be added to both sides of each dimension in the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side\\nof each dimension in the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  Shape:  Input:(  N  ,  C  i  n  ,  H  i  n  ,  W  i  n  )  (N, C_{in}, H_{in}, W_{in})(N,Cin\\u200b,Hin\\u200b,Win\\u200b)or(  C  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in}, H_{in}, W_{in})(Cin\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, H_{out}, W_{out})(N,Cout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C_{out}, H_{out}, W_{out})(Cout\\u200b,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride  [  0  ]  −  2  ×  padding  [  0  ]  +  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  +  output_padding  [  0  ]  +  1  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{dilation}[0]\\n          \\\\times (\\\\text{kernel\\\\_size}[0] - 1) + \\\\text{output\\\\_padding}[0] + 1Hout\\u200b=(Hin\\u200b−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1  W  o  u  t  =  (  W  i  n  −  1  )  ×  stride  [  1  ]  −  2  ×  padding  [  1  ]  +  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  +  output_padding  [  1  ]  +  1  W_{out} = (W_{in} - 1) \\\\times \\\\text{stride}[1] - 2 \\\\times \\\\text{padding}[1] + \\\\text{dilation}[1]\\n          \\\\times (\\\\text{kernel\\\\_size}[1] - 1) + \\\\text{output\\\\_padding}[1] + 1Wout\\u200b=(Win\\u200b−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  in_channels  ,  out_channels  groups  ,  (\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},(in_channels,groupsout_channels\\u200b,kernel_size[0]  ,  kernel_size[1]  )  \\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]})kernel_size[0],kernel_size[1]).\\nThe values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  ∏  i  =  0  1  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}k=Cout\\u200b∗∏i=01\\u200bkernel_size[i]groups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape (out_channels)\\nIf bias is True , then the values of these weights are\\nsampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  ∏  i  =  0  1  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}k=Cout\\u200b∗∏i=01\\u200bkernel_size[i]groups\\u200b  Examples:  >>># With square kernels and equal stride>>>m=nn.ConvTranspose2d(16,33,3,stride=2)>>># non-square kernels and unequal stride and with padding>>>m=nn.ConvTranspose2d(16,33,(3,5),stride=(2,1),padding=(4,2))>>>input=torch.randn(20,16,50,100)>>>output=m(input)>>># exact output size can be also specified as an argument>>>input=torch.randn(1,16,12,12)>>>downsample=nn.Conv2d(16,16,3,stride=2,padding=1)>>>upsample=nn.ConvTranspose2d(16,16,3,stride=2,padding=1)>>>h=downsample(input)>>>h.size()torch.Size([1, 16, 6, 6])>>>output=upsample(h,output_size=input.size())>>>output.size()torch.Size([1, 16, 12, 12])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#convtranspose2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#convtranspose2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L986', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf', 'https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017997'), 'title': 'nn.ConvTranspose3d', 'page_text': 'ConvTranspose3d [LINK_1]  class torch.nn.ConvTranspose3d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 3D transposed convolution operator over an input image composed of several input\\nplanes.\\nThe transposed convolution operator multiplies each input value element-wise by a learnable kernel,\\nand sums over the outputs from all input feature planes.  This module can be seen as the gradient of Conv3d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\\na deconvolution (although it is not an actual deconvolution operation as it does\\nnot compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation.  padding controls the amount of implicit zero padding on both\\nsides for dilation*(kernel_size-1)-padding number of points. See note\\nbelow for details.  output_padding controls the additional size added to one side\\nof the output shape. See note below for details.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but the link here has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  The parameters kernel_size , stride , padding , output_padding can either be:  a single int – in which case the same value is used for the depth, height and width dimensions  a tuple of three ints – in which case, the first int is used for the depth dimension,\\nthe second int for the height dimension and the third int for the width dimension  Note  The padding argument effectively adds dilation*(kernel_size-1)-padding amount of zero padding to both sizes of the input. This is set so that\\nwhen a Conv3d and a ConvTranspose3d are initialized with same parameters, they are inverses of each other in\\nregard to the input and output shapes. However, when stride>1 , Conv3d maps multiple input shapes to the same output\\nshape. output_padding is provided to resolve this ambiguity by\\neffectively increasing the calculated output shape on one side. Note\\nthat output_padding is only used to find output shape, but does\\nnot actually add zero-padding to output.  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding\\nwill be added to both sides of each dimension in the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side\\nof each dimension in the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  Shape:  Input:(  N  ,  C  i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\\u200b,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in}, D_{in}, H_{in}, W_{in})(Cin\\u200b,Din\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C_{out}, D_{out}, H_{out}, W_{out})(Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  (  D  i  n  −  1  )  ×  stride  [  0  ]  −  2  ×  padding  [  0  ]  +  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  +  output_padding  [  0  ]  +  1  D_{out} = (D_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{dilation}[0]\\n          \\\\times (\\\\text{kernel\\\\_size}[0] - 1) + \\\\text{output\\\\_padding}[0] + 1Dout\\u200b=(Din\\u200b−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride  [  1  ]  −  2  ×  padding  [  1  ]  +  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  +  output_padding  [  1  ]  +  1  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[1] - 2 \\\\times \\\\text{padding}[1] + \\\\text{dilation}[1]\\n          \\\\times (\\\\text{kernel\\\\_size}[1] - 1) + \\\\text{output\\\\_padding}[1] + 1Hout\\u200b=(Hin\\u200b−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1  W  o  u  t  =  (  W  i  n  −  1  )  ×  stride  [  2  ]  −  2  ×  padding  [  2  ]  +  dilation  [  2  ]  ×  (  kernel_size  [  2  ]  −  1  )  +  output_padding  [  2  ]  +  1  W_{out} = (W_{in} - 1) \\\\times \\\\text{stride}[2] - 2 \\\\times \\\\text{padding}[2] + \\\\text{dilation}[2]\\n          \\\\times (\\\\text{kernel\\\\_size}[2] - 1) + \\\\text{output\\\\_padding}[2] + 1Wout\\u200b=(Win\\u200b−1)×stride[2]−2×padding[2]+dilation[2]×(kernel_size[2]−1)+output_padding[2]+1  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  in_channels  ,  out_channels  groups  ,  (\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},(in_channels,groupsout_channels\\u200b,kernel_size[0]  ,  kernel_size[1]  ,  kernel_size[2]  )  \\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]}, \\\\text{kernel\\\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]).\\nThe values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  ∏  i  =  0  2  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}k=Cout\\u200b∗∏i=02\\u200bkernel_size[i]groups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape (out_channels)\\nIf bias is True , then the values of these weights are\\nsampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  ∏  i  =  0  2  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}k=Cout\\u200b∗∏i=02\\u200bkernel_size[i]groups\\u200b  Examples:  >>># With square kernels and equal stride>>>m=nn.ConvTranspose3d(16,33,3,stride=2)>>># non-square kernels and unequal stride and with padding>>>m=nn.ConvTranspose3d(16,33,(3,5,2),stride=(2,1,1),padding=(0,4,2))>>>input=torch.randn(20,16,10,50,100)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#convtranspose3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#convtranspose3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L1174', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf', 'https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017998'), 'title': 'nn.LazyConv1d', 'page_text': 'LazyConv1d [LINK_1]  class torch.nn.LazyConv1d( out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.Conv1d module with lazy initialization of the in_channels argument.  The in_channels argument of the Conv1d is inferred from the input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – Zero-padding added to both sides of\\nthe input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel\\nelements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the\\noutput. Default: True  padding_mode ( str  ,  optional ) – \\'zeros\\' , \\'reflect\\' , \\'replicate\\' or \\'circular\\' . Default: \\'zeros\\'  See also  torch.nn.Conv1d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of Conv1d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#lazyconv1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#lazyconv1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#LazyConv1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L1455', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L214', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017999'), 'title': 'nn.LazyConv2d', 'page_text': 'LazyConv2d [LINK_1]  class torch.nn.LazyConv2d( out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.Conv2d module with lazy initialization of the in_channels argument.  The in_channels argument of the Conv2d that is inferred from the input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – Zero-padding added to both sides of\\nthe input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel\\nelements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the\\noutput. Default: True  padding_mode ( str  ,  optional ) – \\'zeros\\' , \\'reflect\\' , \\'replicate\\' or \\'circular\\' . Default: \\'zeros\\'  See also  torch.nn.Conv2d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of Conv2d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#lazyconv2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#lazyconv2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#LazyConv2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L1524', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L378', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01799a'), 'title': 'nn.LazyConv3d', 'page_text': 'LazyConv3d [LINK_1]  class torch.nn.LazyConv3d( out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.Conv3d module with lazy initialization of the in_channels argument.  The in_channels argument of the Conv3d that is inferred from\\nthe input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – Zero-padding added to both sides of\\nthe input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel\\nelements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the\\noutput. Default: True  padding_mode ( str  ,  optional ) – \\'zeros\\' , \\'reflect\\' , \\'replicate\\' or \\'circular\\' . Default: \\'zeros\\'  See also  torch.nn.Conv3d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of Conv3d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#lazyconv3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#lazyconv3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#LazyConv3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L1593', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L557', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01799b'), 'title': 'nn.LazyConvTranspose1d', 'page_text': 'LazyConvTranspose1d [LINK_1]  class torch.nn.LazyConvTranspose1d( out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument.  The in_channels argument of the ConvTranspose1d that is inferred from\\nthe input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding\\nwill be added to both sides of the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side\\nof the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  See also  torch.nn.ConvTranspose1d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of ConvTranspose1d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#lazyconvtranspose1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#lazyconvtranspose1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#LazyConvTranspose1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L1663', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L822', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01799c'), 'title': 'nn.LazyConvTranspose2d', 'page_text': 'LazyConvTranspose2d [LINK_1]  class torch.nn.LazyConvTranspose2d( out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument.  The in_channels argument of the ConvTranspose2d is inferred from\\nthe input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding\\nwill be added to both sides of each dimension in the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side\\nof each dimension in the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  See also  torch.nn.ConvTranspose2d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of ConvTranspose2d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#lazyconvtranspose2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#lazyconvtranspose2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#LazyConvTranspose2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L1732', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L986', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01799d'), 'title': 'nn.LazyConvTranspose3d', 'page_text': 'LazyConvTranspose3d [LINK_1]  class torch.nn.LazyConvTranspose3d( out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode=\\'zeros\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument.  The in_channels argument of the ConvTranspose3d is inferred from\\nthe input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding\\nwill be added to both sides of each dimension in the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side\\nof each dimension in the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  See also  torch.nn.ConvTranspose3d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of ConvTranspose3d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#lazyconvtranspose3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#lazyconvtranspose3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#LazyConvTranspose3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L1801', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/conv.py#L1174', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01799e'), 'title': 'nn.Unfold', 'page_text': 'Unfold [LINK_1]  class torch.nn.Unfold( kernel_size , dilation=1 , padding=0 , stride=1 ) [source]  [source]  [LINK_2]  Extracts sliding local blocks from a batched input tensor.  Consider a batched input tensor of shape(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗),\\nwhereN  NNis the batch dimension,C  CCis the channel dimension,\\nand∗  *∗represent arbitrary spatial dimensions. This operation flattens\\neach sliding kernel_size -sized block within the spatial dimensions\\nof input into a column (i.e., last dimension) of a 3-D output tensor of shape(  N  ,  C  ×  ∏  (  kernel_size  )  ,  L  )  (N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)(N,C×∏(kernel_size),L), whereC  ×  ∏  (  kernel_size  )  C \\\\times \\\\prod(\\\\text{kernel\\\\_size})C×∏(kernel_size)is the total number of values\\nwithin each block (a block has∏  (  kernel_size  )  \\\\prod(\\\\text{kernel\\\\_size})∏(kernel_size)spatial\\nlocations each containing aC  CC-channeled vector), andL  LLis\\nthe total number of such blocks:  L  =  ∏  d  ⌊  spatial_size  [  d  ]  +  2  ×  padding  [  d  ]  −  dilation  [  d  ]  ×  (  kernel_size  [  d  ]  −  1  )  −  1  stride  [  d  ]  +  1  ⌋  ,  L = \\\\prod_d \\\\left\\\\lfloor\\\\frac{\\\\text{spatial\\\\_size}[d] + 2 \\\\times \\\\text{padding}[d] %\\n    - \\\\text{dilation}[d] \\\\times (\\\\text{kernel\\\\_size}[d] - 1) - 1}{\\\\text{stride}[d]} + 1\\\\right\\\\rfloor,L=d∏\\u200b⌊stride[d]spatial_size[d]+2×padding[d]−dilation[d]×(kernel_size[d]−1)−1\\u200b+1⌋,  wherespatial_size  \\\\text{spatial\\\\_size}spatial_sizeis formed by the spatial dimensions\\nof input (∗  *∗above), andd  ddis over all spatial\\ndimensions.  Therefore, indexing output at the last dimension (column dimension)\\ngives all values within a certain block.  The padding , stride and dilation arguments specify\\nhow the sliding blocks are retrieved.  stride controls the stride for the sliding blocks.  padding controls the amount of implicit zero-paddings on both\\nsides for padding number of points for each dimension before\\nreshaping.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but this link has a nice visualization of what dilation does.  Parameters  kernel_size ( int  or  tuple ) – the size of the sliding blocks  dilation ( int  or  tuple  ,  optional ) – a parameter that controls the\\nstride of elements within the\\nneighborhood. Default: 1  padding ( int  or  tuple  ,  optional ) – implicit zero padding to be added on\\nboth sides of input. Default: 0  stride ( int  or  tuple  ,  optional ) – the stride of the sliding blocks in the input\\nspatial dimensions. Default: 1  If kernel_size , dilation , padding or stride is an int or a tuple of length 1, their values will be\\nreplicated across all spatial dimensions.  For the case of two input spatial dimensions this operation is sometimes\\ncalled im2col .  Note  Fold calculates each combined value in the resulting\\nlarge tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by\\ncopying from the large tensor. So, if the blocks overlap, they are not\\ninverses of each other.  In general, folding and unfolding operations are related as\\nfollows. Consider Fold and Unfold instances created with the same\\nparameters:  >>>fold_params=dict(kernel_size=...,dilation=...,padding=...,stride=...)>>>fold=nn.Fold(output_size=...,**fold_params)>>>unfold=nn.Unfold(**fold_params)  Then for any (supported) input tensor the following\\nequality holds:  fold(unfold(input))==divisor*input  where divisor is a tensor that depends only on the shape\\nand dtype of the input :  >>>input_ones=torch.ones(input.shape,dtype=input.dtype)>>>divisor=fold(unfold(input_ones))  When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each\\nother (up to constant divisor).  Warning  Currently, only 4-D input tensors (batched image-like tensors) are\\nsupported.  Shape:  Input:(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)  Output:(  N  ,  C  ×  ∏  (  kernel_size  )  ,  L  )  (N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)(N,C×∏(kernel_size),L)as described above  Examples:  >>>unfold=nn.Unfold(kernel_size=(2,3))>>>input=torch.randn(2,5,3,4)>>>output=unfold(input)>>># each patch contains 30 values (2x3=6 vectors, each of 5 channels)>>># 4 blocks (2x3 kernels) in total in the 3x4 input>>>output.size()torch.Size([2, 30, 4])>>># Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)>>>inp=torch.randn(1,3,10,12)>>>w=torch.randn(2,3,4,5)>>>inp_unf=torch.nn.functional.unfold(inp,(4,5))>>>out_unf=inp_unf.transpose(1,2).matmul(w.view(w.size(0),-1).t()).transpose(1,2)>>>out=torch.nn.functional.fold(out_unf,(7,8),(1,1))>>># or equivalently (and avoiding a copy),>>># out = out_unf.view(1, 2, 7, 8)>>>(torch.nn.functional.conv2d(inp,w)-out).abs().max()tensor(1.9073e-06)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#unfold\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#unfold', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/fold.html#Unfold', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/fold.py#L164', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold', 'https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold', 'https://pytorch.org/docs/stable/generated/torch.nn.Fold.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe01799f'), 'title': 'nn.Fold', 'page_text': 'Fold [LINK_1]  class torch.nn.Fold( output_size , kernel_size , dilation=1 , padding=0 , stride=1 ) [source]  [source]  [LINK_2]  Combines an array of sliding local blocks into a large containing tensor.  Consider a batched input tensor containing sliding local blocks,\\ne.g., patches of images, of shape(  N  ,  C  ×  ∏  (  kernel_size  )  ,  L  )  (N, C \\\\times  \\\\prod(\\\\text{kernel\\\\_size}), L)(N,C×∏(kernel_size),L),\\nwhereN  NNis batch dimension,C  ×  ∏  (  kernel_size  )  C \\\\times \\\\prod(\\\\text{kernel\\\\_size})C×∏(kernel_size)is the number of values within a block (a block has∏  (  kernel_size  )  \\\\prod(\\\\text{kernel\\\\_size})∏(kernel_size)spatial locations each containing aC  CC-channeled vector), andL  LLis the total number of blocks. (This is exactly the\\nsame specification as the output shape of Unfold .) This\\noperation combines these local blocks into the large output tensor\\nof shape(  N  ,  C  ,  output_size  [  0  ]  ,  output_size  [  1  ]  ,  …  )  (N, C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)(N,C,output_size[0],output_size[1],…)by summing the overlapping values. Similar to Unfold , the\\narguments must satisfy  L  =  ∏  d  ⌊  output_size  [  d  ]  +  2  ×  padding  [  d  ]  −  dilation  [  d  ]  ×  (  kernel_size  [  d  ]  −  1  )  −  1  stride  [  d  ]  +  1  ⌋  ,  L = \\\\prod_d \\\\left\\\\lfloor\\\\frac{\\\\text{output\\\\_size}[d] + 2 \\\\times \\\\text{padding}[d] %\\n    - \\\\text{dilation}[d] \\\\times (\\\\text{kernel\\\\_size}[d] - 1) - 1}{\\\\text{stride}[d]} + 1\\\\right\\\\rfloor,L=d∏\\u200b⌊stride[d]output_size[d]+2×padding[d]−dilation[d]×(kernel_size[d]−1)−1\\u200b+1⌋,  whered  ddis over all spatial dimensions.  output_size describes the spatial shape of the large containing\\ntensor of the sliding local blocks. It is useful to resolve the ambiguity\\nwhen multiple input shapes map to same number of sliding blocks, e.g.,\\nwith stride>0 .  The padding , stride and dilation arguments specify\\nhow the sliding blocks are retrieved.  stride controls the stride for the sliding blocks.  padding controls the amount of implicit zero-paddings on both\\nsides for padding number of points for each dimension before\\nreshaping.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but this link has a nice visualization of what dilation does.  Parameters  output_size ( int  or  tuple ) – the shape of the spatial dimensions of the\\noutput (i.e., output.sizes()[2:] )  kernel_size ( int  or  tuple ) – the size of the sliding blocks  dilation ( int  or  tuple  ,  optional ) – a parameter that controls the\\nstride of elements within the\\nneighborhood. Default: 1  padding ( int  or  tuple  ,  optional ) – implicit zero padding to be added on\\nboth sides of input. Default: 0  stride ( int  or  tuple ) – the stride of the sliding blocks in the input\\nspatial dimensions. Default: 1  If output_size , kernel_size , dilation , padding or stride is an int or a tuple of length 1 then\\ntheir values will be replicated across all spatial dimensions.  For the case of two output spatial dimensions this operation is sometimes\\ncalled col2im .  Note  Fold calculates each combined value in the resulting\\nlarge tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by\\ncopying from the large tensor. So, if the blocks overlap, they are not\\ninverses of each other.  In general, folding and unfolding operations are related as\\nfollows. Consider Fold and Unfold instances created with the same\\nparameters:  >>>fold_params=dict(kernel_size=...,dilation=...,padding=...,stride=...)>>>fold=nn.Fold(output_size=...,**fold_params)>>>unfold=nn.Unfold(**fold_params)  Then for any (supported) input tensor the following\\nequality holds:  fold(unfold(input))==divisor*input  where divisor is a tensor that depends only on the shape\\nand dtype of the input :  >>>input_ones=torch.ones(input.shape,dtype=input.dtype)>>>divisor=fold(unfold(input_ones))  When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each\\nother (up to constant divisor).  Warning  Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.  Shape:  Input:(  N  ,  C  ×  ∏  (  kernel_size  )  ,  L  )  (N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)(N,C×∏(kernel_size),L)or(  C  ×  ∏  (  kernel_size  )  ,  L  )  (C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)(C×∏(kernel_size),L)  Output:(  N  ,  C  ,  output_size  [  0  ]  ,  output_size  [  1  ]  ,  …  )  (N, C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)(N,C,output_size[0],output_size[1],…)or(  C  ,  output_size  [  0  ]  ,  output_size  [  1  ]  ,  …  )  (C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)(C,output_size[0],output_size[1],…)as described above  Examples:  >>>fold=nn.Fold(output_size=(4,5),kernel_size=(2,2))>>>input=torch.randn(1,3*2*2,12)>>>output=fold(input)>>>output.size()torch.Size([1, 3, 4, 5])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#fold\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#fold', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/fold.html#Fold', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/fold.py#L11', 'https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold', 'https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a0'), 'title': 'nn.MaxPool1d', 'page_text': 'MaxPool1d [LINK_1]  class torch.nn.MaxPool1d( kernel_size , stride=None , padding=0 , dilation=1 , return_indices=False , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 1D max pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)and output(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)can be precisely described as:  o  u  t  (  N  i  ,  C  j  ,  k  )  =  max  \\u2061  m  =  0  ,  …  ,  kernel_size  −  1  i  n  p  u  t  (  N  i  ,  C  j  ,  s  t  r  i  d  e  ×  k  +  m  )  out(N_i, C_j, k) = \\\\max_{m=0, \\\\ldots, \\\\text{kernel\\\\_size} - 1}\\n        input(N_i, C_j, stride \\\\times k + m)out(Ni\\u200b,Cj\\u200b,k)=m=0,…,kernel_size−1max\\u200binput(Ni\\u200b,Cj\\u200b,stride×k+m)  If padding is non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor padding number of points. dilation is the stride between the elements within the\\nsliding window. This link has a nice visualization of the pooling parameters.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – The size of the sliding window, must be > 0.  stride ( Union  [  int  ,  Tuple  [  int  ]  ] ) – The stride of the sliding window, must be > 0. Default value is kernel_size .  padding ( Union  [  int  ,  Tuple  [  int  ]  ] ) – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.  dilation ( Union  [  int  ,  Tuple  [  int  ]  ] ) – The stride between elements within a sliding window, must be > 0.  return_indices ( bool ) – If True , will return the argmax along with the max values.\\nUseful for torch.nn.MaxUnpool1d later  ceil_mode ( bool ) – If True , will use ceil instead of floor to compute the output shape. This\\nensures that every element in the input tensor is covered by a sliding window.  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), where  L  o  u  t  =  ⌊  L  i  n  +  2  ×  padding  −  dilation  ×  (  kernel_size  −  1  )  −  1  stride  +  1  ⌋  L_{out} = \\\\left\\\\lfloor \\\\frac{L_{in} + 2 \\\\times \\\\text{padding} - \\\\text{dilation}\\n      \\\\times (\\\\text{kernel\\\\_size} - 1) - 1}{\\\\text{stride}} + 1\\\\right\\\\rfloorLout\\u200b=⌊strideLin\\u200b+2×padding−dilation×(kernel_size−1)−1\\u200b+1⌋  Examples:  >>># pool of size=3, stride=2>>>m=nn.MaxPool1d(3,stride=2)>>>input=torch.randn(20,16,50)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#maxpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#maxpool1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L81', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Fold.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a1'), 'title': 'nn.MaxPool2d', 'page_text': 'MaxPool2d [LINK_1]  class torch.nn.MaxPool2d( kernel_size , stride=None , padding=0 , dilation=1 , return_indices=False , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 2D max pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W),\\noutput(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)and kernel_size (  k  H  ,  k  W  )  (kH, kW)(kH,kW)can be precisely described as:  o  u  t  (  N  i  ,  C  j  ,  h  ,  w  )  =  max  \\u2061  m  =  0  ,  …  ,  k  H  −  1  max  \\u2061  n  =  0  ,  …  ,  k  W  −  1  input  (  N  i  ,  C  j  ,  stride[0]  ×  h  +  m  ,  stride[1]  ×  w  +  n  )  \\\\begin{aligned}\\n    out(N_i, C_j, h, w) ={} & \\\\max_{m=0, \\\\ldots, kH-1} \\\\max_{n=0, \\\\ldots, kW-1} \\\\\\\\\\n                            & \\\\text{input}(N_i, C_j, \\\\text{stride[0]} \\\\times h + m,\\n                                           \\\\text{stride[1]} \\\\times w + n)\\n\\\\end{aligned}out(Ni\\u200b,Cj\\u200b,h,w)=\\u200bm=0,…,kH−1max\\u200bn=0,…,kW−1max\\u200binput(Ni\\u200b,Cj\\u200b,stride[0]×h+m,stride[1]×w+n)\\u200b  If padding is non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor padding number of points. dilation controls the spacing between the kernel points.\\nIt is harder to describe, but this link has a nice visualization of what dilation does.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride , padding , dilation can either be:  a single int – in which case the same value is used for the height and width dimension  a tuple of two ints – in which case, the first int is used for the height dimension,\\nand the second int for the width dimension  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the size of the window to take a max over  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – Implicit negative infinity padding to be added on both sides  dilation ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – a parameter that controls the stride of elements in the window  return_indices ( bool ) – if True , will return the max indices along with the outputs.\\nUseful for torch.nn.MaxUnpool2d later  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  ⌊  H  i  n  +  2  ∗  padding[0]  −  dilation[0]  ×  (  kernel_size[0]  −  1  )  −  1  stride[0]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 * \\\\text{padding[0]} - \\\\text{dilation[0]}\\n      \\\\times (\\\\text{kernel\\\\_size[0]} - 1) - 1}{\\\\text{stride[0]}} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[0]Hin\\u200b+2∗padding[0]−dilation[0]×(kernel_size[0]−1)−1\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ∗  padding[1]  −  dilation[1]  ×  (  kernel_size[1]  −  1  )  −  1  stride[1]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 * \\\\text{padding[1]} - \\\\text{dilation[1]}\\n      \\\\times (\\\\text{kernel\\\\_size[1]} - 1) - 1}{\\\\text{stride[1]}} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[1]Win\\u200b+2∗padding[1]−dilation[1]×(kernel_size[1]−1)−1\\u200b+1⌋  Examples:  >>># pool of square window of size=3, stride=2>>>m=nn.MaxPool2d(3,stride=2)>>># pool of non-square window>>>m=nn.MaxPool2d((3,2),stride=(2,1))>>>input=torch.randn(20,16,50,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#maxpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#maxpool2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L145', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a2'), 'title': 'nn.MaxPool3d', 'page_text': 'MaxPool3d [LINK_1]  class torch.nn.MaxPool3d( kernel_size , stride=None , padding=0 , dilation=1 , return_indices=False , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 3D max pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W),\\noutput(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)and kernel_size (  k  D  ,  k  H  ,  k  W  )  (kD, kH, kW)(kD,kH,kW)can be precisely described as:  out  (  N  i  ,  C  j  ,  d  ,  h  ,  w  )  =  max  \\u2061  k  =  0  ,  …  ,  k  D  −  1  max  \\u2061  m  =  0  ,  …  ,  k  H  −  1  max  \\u2061  n  =  0  ,  …  ,  k  W  −  1  input  (  N  i  ,  C  j  ,  stride[0]  ×  d  +  k  ,  stride[1]  ×  h  +  m  ,  stride[2]  ×  w  +  n  )  \\\\begin{aligned}\\n    \\\\text{out}(N_i, C_j, d, h, w) ={} & \\\\max_{k=0, \\\\ldots, kD-1} \\\\max_{m=0, \\\\ldots, kH-1} \\\\max_{n=0, \\\\ldots, kW-1} \\\\\\\\\\n                                      & \\\\text{input}(N_i, C_j, \\\\text{stride[0]} \\\\times d + k,\\n                                                     \\\\text{stride[1]} \\\\times h + m, \\\\text{stride[2]} \\\\times w + n)\\n\\\\end{aligned}out(Ni\\u200b,Cj\\u200b,d,h,w)=\\u200bk=0,…,kD−1max\\u200bm=0,…,kH−1max\\u200bn=0,…,kW−1max\\u200binput(Ni\\u200b,Cj\\u200b,stride[0]×d+k,stride[1]×h+m,stride[2]×w+n)\\u200b  If padding is non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor padding number of points. dilation controls the spacing between the kernel points.\\nIt is harder to describe, but this link has a nice visualization of what dilation does.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride , padding , dilation can either be:  a single int – in which case the same value is used for the depth, height and width dimension  a tuple of three ints – in which case, the first int is used for the depth dimension,\\nthe second int for the height dimension and the third int for the width dimension  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the size of the window to take a max over  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – Implicit negative infinity padding to be added on all three sides  dilation ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – a parameter that controls the stride of elements in the window  return_indices ( bool ) – if True , will return the max indices along with the outputs.\\nUseful for torch.nn.MaxUnpool3d later  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  ⌊  D  i  n  +  2  ×  padding  [  0  ]  −  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  −  1  stride  [  0  ]  +  1  ⌋  D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0] \\\\times\\n  (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorDout\\u200b=⌊stride[0]Din\\u200b+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1\\u200b+1⌋  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  1  ]  −  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  −  1  stride  [  1  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1] \\\\times\\n  (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[1]Hin\\u200b+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  2  ]  −  dilation  [  2  ]  ×  (  kernel_size  [  2  ]  −  1  )  −  1  stride  [  2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] - \\\\text{dilation}[2] \\\\times\\n  (\\\\text{kernel\\\\_size}[2] - 1) - 1}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[2]Win\\u200b+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1\\u200b+1⌋  Examples:  >>># pool of square window of size=3, stride=2>>>m=nn.MaxPool3d(3,stride=2)>>># pool of non-square window>>>m=nn.MaxPool3d((3,2,2),stride=(2,1,2))>>>input=torch.randn(20,16,50,44,31)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#maxpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#maxpool3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L224', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d', 'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a3'), 'title': 'nn.MaxUnpool1d', 'page_text': 'MaxUnpool1d [LINK_1]  class torch.nn.MaxUnpool1d( kernel_size , stride=None , padding=0 ) [source]  [source]  [LINK_2]  Computes a partial inverse of MaxPool1d .  MaxPool1d is not fully invertible, since the non-maximal values are lost.  MaxUnpool1d takes in as input the output of MaxPool1d including the indices of the maximal values and computes a partial inverse\\nin which all non-maximal values are set to zero.  Note  This operation may behave nondeterministically when the input indices has repeat values.\\nSee https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.  Note  MaxPool1d can map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument output_size in the forward call.\\nSee the Inputs and Example below.  Parameters  kernel_size ( int  or  tuple ) – Size of the max pooling window.  stride ( int  or  tuple ) – Stride of the max pooling window.\\nIt is set to kernel_size by default.  padding ( int  or  tuple ) – Padding that was added to the input  Inputs:  input : the input Tensor to invert  indices : the indices given out by MaxPool1d  output_size (optional): the targeted output size  Shape:  Input:(  N  ,  C  ,  H  i  n  )  (N, C, H_{in})(N,C,Hin\\u200b)or(  C  ,  H  i  n  )  (C, H_{in})(C,Hin\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  )  (N, C, H_{out})(N,C,Hout\\u200b)or(  C  ,  H  o  u  t  )  (C, H_{out})(C,Hout\\u200b), where  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride  [  0  ]  −  2  ×  padding  [  0  ]  +  kernel_size  [  0  ]  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{kernel\\\\_size}[0]Hout\\u200b=(Hin\\u200b−1)×stride[0]−2×padding[0]+kernel_size[0]  or as given by output_size in the call operator  Example:  >>>pool=nn.MaxPool1d(2,stride=2,return_indices=True)>>>unpool=nn.MaxUnpool1d(2,stride=2)>>>input=torch.tensor([[[1.,2,3,4,5,6,7,8]]])>>>output,indices=pool(input)>>>unpool(output,indices)tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])>>># Example showcasing the use of output_size>>>input=torch.tensor([[[1.,2,3,4,5,6,7,8,9]]])>>>output,indices=pool(input)>>>unpool(output,indices,output_size=input.size())tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])>>>unpool(output,indices)tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#maxunpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#maxunpool1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxUnpool1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L312', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d', 'https://github.com/pytorch/pytorch/issues/80827', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a4'), 'title': 'nn.MaxUnpool2d', 'page_text': 'MaxUnpool2d [LINK_1]  class torch.nn.MaxUnpool2d( kernel_size , stride=None , padding=0 ) [source]  [source]  [LINK_2]  Computes a partial inverse of MaxPool2d .  MaxPool2d is not fully invertible, since the non-maximal values are lost.  MaxUnpool2d takes in as input the output of MaxPool2d including the indices of the maximal values and computes a partial inverse\\nin which all non-maximal values are set to zero.  Note  This operation may behave nondeterministically when the input indices has repeat values.\\nSee https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.  Note  MaxPool2d can map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument output_size in the forward call.\\nSee the Inputs and Example below.  Parameters  kernel_size ( int  or  tuple ) – Size of the max pooling window.  stride ( int  or  tuple ) – Stride of the max pooling window.\\nIt is set to kernel_size by default.  padding ( int  or  tuple ) – Padding that was added to the input  Inputs:  input : the input Tensor to invert  indices : the indices given out by MaxPool2d  output_size (optional): the targeted output size  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride[0]  −  2  ×  padding[0]  +  kernel_size[0]  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride[0]} - 2 \\\\times \\\\text{padding[0]} + \\\\text{kernel\\\\_size[0]}Hout\\u200b=(Hin\\u200b−1)×stride[0]−2×padding[0]+kernel_size[0]  W  o  u  t  =  (  W  i  n  −  1  )  ×  stride[1]  −  2  ×  padding[1]  +  kernel_size[1]  W_{out} = (W_{in} - 1) \\\\times \\\\text{stride[1]} - 2 \\\\times \\\\text{padding[1]} + \\\\text{kernel\\\\_size[1]}Wout\\u200b=(Win\\u200b−1)×stride[1]−2×padding[1]+kernel_size[1]  or as given by output_size in the call operator  Example:  >>>pool=nn.MaxPool2d(2,stride=2,return_indices=True)>>>unpool=nn.MaxUnpool2d(2,stride=2)>>>input=torch.tensor([[[[1.,2.,3.,4.],[ 5.,  6.,  7.,  8.],[ 9., 10., 11., 12.],[13., 14., 15., 16.]]]])>>>output,indices=pool(input)>>>unpool(output,indices)tensor([[[[  0.,   0.,   0.,   0.],[  0.,   6.,   0.,   8.],[  0.,   0.,   0.,   0.],[  0.,  14.,   0.,  16.]]]])>>># Now using output_size to resolve an ambiguous size for the inverse>>>input=torch.tensor([[[[1.,2.,3.,4.,5.],[ 6.,  7.,  8.,  9., 10.],[11., 12., 13., 14., 15.],[16., 17., 18., 19., 20.]]]])>>>output,indices=pool(input)>>># This call will not work without specifying output_size>>>unpool(output,indices,output_size=input.size())tensor([[[[ 0.,  0.,  0.,  0.,  0.],[ 0.,  7.,  0.,  9.,  0.],[ 0.,  0.,  0.,  0.,  0.],[ 0., 17.,  0., 19.,  0.]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#maxunpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#maxunpool2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxUnpool2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L394', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d', 'https://github.com/pytorch/pytorch/issues/80827', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a5'), 'title': 'nn.MaxUnpool3d', 'page_text': 'MaxUnpool3d [LINK_1]  class torch.nn.MaxUnpool3d( kernel_size , stride=None , padding=0 ) [source]  [source]  [LINK_2]  Computes a partial inverse of MaxPool3d .  MaxPool3d is not fully invertible, since the non-maximal values are lost. MaxUnpool3d takes in as input the output of MaxPool3d including the indices of the maximal values and computes a partial inverse\\nin which all non-maximal values are set to zero.  Note  This operation may behave nondeterministically when the input indices has repeat values.\\nSee https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.  Note  MaxPool3d can map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument output_size in the forward call.\\nSee the Inputs section below.  Parameters  kernel_size ( int  or  tuple ) – Size of the max pooling window.  stride ( int  or  tuple ) – Stride of the max pooling window.\\nIt is set to kernel_size by default.  padding ( int  or  tuple ) – Padding that was added to the input  Inputs:  input : the input Tensor to invert  indices : the indices given out by MaxPool3d  output_size (optional): the targeted output size  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  (  D  i  n  −  1  )  ×  stride[0]  −  2  ×  padding[0]  +  kernel_size[0]  D_{out} = (D_{in} - 1) \\\\times \\\\text{stride[0]} - 2 \\\\times \\\\text{padding[0]} + \\\\text{kernel\\\\_size[0]}Dout\\u200b=(Din\\u200b−1)×stride[0]−2×padding[0]+kernel_size[0]  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride[1]  −  2  ×  padding[1]  +  kernel_size[1]  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride[1]} - 2 \\\\times \\\\text{padding[1]} + \\\\text{kernel\\\\_size[1]}Hout\\u200b=(Hin\\u200b−1)×stride[1]−2×padding[1]+kernel_size[1]  W  o  u  t  =  (  W  i  n  −  1  )  ×  stride[2]  −  2  ×  padding[2]  +  kernel_size[2]  W_{out} = (W_{in} - 1) \\\\times \\\\text{stride[2]} - 2 \\\\times \\\\text{padding[2]} + \\\\text{kernel\\\\_size[2]}Wout\\u200b=(Win\\u200b−1)×stride[2]−2×padding[2]+kernel_size[2]  or as given by output_size in the call operator  Example:  >>># pool of square window of size=3, stride=2>>>pool=nn.MaxPool3d(3,stride=2,return_indices=True)>>>unpool=nn.MaxUnpool3d(3,stride=2)>>>output,indices=pool(torch.randn(20,16,51,33,15))>>>unpooled_output=unpool(output,indices)>>>unpooled_output.size()torch.Size([20, 16, 51, 33, 15])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#maxunpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#maxunpool3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxUnpool3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L489', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d', 'https://github.com/pytorch/pytorch/issues/80827', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a6'), 'title': 'nn.AvgPool1d', 'page_text': 'AvgPool1d [LINK_1]  class torch.nn.AvgPool1d( kernel_size , stride=None , padding=0 , ceil_mode=False , count_include_pad=True ) [source]  [source]  [LINK_2]  Applies a 1D average pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  L  )  (N, C, L)(N,C,L),\\noutput(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)and kernel_size k  kkcan be precisely described as:  out  (  N  i  ,  C  j  ,  l  )  =  1  k  ∑  m  =  0  k  −  1  input  (  N  i  ,  C  j  ,  stride  ×  l  +  m  )  \\\\text{out}(N_i, C_j, l) = \\\\frac{1}{k} \\\\sum_{m=0}^{k-1}\\n                       \\\\text{input}(N_i, C_j, \\\\text{stride} \\\\times l + m)out(Ni\\u200b,Cj\\u200b,l)=k1\\u200bm=0∑k−1\\u200binput(Ni\\u200b,Cj\\u200b,stride×l+m)  If padding is non-zero, then the input is implicitly zero-padded on both sides\\nfor padding number of points.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride , padding can each be\\nan int or a one-element tuple.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ]  ] ) – implicit zero padding to be added on both sides  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  count_include_pad ( bool ) – when True, will include the zero-padding in the averaging calculation  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), where  L  o  u  t  =  ⌊  L  i  n  +  2  ×  padding  −  kernel_size  stride  +  1  ⌋  L_{out} = \\\\left\\\\lfloor \\\\frac{L_{in} +\\n2 \\\\times \\\\text{padding} - \\\\text{kernel\\\\_size}}{\\\\text{stride}} + 1\\\\right\\\\rfloorLout\\u200b=⌊strideLin\\u200b+2×padding−kernel_size\\u200b+1⌋  Per the note above, if ceil_mode is True and(  L  o  u  t  −  1  )  ×  stride  ≥  L  i  n  +  padding  (L_{out} - 1) \\\\times \\\\text{stride} \\\\geq L_{in}\\n+ \\\\text{padding}(Lout\\u200b−1)×stride≥Lin\\u200b+padding, we skip the last window as it would start in the right padded region, resulting inL  o  u  t  L_{out}Lout\\u200bbeing reduced by one.  Examples:  >>># pool with window of size=3, stride=2>>>m=nn.AvgPool1d(3,stride=2)>>>m(torch.tensor([[[1.,2,3,4,5,6,7]]]))tensor([[[2., 4., 6.]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html#avgpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html#avgpool1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L580', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a7'), 'title': 'nn.AvgPool2d', 'page_text': 'AvgPool2d [LINK_1]  class torch.nn.AvgPool2d( kernel_size , stride=None , padding=0 , ceil_mode=False , count_include_pad=True , divisor_override=None ) [source]  [source]  [LINK_2]  Applies a 2D average pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W),\\noutput(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)and kernel_size (  k  H  ,  k  W  )  (kH, kW)(kH,kW)can be precisely described as:  o  u  t  (  N  i  ,  C  j  ,  h  ,  w  )  =  1  k  H  ∗  k  W  ∑  m  =  0  k  H  −  1  ∑  n  =  0  k  W  −  1  i  n  p  u  t  (  N  i  ,  C  j  ,  s  t  r  i  d  e  [  0  ]  ×  h  +  m  ,  s  t  r  i  d  e  [  1  ]  ×  w  +  n  )  out(N_i, C_j, h, w)  = \\\\frac{1}{kH * kW} \\\\sum_{m=0}^{kH-1} \\\\sum_{n=0}^{kW-1}\\n                       input(N_i, C_j, stride[0] \\\\times h + m, stride[1] \\\\times w + n)out(Ni\\u200b,Cj\\u200b,h,w)=kH∗kW1\\u200bm=0∑kH−1\\u200bn=0∑kW−1\\u200binput(Ni\\u200b,Cj\\u200b,stride[0]×h+m,stride[1]×w+n)  If padding is non-zero, then the input is implicitly zero-padded on both sides\\nfor padding number of points.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride , padding can either be:  a single int – in which case the same value is used for the height and width dimension  a tuple of two ints – in which case, the first int is used for the height dimension,\\nand the second int for the width dimension  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – implicit zero padding to be added on both sides  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  count_include_pad ( bool ) – when True, will include the zero-padding in the averaging calculation  divisor_override ( Optional  [  int  ] ) – if specified, it will be used as divisor, otherwise size of the pooling region will be used.  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  0  ]  −  kernel_size  [  0  ]  stride  [  0  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times \\\\text{padding}[0] -\\n  \\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[0]Hin\\u200b+2×padding[0]−kernel_size[0]\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  1  ]  −  kernel_size  [  1  ]  stride  [  1  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times \\\\text{padding}[1] -\\n  \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[1]Win\\u200b+2×padding[1]−kernel_size[1]\\u200b+1⌋  Per the note above, if ceil_mode is True and(  H  o  u  t  −  1  )  ×  stride  [  0  ]  ≥  H  i  n  +  padding  [  0  ]  (H_{out} - 1)\\\\times \\\\text{stride}[0]\\\\geq H_{in}\\n+ \\\\text{padding}[0](Hout\\u200b−1)×stride[0]≥Hin\\u200b+padding[0], we skip the last window as it would start in the bottom padded region,\\nresulting inH  o  u  t  H_{out}Hout\\u200bbeing reduced by one.  The same applies forW  o  u  t  W_{out}Wout\\u200b.  Examples:  >>># pool of square window of size=3, stride=2>>>m=nn.AvgPool2d(3,stride=2)>>># pool of non-square window>>>m=nn.AvgPool2d((3,2),stride=(2,1))>>>input=torch.randn(20,16,50,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#avgpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#avgpool2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L661', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a8'), 'title': 'nn.AvgPool3d', 'page_text': 'AvgPool3d [LINK_1]  class torch.nn.AvgPool3d( kernel_size , stride=None , padding=0 , ceil_mode=False , count_include_pad=True , divisor_override=None ) [source]  [source]  [LINK_2]  Applies a 3D average pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W),\\noutput(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)and kernel_size (  k  D  ,  k  H  ,  k  W  )  (kD, kH, kW)(kD,kH,kW)can be precisely described as:  out  (  N  i  ,  C  j  ,  d  ,  h  ,  w  )  =  ∑  k  =  0  k  D  −  1  ∑  m  =  0  k  H  −  1  ∑  n  =  0  k  W  −  1  input  (  N  i  ,  C  j  ,  stride  [  0  ]  ×  d  +  k  ,  stride  [  1  ]  ×  h  +  m  ,  stride  [  2  ]  ×  w  +  n  )  k  D  ×  k  H  ×  k  W  \\\\begin{aligned}\\n    \\\\text{out}(N_i, C_j, d, h, w) ={} & \\\\sum_{k=0}^{kD-1} \\\\sum_{m=0}^{kH-1} \\\\sum_{n=0}^{kW-1} \\\\\\\\\\n                                      & \\\\frac{\\\\text{input}(N_i, C_j, \\\\text{stride}[0] \\\\times d + k,\\n                                              \\\\text{stride}[1] \\\\times h + m, \\\\text{stride}[2] \\\\times w + n)}\\n                                             {kD \\\\times kH \\\\times kW}\\n\\\\end{aligned}out(Ni\\u200b,Cj\\u200b,d,h,w)=\\u200bk=0∑kD−1\\u200bm=0∑kH−1\\u200bn=0∑kW−1\\u200bkD×kH×kWinput(Ni\\u200b,Cj\\u200b,stride[0]×d+k,stride[1]×h+m,stride[2]×w+n)\\u200b\\u200b  If padding is non-zero, then the input is implicitly zero-padded on all three sides\\nfor padding number of points.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride can either be:  a single int – in which case the same value is used for the depth, height and width dimension  a tuple of three ints – in which case, the first int is used for the depth dimension,\\nthe second int for the height dimension and the third int for the width dimension  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – implicit zero padding to be added on all three sides  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  count_include_pad ( bool ) – when True, will include the zero-padding in the averaging calculation  divisor_override ( Optional  [  int  ] ) – if specified, it will be used as divisor, otherwise kernel_size will be used  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  ⌊  D  i  n  +  2  ×  padding  [  0  ]  −  kernel_size  [  0  ]  stride  [  0  ]  +  1  ⌋  D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] -\\n      \\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorDout\\u200b=⌊stride[0]Din\\u200b+2×padding[0]−kernel_size[0]\\u200b+1⌋  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  1  ]  −  kernel_size  [  1  ]  stride  [  1  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] -\\n      \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[1]Hin\\u200b+2×padding[1]−kernel_size[1]\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  2  ]  −  kernel_size  [  2  ]  stride  [  2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] -\\n      \\\\text{kernel\\\\_size}[2]}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[2]Win\\u200b+2×padding[2]−kernel_size[2]\\u200b+1⌋  Per the note above, if ceil_mode is True and(  D  o  u  t  −  1  )  ×  stride  [  0  ]  ≥  D  i  n  +  padding  [  0  ]  (D_{out} - 1)\\\\times \\\\text{stride}[0]\\\\geq D_{in}\\n+ \\\\text{padding}[0](Dout\\u200b−1)×stride[0]≥Din\\u200b+padding[0], we skip the last window as it would start in the padded region,\\nresulting inD  o  u  t  D_{out}Dout\\u200bbeing reduced by one.  The same applies forW  o  u  t  W_{out}Wout\\u200bandH  o  u  t  H_{out}Hout\\u200b.  Examples:  >>># pool of square window of size=3, stride=2>>>m=nn.AvgPool3d(3,stride=2)>>># pool of non-square window>>>m=nn.AvgPool3d((3,2,2),stride=(2,1,2))>>>input=torch.randn(20,16,50,44,31)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html#avgpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html#avgpool3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L767', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179a9'), 'title': 'nn.FractionalMaxPool2d', 'page_text': 'FractionalMaxPool2d [LINK_1]  class torch.nn.FractionalMaxPool2d( kernel_size , output_size=None , output_ratio=None , return_indices=False , _random_samples=None ) [source]  [source]  [LINK_2]  Applies a 2D fractional max pooling over an input signal composed of several input planes.  Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham  The max-pooling operation is applied ink  H  ×  k  W  kH \\\\times kWkH×kWregions by a stochastic\\nstep size determined by the target output size.\\nThe number of output features is equal to the number of input planes.  Note  Exactly one of output_size or output_ratio must be defined.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the size of the window to take a max over.\\nCan be a single number k (for a square kernel of k x k) or a tuple (kh, kw)  output_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the target output size of the image of the form oH x oW .\\nCan be a tuple (oH, oW) or a single number oH for a square image oH x oH .\\nNote that we must havek  H  +  o  H  −  1  <  =  H  i  n  kH + oH - 1 <= H_{in}kH+oH−1<=Hin\\u200bandk  W  +  o  W  −  1  <  =  W  i  n  kW + oW - 1 <= W_{in}kW+oW−1<=Win\\u200b  output_ratio ( Union  [  float  ,  Tuple  [  float  ,  float  ]  ] ) – If one wants to have an output size as a ratio of the input size, this option can be given.\\nThis has to be a number or tuple in the range (0, 1).\\nNote that we must havek  H  +  (  o  u  t  p  u  t  _  r  a  t  i  o  _  H  ∗  H  i  n  )  −  1  <  =  H  i  n  kH + (output\\\\_ratio\\\\_H * H_{in}) - 1 <= H_{in}kH+(output_ratio_H∗Hin\\u200b)−1<=Hin\\u200bandk  W  +  (  o  u  t  p  u  t  _  r  a  t  i  o  _  W  ∗  W  i  n  )  −  1  <  =  W  i  n  kW + (output\\\\_ratio\\\\_W * W_{in}) - 1 <= W_{in}kW+(output_ratio_W∗Win\\u200b)−1<=Win\\u200b  return_indices ( bool ) – if True , will return the indices along with the outputs.\\nUseful to pass to nn.MaxUnpool2d() . Default: False  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where(  H  o  u  t  ,  W  o  u  t  )  =  output_size  (H_{out}, W_{out})=\\\\text{output\\\\_size}(Hout\\u200b,Wout\\u200b)=output_sizeor(  H  o  u  t  ,  W  o  u  t  )  =  output_ratio  ×  (  H  i  n  ,  W  i  n  )  (H_{out}, W_{out})=\\\\text{output\\\\_ratio} \\\\times (H_{in}, W_{in})(Hout\\u200b,Wout\\u200b)=output_ratio×(Hin\\u200b,Win\\u200b).  Examples  >>># pool of square window of size=3, and target output size 13x12>>>m=nn.FractionalMaxPool2d(3,output_size=(13,12))>>># pool of square window and target output size being half of input image size>>>m=nn.FractionalMaxPool2d(3,output_ratio=(0.5,0.5))>>>input=torch.randn(20,16,50,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html#fractionalmaxpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html#fractionalmaxpool2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#FractionalMaxPool2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L886', 'https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d', 'https://arxiv.org/abs/1412.6071', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179aa'), 'title': 'nn.FractionalMaxPool3d', 'page_text': 'FractionalMaxPool3d [LINK_1]  class torch.nn.FractionalMaxPool3d( kernel_size , output_size=None , output_ratio=None , return_indices=False , _random_samples=None ) [source]  [source]  [LINK_2]  Applies a 3D fractional max pooling over an input signal composed of several input planes.  Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham  The max-pooling operation is applied ink  T  ×  k  H  ×  k  W  kT \\\\times kH \\\\times kWkT×kH×kWregions by a stochastic\\nstep size determined by the target output size.\\nThe number of output features is equal to the number of input planes.  Note  Exactly one of output_size or output_ratio must be defined.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the size of the window to take a max over.\\nCan be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)  output_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the target output size of the image of the form oT x oH x oW .\\nCan be a tuple (oT, oH, oW) or a single number oH for a square image oH x oH x oH  output_ratio ( Union  [  float  ,  Tuple  [  float  ,  float  ,  float  ]  ] ) – If one wants to have an output size as a ratio of the input size, this option can be given.\\nThis has to be a number or tuple in the range (0, 1)  return_indices ( bool ) – if True , will return the indices along with the outputs.\\nUseful to pass to nn.MaxUnpool3d() . Default: False  Shape:  Input:(  N  ,  C  ,  T  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, T_{in}, H_{in}, W_{in})(N,C,Tin\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  T  i  n  ,  H  i  n  ,  W  i  n  )  (C, T_{in}, H_{in}, W_{in})(C,Tin\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  T  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, T_{out}, H_{out}, W_{out})(N,C,Tout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  T  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, T_{out}, H_{out}, W_{out})(C,Tout\\u200b,Hout\\u200b,Wout\\u200b), where(  T  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  =  output_size  (T_{out}, H_{out}, W_{out})=\\\\text{output\\\\_size}(Tout\\u200b,Hout\\u200b,Wout\\u200b)=output_sizeor(  T  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  =  output_ratio  ×  (  T  i  n  ,  H  i  n  ,  W  i  n  )  (T_{out}, H_{out}, W_{out})=\\\\text{output\\\\_ratio} \\\\times (T_{in}, H_{in}, W_{in})(Tout\\u200b,Hout\\u200b,Wout\\u200b)=output_ratio×(Tin\\u200b,Hin\\u200b,Win\\u200b)  Examples  >>># pool of cubic window of size=3, and target output size 13x12x11>>>m=nn.FractionalMaxPool3d(3,output_size=(13,12,11))>>># pool of cubic window and target output size being half of input size>>>m=nn.FractionalMaxPool3d(3,output_ratio=(0.5,0.5,0.5))>>>input=torch.randn(20,16,50,32,16)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html#fractionalmaxpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html#fractionalmaxpool3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#FractionalMaxPool3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L975', 'https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d', 'https://arxiv.org/abs/1412.6071', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ab'), 'title': 'nn.LPPool1d', 'page_text': 'LPPool1d [LINK_1]  class torch.nn.LPPool1d( norm_type , kernel_size , stride=None , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 1D power-average pooling over an input signal composed of several input planes.  On each window, the function computed is:  f  (  X  )  =  ∑  x  ∈  X  x  p  p  f(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}f(X)=p\\u200bx∈X∑\\u200bxp\\u200b  At p =∞  \\\\infty∞, one gets Max Pooling  At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)  Note  If the sum to the power of p is zero, the gradient of this function is\\nnot defined. This implementation will set the gradient to zero in this case.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – a single int, the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ]  ] ) – a single int, the stride of the window. Default value is kernel_size  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), where  L  o  u  t  =  ⌊  L  i  n  −  kernel_size  stride  +  1  ⌋  L_{out} = \\\\left\\\\lfloor\\\\frac{L_{in} - \\\\text{kernel\\\\_size}}{\\\\text{stride}} + 1\\\\right\\\\rfloorLout\\u200b=⌊strideLin\\u200b−kernel_size\\u200b+1⌋  Examples::  >>># power-2 pool of window of length 3, with stride 2.>>>m=nn.LPPool1d(2,3,stride=2)>>>input=torch.randn(20,16,50)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html#lppool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html#lppool1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#LPPool1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1090', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ac'), 'title': 'nn.LPPool2d', 'page_text': 'LPPool2d [LINK_1]  class torch.nn.LPPool2d( norm_type , kernel_size , stride=None , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 2D power-average pooling over an input signal composed of several input planes.  On each window, the function computed is:  f  (  X  )  =  ∑  x  ∈  X  x  p  p  f(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}f(X)=p\\u200bx∈X∑\\u200bxp\\u200b  At p =∞  \\\\infty∞, one gets Max Pooling  At p = 1, one gets Sum Pooling (which is proportional to average pooling)  The parameters kernel_size , stride can either be:  a single int – in which case the same value is used for the height and width dimension  a tuple of two ints – in which case, the first int is used for the height dimension,\\nand the second int for the width dimension  Note  If the sum to the power of p is zero, the gradient of this function is\\nnot defined. This implementation will set the gradient to zero in this case.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  ⌊  H  i  n  −  kernel_size  [  0  ]  stride  [  0  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} - \\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[0]Hin\\u200b−kernel_size[0]\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  −  kernel_size  [  1  ]  stride  [  1  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} - \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[1]Win\\u200b−kernel_size[1]\\u200b+1⌋  Examples:  >>># power-2 pool of square window of size=3, stride=2>>>m=nn.LPPool2d(2,3,stride=2)>>># pool of non-square window of power 1.2>>>m=nn.LPPool2d(1.2,(3,2),stride=(2,1))>>>input=torch.randn(20,16,50,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html#lppool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html#lppool2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#LPPool2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1132', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ad'), 'title': 'nn.LPPool3d', 'page_text': 'LPPool3d [LINK_1]  class torch.nn.LPPool3d( norm_type , kernel_size , stride=None , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 3D power-average pooling over an input signal composed of several input planes.  On each window, the function computed is:  f  (  X  )  =  ∑  x  ∈  X  x  p  p  f(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}f(X)=p\\u200bx∈X∑\\u200bxp\\u200b  At p =∞  \\\\infty∞, one gets Max Pooling  At p = 1, one gets Sum Pooling (which is proportional to average pooling)  The parameters kernel_size , stride can either be:  a single int – in which case the same value is used for the height, width and depth dimension  a tuple of three ints – in which case, the first int is used for the depth dimension,\\nthe second int for the height dimension and the third int for the width dimension  Note  If the sum to the power of p is zero, the gradient of this function is\\nnot defined. This implementation will set the gradient to zero in this case.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  ⌊  D  i  n  −  kernel_size  [  0  ]  stride  [  0  ]  +  1  ⌋  D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} - \\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorDout\\u200b=⌊stride[0]Din\\u200b−kernel_size[0]\\u200b+1⌋  H  o  u  t  =  ⌊  H  i  n  −  kernel_size  [  1  ]  stride  [  1  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} - \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[1]Hin\\u200b−kernel_size[1]\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  −  kernel_size  [  2  ]  stride  [  2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} - \\\\text{kernel\\\\_size}[2]}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[2]Win\\u200b−kernel_size[2]\\u200b+1⌋  Examples:  >>># power-2 pool of square window of size=3, stride=2>>>m=nn.LPPool3d(2,3,stride=2)>>># pool of non-square window of power 1.2>>>m=nn.LPPool3d(1.2,(3,2,2),stride=(2,1,2))>>>input=torch.randn(20,16,50,44,31)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html#lppool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html#torch.nn.LPPool3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html#lppool3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#LPPool3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1187', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html#torch.nn.LPPool3d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ae'), 'title': 'nn.AdaptiveMaxPool1d', 'page_text': 'AdaptiveMaxPool1d [LINK_1]  class torch.nn.AdaptiveMaxPool1d( output_size , return_indices=False ) [source]  [source]  [LINK_2]  Applies a 1D adaptive max pooling over an input signal composed of several input planes.  The output size isL  o  u  t  L_{out}Lout\\u200b, for any input size.\\nThe number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – the target output sizeL  o  u  t  L_{out}Lout\\u200b.  return_indices ( bool ) – if True , will return the indices along with the outputs.\\nUseful to pass to nn.MaxUnpool1d. Default: False  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), whereL  o  u  t  =  output_size  L_{out}=\\\\text{output\\\\_size}Lout\\u200b=output_size.  Examples  >>># target output size of 5>>>m=nn.AdaptiveMaxPool1d(5)>>>input=torch.randn(1,64,8)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html#adaptivemaxpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html#adaptivemaxpool1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1265', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179af'), 'title': 'nn.AdaptiveMaxPool2d', 'page_text': 'AdaptiveMaxPool2d [LINK_1]  class torch.nn.AdaptiveMaxPool2d( output_size , return_indices=False ) [source]  [source]  [LINK_2]  Applies a 2D adaptive max pooling over an input signal composed of several input planes.  The output is of sizeH  o  u  t  ×  W  o  u  t  H_{out} \\\\times W_{out}Hout\\u200b×Wout\\u200b, for any input size.\\nThe number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  None  ,  Tuple  [  Optional  [  int  ]  ,  Optional  [  int  ]  ]  ] ) – the target output size of the image of the formH  o  u  t  ×  W  o  u  t  H_{out} \\\\times W_{out}Hout\\u200b×Wout\\u200b.\\nCan be a tuple(  H  o  u  t  ,  W  o  u  t  )  (H_{out}, W_{out})(Hout\\u200b,Wout\\u200b)or a singleH  o  u  t  H_{out}Hout\\u200bfor a\\nsquare imageH  o  u  t  ×  H  o  u  t  H_{out} \\\\times H_{out}Hout\\u200b×Hout\\u200b.H  o  u  t  H_{out}Hout\\u200bandW  o  u  t  W_{out}Wout\\u200bcan be either a int , or None which means the size will be the same as that\\nof the input.  return_indices ( bool ) – if True , will return the indices along with the outputs.\\nUseful to pass to nn.MaxUnpool2d. Default: False  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where(  H  o  u  t  ,  W  o  u  t  )  =  output_size  (H_{out}, W_{out})=\\\\text{output\\\\_size}(Hout\\u200b,Wout\\u200b)=output_size.  Examples  >>># target output size of 5x7>>>m=nn.AdaptiveMaxPool2d((5,7))>>>input=torch.randn(1,64,8,9)>>>output=m(input)>>># target output size of 7x7 (square)>>>m=nn.AdaptiveMaxPool2d(7)>>>input=torch.randn(1,64,10,9)>>>output=m(input)>>># target output size of 10x7>>>m=nn.AdaptiveMaxPool2d((None,7))>>>input=torch.randn(1,64,10,9)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#adaptivemaxpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#adaptivemaxpool2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1295', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b0'), 'title': 'nn.AdaptiveMaxPool3d', 'page_text': 'AdaptiveMaxPool3d [LINK_1]  class torch.nn.AdaptiveMaxPool3d( output_size , return_indices=False ) [source]  [source]  [LINK_2]  Applies a 3D adaptive max pooling over an input signal composed of several input planes.  The output is of sizeD  o  u  t  ×  H  o  u  t  ×  W  o  u  t  D_{out} \\\\times H_{out} \\\\times W_{out}Dout\\u200b×Hout\\u200b×Wout\\u200b, for any input size.\\nThe number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  None  ,  Tuple  [  Optional  [  int  ]  ,  Optional  [  int  ]  ,  Optional  [  int  ]  ]  ] ) – the target output size of the image of the formD  o  u  t  ×  H  o  u  t  ×  W  o  u  t  D_{out} \\\\times H_{out} \\\\times W_{out}Dout\\u200b×Hout\\u200b×Wout\\u200b.\\nCan be a tuple(  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (D_{out}, H_{out}, W_{out})(Dout\\u200b,Hout\\u200b,Wout\\u200b)or a singleD  o  u  t  D_{out}Dout\\u200bfor a cubeD  o  u  t  ×  D  o  u  t  ×  D  o  u  t  D_{out} \\\\times D_{out} \\\\times D_{out}Dout\\u200b×Dout\\u200b×Dout\\u200b.D  o  u  t  D_{out}Dout\\u200b,H  o  u  t  H_{out}Hout\\u200bandW  o  u  t  W_{out}Wout\\u200bcan be either a int , or None which means the size will be the same as that of the input.  return_indices ( bool ) – if True , will return the indices along with the outputs.\\nUseful to pass to nn.MaxUnpool3d. Default: False  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b),\\nwhere(  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  =  output_size  (D_{out}, H_{out}, W_{out})=\\\\text{output\\\\_size}(Dout\\u200b,Hout\\u200b,Wout\\u200b)=output_size.  Examples  >>># target output size of 5x7x9>>>m=nn.AdaptiveMaxPool3d((5,7,9))>>>input=torch.randn(1,64,8,9,10)>>>output=m(input)>>># target output size of 7x7x7 (cube)>>>m=nn.AdaptiveMaxPool3d(7)>>>input=torch.randn(1,64,10,9,8)>>>output=m(input)>>># target output size of 7x9x8>>>m=nn.AdaptiveMaxPool3d((7,None,None))>>>input=torch.randn(1,64,10,9,8)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html#adaptivemaxpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html#adaptivemaxpool3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1337', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b1'), 'title': 'nn.AdaptiveAvgPool1d', 'page_text': 'AdaptiveAvgPool1d [LINK_1]  class torch.nn.AdaptiveAvgPool1d( output_size ) [source]  [source]  [LINK_2]  Applies a 1D adaptive average pooling over an input signal composed of several input planes.  The output size isL  o  u  t  L_{out}Lout\\u200b, for any input size.\\nThe number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – the target output sizeL  o  u  t  L_{out}Lout\\u200b.  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), whereL  o  u  t  =  output_size  L_{out}=\\\\text{output\\\\_size}Lout\\u200b=output_size.  Examples  >>># target output size of 5>>>m=nn.AdaptiveAvgPool1d(5)>>>input=torch.randn(1,64,8)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html#adaptiveavgpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html#adaptiveavgpool1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1391', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b2'), 'title': 'nn.AdaptiveAvgPool2d', 'page_text': 'AdaptiveAvgPool2d [LINK_1]  class torch.nn.AdaptiveAvgPool2d( output_size ) [source]  [source]  [LINK_2]  Applies a 2D adaptive average pooling over an input signal composed of several input planes.  The output is of size H x W, for any input size.\\nThe number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  None  ,  Tuple  [  Optional  [  int  ]  ,  Optional  [  int  ]  ]  ] ) – the target output size of the image of the form H x W.\\nCan be a tuple (H, W) or a single H for a square image H x H.\\nH and W can be either a int , or None which means the size will\\nbe the same as that of the input.  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  S  0  ,  S  1  )  (N, C, S_{0}, S_{1})(N,C,S0\\u200b,S1\\u200b)or(  C  ,  S  0  ,  S  1  )  (C, S_{0}, S_{1})(C,S0\\u200b,S1\\u200b), whereS  =  output_size  S=\\\\text{output\\\\_size}S=output_size.  Examples  >>># target output size of 5x7>>>m=nn.AdaptiveAvgPool2d((5,7))>>>input=torch.randn(1,64,8,9)>>>output=m(input)>>># target output size of 7x7 (square)>>>m=nn.AdaptiveAvgPool2d(7)>>>input=torch.randn(1,64,10,9)>>>output=m(input)>>># target output size of 10x7>>>m=nn.AdaptiveAvgPool2d((None,7))>>>input=torch.randn(1,64,10,9)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#adaptiveavgpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#adaptiveavgpool2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1419', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b3'), 'title': 'nn.AdaptiveAvgPool3d', 'page_text': 'AdaptiveAvgPool3d [LINK_1]  class torch.nn.AdaptiveAvgPool3d( output_size ) [source]  [source]  [LINK_2]  Applies a 3D adaptive average pooling over an input signal composed of several input planes.  The output is of size D x H x W, for any input size.\\nThe number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  None  ,  Tuple  [  Optional  [  int  ]  ,  Optional  [  int  ]  ,  Optional  [  int  ]  ]  ] ) – the target output size of the form D x H x W.\\nCan be a tuple (D, H, W) or a single number D for a cube D x D x D.\\nD, H and W can be either a int , or None which means the size will\\nbe the same as that of the input.  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  S  0  ,  S  1  ,  S  2  )  (N, C, S_{0}, S_{1}, S_{2})(N,C,S0\\u200b,S1\\u200b,S2\\u200b)or(  C  ,  S  0  ,  S  1  ,  S  2  )  (C, S_{0}, S_{1}, S_{2})(C,S0\\u200b,S1\\u200b,S2\\u200b),\\nwhereS  =  output_size  S=\\\\text{output\\\\_size}S=output_size.  Examples  >>># target output size of 5x7x9>>>m=nn.AdaptiveAvgPool3d((5,7,9))>>>input=torch.randn(1,64,8,9,10)>>>output=m(input)>>># target output size of 7x7x7 (cube)>>>m=nn.AdaptiveAvgPool3d(7)>>>input=torch.randn(1,64,10,9,8)>>>output=m(input)>>># target output size of 7x9x8>>>m=nn.AdaptiveAvgPool3d((7,None,None))>>>input=torch.randn(1,64,10,9,8)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html#adaptiveavgpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html#adaptiveavgpool3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pooling.py#L1458', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b4'), 'title': 'nn.ReflectionPad1d', 'page_text': 'ReflectionPad1d [LINK_1]  class torch.nn.ReflectionPad1d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using the reflection of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 2- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ReflectionPad1d(2)>>>input=torch.arange(8,dtype=torch.float).reshape(1,2,4)>>>inputtensor([[[0., 1., 2., 3.],[4., 5., 6., 7.]]])>>>m(input)tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],[6., 5., 4., 5., 6., 7., 6., 5.]]])>>># using different paddings for different sides>>>m=nn.ReflectionPad1d((3,1))>>>m(input)tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],[7., 6., 5., 4., 5., 6., 7., 6.]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html#reflectionpad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html#reflectionpad1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReflectionPad1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L374', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b5'), 'title': 'nn.ReflectionPad2d', 'page_text': 'ReflectionPad2d [LINK_1]  class torch.nn.ReflectionPad2d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using the reflection of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)\\nNote that padding size should be less than the corresponding input dimension.  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b)where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ReflectionPad2d(2)>>>input=torch.arange(9,dtype=torch.float).reshape(1,1,3,3)>>>inputtensor([[[[0., 1., 2.],[3., 4., 5.],[6., 7., 8.]]]])>>>m(input)tensor([[[[8., 7., 6., 7., 8., 7., 6.],[5., 4., 3., 4., 5., 4., 3.],[2., 1., 0., 1., 2., 1., 0.],[5., 4., 3., 4., 5., 4., 3.],[8., 7., 6., 7., 8., 7., 6.],[5., 4., 3., 4., 5., 4., 3.],[2., 1., 0., 1., 2., 1., 0.]]]])>>># using different paddings for different sides>>>m=nn.ReflectionPad2d((1,1,2,0))>>>m(input)tensor([[[[7., 6., 7., 8., 7.],[4., 3., 4., 5., 4.],[1., 0., 1., 2., 1.],[4., 3., 4., 5., 4.],[7., 6., 7., 8., 7.]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html#reflectionpad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html#reflectionpad2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReflectionPad2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L415', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b6'), 'title': 'nn.ReflectionPad3d', 'page_text': 'ReflectionPad3d [LINK_1]  class torch.nn.ReflectionPad3d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using the reflection of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b),\\nwhere  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ReflectionPad3d(1)>>>input=torch.arange(8,dtype=torch.float).reshape(1,1,2,2,2)>>>m(input)tensor([[[[[7., 6., 7., 6.],[5., 4., 5., 4.],[7., 6., 7., 6.],[5., 4., 5., 4.]],[[3., 2., 3., 2.],[1., 0., 1., 0.],[3., 2., 3., 2.],[1., 0., 1., 0.]],[[7., 6., 7., 6.],[5., 4., 5., 4.],[7., 6., 7., 6.],[5., 4., 5., 4.]],[[3., 2., 3., 2.],[1., 0., 1., 0.],[3., 2., 3., 2.],[1., 0., 1., 0.]]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html#reflectionpad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html#reflectionpad3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReflectionPad3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L468', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b7'), 'title': 'nn.ReplicationPad1d', 'page_text': 'ReplicationPad1d [LINK_1]  class torch.nn.ReplicationPad1d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using replication of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 2- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ReplicationPad1d(2)>>>input=torch.arange(8,dtype=torch.float).reshape(1,2,4)>>>inputtensor([[[0., 1., 2., 3.],[4., 5., 6., 7.]]])>>>m(input)tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],[4., 4., 4., 5., 6., 7., 7., 7.]]])>>># using different paddings for different sides>>>m=nn.ReplicationPad1d((3,1))>>>m(input)tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],[4., 4., 4., 4., 5., 6., 7., 7.]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html#replicationpad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html#replicationpad1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReplicationPad1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L533', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b8'), 'title': 'nn.ReplicationPad2d', 'page_text': 'ReplicationPad2d [LINK_1]  class torch.nn.ReplicationPad2d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using replication of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ReplicationPad2d(2)>>>input=torch.arange(9,dtype=torch.float).reshape(1,1,3,3)>>>inputtensor([[[[0., 1., 2.],[3., 4., 5.],[6., 7., 8.]]]])>>>m(input)tensor([[[[0., 0., 0., 1., 2., 2., 2.],[0., 0., 0., 1., 2., 2., 2.],[0., 0., 0., 1., 2., 2., 2.],[3., 3., 3., 4., 5., 5., 5.],[6., 6., 6., 7., 8., 8., 8.],[6., 6., 6., 7., 8., 8., 8.],[6., 6., 6., 7., 8., 8., 8.]]]])>>># using different paddings for different sides>>>m=nn.ReplicationPad2d((1,1,2,0))>>>m(input)tensor([[[[0., 0., 1., 2., 2.],[0., 0., 1., 2., 2.],[0., 0., 1., 2., 2.],[3., 3., 4., 5., 5.],[6., 6., 7., 8., 8.]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#replicationpad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#replicationpad2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReplicationPad2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L574', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179b9'), 'title': 'nn.ReplicationPad3d', 'page_text': 'ReplicationPad3d [LINK_1]  class torch.nn.ReplicationPad3d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using replication of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b),\\nwhere  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ReplicationPad3d(3)>>>input=torch.randn(16,3,8,320,480)>>>output=m(input)>>># using different paddings for different sides>>>m=nn.ReplicationPad3d((3,3,6,6,1,1))>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html#replicationpad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html#replicationpad3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReplicationPad3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L626', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ba'), 'title': 'nn.ZeroPad1d', 'page_text': 'ZeroPad1d [LINK_1]  class torch.nn.ZeroPad1d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with zero.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in both boundaries. If a 2- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ZeroPad1d(2)>>>input=torch.randn(1,2,4)>>>inputtensor([[[-1.0491, -0.7152, -0.0749,  0.8530],[-1.3287,  1.8966,  0.1466, -0.2771]]])>>>m(input)tensor([[[ 0.0000,  0.0000, -1.0491, -0.7152, -0.0749,  0.8530,  0.0000,0.0000],[ 0.0000,  0.0000, -1.3287,  1.8966,  0.1466, -0.2771,  0.0000,0.0000]]])>>>m=nn.ZeroPad1d(2)>>>input=torch.randn(1,2,3)>>>inputtensor([[[ 1.6616,  1.4523, -1.1255],[-3.6372,  0.1182, -1.8652]]])>>>m(input)tensor([[[ 0.0000,  0.0000,  1.6616,  1.4523, -1.1255,  0.0000,  0.0000],[ 0.0000,  0.0000, -3.6372,  0.1182, -1.8652,  0.0000,  0.0000]]])>>># using different paddings for different sides>>>m=nn.ZeroPad1d((3,1))>>>m(input)tensor([[[ 0.0000,  0.0000,  0.0000,  1.6616,  1.4523, -1.1255,  0.0000],[ 0.0000,  0.0000,  0.0000, -3.6372,  0.1182, -1.8652,  0.0000]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html#zeropad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html#zeropad1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ZeroPad1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L667', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179bb'), 'title': 'nn.ZeroPad2d', 'page_text': 'ZeroPad2d [LINK_1]  class torch.nn.ZeroPad2d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with zero.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ZeroPad2d(2)>>>input=torch.randn(1,1,3,3)>>>inputtensor([[[[-0.1678, -0.4418,  1.9466],[ 0.9604, -0.4219, -0.5241],[-0.9162, -0.5436, -0.6446]]]])>>>m(input)tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],[ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])>>># using different paddings for different sides>>>m=nn.ZeroPad2d((1,1,2,0))>>>m(input)tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],[ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],[ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html#zeropad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html#zeropad2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ZeroPad2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L720', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179bc'), 'title': 'nn.ZeroPad3d', 'page_text': 'ZeroPad3d [LINK_1]  class torch.nn.ZeroPad3d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with zero.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ZeroPad3d(3)>>>input=torch.randn(16,3,10,20,30)>>>output=m(input)>>># using different paddings for different sides>>>m=nn.ZeroPad3d((3,3,6,6,0,1))>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html#zeropad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html#zeropad3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ZeroPad3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L774', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179bd'), 'title': 'nn.ConstantPad1d', 'page_text': 'ConstantPad1d [LINK_1]  class torch.nn.ConstantPad1d( padding , value ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with a constant value.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in both boundaries. If a 2- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ConstantPad1d(2,3.5)>>>input=torch.randn(1,2,4)>>>inputtensor([[[-1.0491, -0.7152, -0.0749,  0.8530],[-1.3287,  1.8966,  0.1466, -0.2771]]])>>>m(input)tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,3.5000],[ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,3.5000]]])>>>m=nn.ConstantPad1d(2,3.5)>>>input=torch.randn(1,2,3)>>>inputtensor([[[ 1.6616,  1.4523, -1.1255],[-3.6372,  0.1182, -1.8652]]])>>>m(input)tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],[ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])>>># using different paddings for different sides>>>m=nn.ConstantPad1d((3,1),3.5)>>>m(input)tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],[ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html#constantpad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html#constantpad1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L221', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179be'), 'title': 'nn.ConstantPad2d', 'page_text': 'ConstantPad2d [LINK_1]  class torch.nn.ConstantPad2d( padding , value ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with a constant value.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ConstantPad2d(2,3.5)>>>input=torch.randn(1,2,2)>>>inputtensor([[[ 1.6585,  0.4320],[-0.8701, -0.4649]]])>>>m(input)tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],[ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])>>># using different paddings for different sides>>>m=nn.ConstantPad2d((3,0,2,1),3.5)>>>m(input)tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],[ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html#constantpad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html#constantpad2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L272', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179bf'), 'title': 'nn.ConstantPad3d', 'page_text': 'ConstantPad3d [LINK_1]  class torch.nn.ConstantPad3d( padding , value ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with a constant value.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ConstantPad3d(3,3.5)>>>input=torch.randn(16,3,10,20,30)>>>output=m(input)>>># using different paddings for different sides>>>m=nn.ConstantPad3d((3,3,6,6,0,1),3.5)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html#constantpad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html#constantpad3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L323', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c0'), 'title': 'nn.CircularPad1d', 'page_text': 'CircularPad1d [LINK_1]  class torch.nn.CircularPad1d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using circular padding of the input boundary.  Tensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is\\napplied then the ends of the tensor get removed.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 2- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.CircularPad1d(2)>>>input=torch.arange(8,dtype=torch.float).reshape(1,2,4)>>>inputtensor([[[0., 1., 2., 3.],[4., 5., 6., 7.]]])>>>m(input)tensor([[[2., 3., 0., 1., 2., 3., 0., 1.],[6., 7., 4., 5., 6., 7., 4., 5.]]])>>># using different paddings for different sides>>>m=nn.CircularPad1d((3,1))>>>m(input)tensor([[[1., 2., 3., 0., 1., 2., 3., 0.],[5., 6., 7., 4., 5., 6., 7., 4.]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html#circularpad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html#torch.nn.CircularPad1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html#circularpad1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#CircularPad1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L48', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html#torch.nn.CircularPad1d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c1'), 'title': 'nn.CircularPad2d', 'page_text': 'CircularPad2d [LINK_1]  class torch.nn.CircularPad2d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using circular padding of the input boundary.  Tensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is\\napplied then the ends of the tensor get removed.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.CircularPad2d(2)>>>input=torch.arange(9,dtype=torch.float).reshape(1,1,3,3)>>>inputtensor([[[[0., 1., 2.],[3., 4., 5.],[6., 7., 8.]]]])>>>m(input)tensor([[[[4., 5., 3., 4., 5., 3., 4.],[7., 8., 6., 7., 8., 6., 7.],[1., 2., 0., 1., 2., 0., 1.],[4., 5., 3., 4., 5., 3., 4.],[7., 8., 6., 7., 8., 6., 7.],[1., 2., 0., 1., 2., 0., 1.],[4., 5., 3., 4., 5., 3., 4.]]]])>>># using different paddings for different sides>>>m=nn.CircularPad2d((1,1,2,0))>>>m(input)tensor([[[[5., 3., 4., 5., 3.],[8., 6., 7., 8., 6.],[2., 0., 1., 2., 0.],[5., 3., 4., 5., 3.],[8., 6., 7., 8., 6.]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html#circularpad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html#torch.nn.CircularPad2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html#circularpad2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#CircularPad2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L97', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html#torch.nn.CircularPad2d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c2'), 'title': 'nn.CircularPad3d', 'page_text': 'CircularPad3d [LINK_1]  class torch.nn.CircularPad3d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using circular padding of the input boundary.  Tensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is\\napplied then the ends of the tensor get removed.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses\\n(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b),\\nwhere  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.CircularPad3d(3)>>>input=torch.randn(16,3,8,320,480)>>>output=m(input)>>># using different paddings for different sides>>>m=nn.CircularPad3d((3,3,6,6,1,1))>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html#circularpad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html#torch.nn.CircularPad3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html#circularpad3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#CircularPad3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/padding.py#L156', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html#torch.nn.CircularPad3d', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.ELU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c3'), 'title': 'nn.ELU', 'page_text': 'ELU [LINK_1]  class torch.nn.ELU( alpha=1.0 , inplace=False ) [source]  [source]  [LINK_2]  Applies the Exponential Linear Unit (ELU) function, element-wise.  Method described in the paper: Fast and Accurate Deep Network Learning by Exponential Linear\\nUnits (ELUs) .  ELU is defined as:  ELU  (  x  )  =  {  x  ,  if  x  >  0  α  ∗  (  exp  \\u2061  (  x  )  −  1  )  ,  if  x  ≤  0  \\\\text{ELU}(x) = \\\\begin{cases}\\nx, & \\\\text{ if } x > 0\\\\\\\\\\n\\\\alpha * (\\\\exp(x) - 1), & \\\\text{ if } x \\\\leq 0\\n\\\\end{cases}ELU(x)={x,α∗(exp(x)−1),\\u200bifx>0ifx≤0\\u200b  Parameters  alpha ( float ) – theα  \\\\alphaαvalue for the ELU formulation. Default: 1.0  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.ELU()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#elu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#elu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#ELU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L520', 'https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU', 'https://arxiv.org/abs/1511.07289', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html', 'https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c4'), 'title': 'nn.Hardshrink', 'page_text': 'Hardshrink [LINK_1]  class torch.nn.Hardshrink( lambd=0.5 ) [source]  [source]  [LINK_2]  Applies the Hard Shrinkage (Hardshrink) function element-wise.  Hardshrink is defined as:  HardShrink  (  x  )  =  {  x  ,  if  x  >  λ  x  ,  if  x  <  −  λ  0  ,  otherwise  \\\\text{HardShrink}(x) =\\n\\\\begin{cases}\\nx, & \\\\text{ if } x > \\\\lambda \\\\\\\\\\nx, & \\\\text{ if } x < -\\\\lambda \\\\\\\\\\n0, & \\\\text{ otherwise }\\n\\\\end{cases}HardShrink(x)=⎩⎨⎧\\u200bx,x,0,\\u200bifx>λifx<−λotherwise\\u200b  Parameters  lambd ( float ) – theλ  \\\\lambdaλvalue for the Hardshrink formulation. Default: 0.5  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Hardshrink()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html#hardshrink\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html#hardshrink', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Hardshrink', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L740', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ELU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c5'), 'title': 'nn.Hardsigmoid', 'page_text': 'Hardsigmoid [LINK_1]  class torch.nn.Hardsigmoid( inplace=False ) [source]  [source]  [LINK_2]  Applies the Hardsigmoid function element-wise.  Hardsigmoid is defined as:  Hardsigmoid  (  x  )  =  {  0  if  x  ≤  −  3  ,  1  if  x  ≥  +  3  ,  x  /  6  +  1  /  2  otherwise  \\\\text{Hardsigmoid}(x) = \\\\begin{cases}\\n    0 & \\\\text{if~} x \\\\le -3, \\\\\\\\\\n    1 & \\\\text{if~} x \\\\ge +3, \\\\\\\\\\n    x / 6 + 1 / 2 & \\\\text{otherwise}\\n\\\\end{cases}Hardsigmoid(x)=⎩⎨⎧\\u200b01x/6+1/2\\u200bifx≤−3,ifx≥+3,otherwise\\u200b  Parameters  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Hardsigmoid()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html#hardsigmoid\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html#hardsigmoid', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Hardsigmoid', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L330', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c6'), 'title': 'nn.Hardtanh', 'page_text': 'Hardtanh [LINK_1]  class torch.nn.Hardtanh( min_val=-1.0 , max_val=1.0 , inplace=False , min_value=None , max_value=None ) [source]  [source]  [LINK_2]  Applies the HardTanh function element-wise.  HardTanh is defined as:  HardTanh  (  x  )  =  {  max_val  if  x  >  max_val  min_val  if  x  <  min_val  x  otherwise  \\\\text{HardTanh}(x) = \\\\begin{cases}\\n    \\\\text{max\\\\_val} & \\\\text{ if } x > \\\\text{ max\\\\_val } \\\\\\\\\\n    \\\\text{min\\\\_val} & \\\\text{ if } x < \\\\text{ min\\\\_val } \\\\\\\\\\n    x & \\\\text{ otherwise } \\\\\\\\\\n\\\\end{cases}HardTanh(x)=⎩⎨⎧\\u200bmax_valmin_valx\\u200bifx>max_valifx<min_valotherwise\\u200b  Parameters  min_val ( float ) – minimum value of the linear region range. Default: -1  max_val ( float ) – maximum value of the linear region range. Default: 1  inplace ( bool ) – can optionally do the operation in-place. Default: False  Keyword arguments min_value and max_value have been deprecated in favor of min_val and max_val .  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Hardtanh(-2,2)>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html#hardtanh\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html#hardtanh', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Hardtanh', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L200', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c7'), 'title': 'nn.Hardswish', 'page_text': 'Hardswish [LINK_1]  class torch.nn.Hardswish( inplace=False ) [source]  [source]  [LINK_2]  Applies the Hardswish function, element-wise.  Method described in the paper: Searching for MobileNetV3 .  Hardswish is defined as:  Hardswish  (  x  )  =  {  0  if  x  ≤  −  3  ,  x  if  x  ≥  +  3  ,  x  ⋅  (  x  +  3  )  /  6  otherwise  \\\\text{Hardswish}(x) = \\\\begin{cases}\\n    0 & \\\\text{if~} x \\\\le -3, \\\\\\\\\\n    x & \\\\text{if~} x \\\\ge +3, \\\\\\\\\\n    x \\\\cdot (x + 3) /6 & \\\\text{otherwise}\\n\\\\end{cases}Hardswish(x)=⎩⎨⎧\\u200b0xx⋅(x+3)/6\\u200bifx≤−3,ifx≥+3,otherwise\\u200b  Parameters  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Hardswish()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html#hardswish\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html#torch.nn.Hardswish', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html#hardswish', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Hardswish', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L478', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html#torch.nn.Hardswish', 'https://arxiv.org/abs/1905.02244', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c8'), 'title': 'nn.LeakyReLU', 'page_text': 'LeakyReLU [LINK_1]  class torch.nn.LeakyReLU( negative_slope=0.01 , inplace=False ) [source]  [source]  [LINK_2]  Applies the LeakyReLU function element-wise.  LeakyReLU  (  x  )  =  max  \\u2061  (  0  ,  x  )  +  negative_slope  ∗  min  \\u2061  (  0  ,  x  )  \\\\text{LeakyReLU}(x) = \\\\max(0, x) + \\\\text{negative\\\\_slope} * \\\\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)  or  LeakyReLU  (  x  )  =  {  x  ,  if  x  ≥  0  negative_slope  ×  x  ,  otherwise  \\\\text{LeakyReLU}(x) =\\n\\\\begin{cases}\\nx, & \\\\text{ if } x \\\\geq 0 \\\\\\\\\\n\\\\text{negative\\\\_slope} \\\\times x, & \\\\text{ otherwise }\\n\\\\end{cases}LeakyReLU(x)={x,negative_slope×x,\\u200bifx≥0otherwise\\u200b  Parameters  negative_slope ( float ) – Controls the angle of the negative slope (which is used for\\nnegative input values). Default: 1e-2  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗)where * means, any number of additional\\ndimensions  Output:(  ∗  )  (*)(∗), same shape as the input  Examples:  >>>m=nn.LeakyReLU(0.1)>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#leakyrelu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#leakyrelu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#LeakyReLU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L783', 'https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179c9'), 'title': 'nn.LogSigmoid', 'page_text': 'LogSigmoid [LINK_1]  class torch.nn.LogSigmoid( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the Logsigmoid function element-wise.  LogSigmoid  (  x  )  =  log  \\u2061  (  1  1  +  exp  \\u2061  (  −  x  )  )  \\\\text{LogSigmoid}(x) = \\\\log\\\\left(\\\\frac{ 1 }{ 1 + \\\\exp(-x)}\\\\right)LogSigmoid(x)=log(1+exp(−x)1\\u200b)  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.LogSigmoid()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#logsigmoid\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#logsigmoid', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#LogSigmoid', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L835', 'https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ca'), 'title': 'nn.MultiheadAttention', 'page_text': 'MultiheadAttention [LINK_1]  class torch.nn.MultiheadAttention( embed_dim , num_heads , dropout=0.0 , bias=True , add_bias_kv=False , add_zero_attn=False , kdim=None , vdim=None , batch_first=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Allows the model to jointly attend to information from different representation subspaces.  Note  See this tutorial for an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.  Method described in the paper: Attention Is All You Need .  Multi-Head Attention is defined as:  MultiHead  (  Q  ,  K  ,  V  )  =  Concat  (  head  1  ,  …  ,  head  h  )  W  O  \\\\text{MultiHead}(Q, K, V) = \\\\text{Concat}(\\\\text{head}_1,\\\\dots,\\\\text{head}_h)W^OMultiHead(Q,K,V)=Concat(head1\\u200b,…,headh\\u200b)WO  wherehead  i  =  Attention  (  Q  W  i  Q  ,  K  W  i  K  ,  V  W  i  V  )  \\\\text{head}_i = \\\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)headi\\u200b=Attention(QWiQ\\u200b,KWiK\\u200b,VWiV\\u200b).  nn.MultiheadAttention will use the optimized implementations of scaled_dot_product_attention() when possible.  In addition to support for the new scaled_dot_product_attention() function, for speeding up Inference, MHA will use\\nfastpath inference with support for Nested Tensors, iff:  self attention is being computed (i.e., query , key , and value are the same tensor).  inputs are batched (3D) with batch_first==True  Either autograd is disabled (using torch.inference_mode or torch.no_grad ) or no tensor argument requires_grad  training is disabled (using .eval() )  add_bias_kv is False  add_zero_attn is False  kdim and vdim are equal to embed_dim  if a NestedTensor is passed, neither key_padding_mask nor attn_mask is passed  autocast is disabled  If the optimized inference fastpath implementation is in use, a NestedTensor can be passed for query / key / value to represent padding more efficiently than using a\\npadding mask. In this case, a NestedTensor will be returned, and an additional speedup proportional to the fraction of the input\\nthat is padding can be expected.  Parameters  embed_dim – Total dimension of the model.  num_heads – Number of parallel attention heads. Note that embed_dim will be split\\nacross num_heads (i.e. each head will have dimension embed_dim//num_heads ).  dropout – Dropout probability on attn_output_weights . Default: 0.0 (no dropout).  bias – If specified, adds bias to input / output projection layers. Default: True .  add_bias_kv – If specified, adds bias to the key and value sequences at dim=0. Default: False .  add_zero_attn – If specified, adds a new batch of zeros to the key and value sequences at dim=1.\\nDefault: False .  kdim – Total number of features for keys. Default: None (uses kdim=embed_dim ).  vdim – Total number of features for values. Default: None (uses vdim=embed_dim ).  batch_first – If True , then the input and output tensors are provided\\nas (batch, seq, feature). Default: False (seq, batch, feature).  Examples:  >>>multihead_attn=nn.MultiheadAttention(embed_dim,num_heads)>>>attn_output,attn_output_weights=multihead_attn(query,key,value)  forward( query , key , value , key_padding_mask=None , need_weights=True , attn_mask=None , average_attn_weights=True , is_causal=False ) [source]  [source]  [LINK_3]  Compute attention outputs using query, key, and value embeddings.  Supports optional parameters for padding, masks and attention weights.  Parameters  query ( Tensor ) – Query embeddings of shape(  L  ,  E  q  )  (L, E_q)(L,Eq\\u200b)for unbatched input,(  L  ,  N  ,  E  q  )  (L, N, E_q)(L,N,Eq\\u200b)when batch_first=False or(  N  ,  L  ,  E  q  )  (N, L, E_q)(N,L,Eq\\u200b)when batch_first=True , whereL  LLis the target sequence length,N  NNis the batch size, andE  q  E_qEq\\u200bis the query embedding dimension embed_dim .\\nQueries are compared against key-value pairs to produce the output.\\nSee “Attention Is All You Need” for more details.  key ( Tensor ) – Key embeddings of shape(  S  ,  E  k  )  (S, E_k)(S,Ek\\u200b)for unbatched input,(  S  ,  N  ,  E  k  )  (S, N, E_k)(S,N,Ek\\u200b)when batch_first=False or(  N  ,  S  ,  E  k  )  (N, S, E_k)(N,S,Ek\\u200b)when batch_first=True , whereS  SSis the source sequence length,N  NNis the batch size, andE  k  E_kEk\\u200bis the key embedding dimension kdim .\\nSee “Attention Is All You Need” for more details.  value ( Tensor ) – Value embeddings of shape(  S  ,  E  v  )  (S, E_v)(S,Ev\\u200b)for unbatched input,(  S  ,  N  ,  E  v  )  (S, N, E_v)(S,N,Ev\\u200b)when batch_first=False or(  N  ,  S  ,  E  v  )  (N, S, E_v)(N,S,Ev\\u200b)when batch_first=True , whereS  SSis the source\\nsequence length,N  NNis the batch size, andE  v  E_vEv\\u200bis the value embedding dimension vdim .\\nSee “Attention Is All You Need” for more details.  key_padding_mask ( Optional  [  Tensor  ] ) – If specified, a mask of shape(  N  ,  S  )  (N, S)(N,S)indicating which elements within key to ignore for the purpose of attention (i.e. treat as “padding”). For unbatched query , shape should be(  S  )  (S)(S).\\nBinary and float masks are supported.\\nFor a binary mask, a True value indicates that the corresponding key value will be ignored for\\nthe purpose of attention. For a float mask, it will be directly added to the corresponding key value.  need_weights ( bool ) – If specified, returns attn_output_weights in addition to attn_outputs .\\nSet need_weights=False to use the optimized scaled_dot_product_attention and achieve the best performance for MHA.\\nDefault: True .  attn_mask ( Optional  [  Tensor  ] ) – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape(  L  ,  S  )  (L, S)(L,S)or(  N  ⋅  num_heads  ,  L  ,  S  )  (N\\\\cdot\\\\text{num\\\\_heads}, L, S)(N⋅num_heads,L,S), whereN  NNis the batch size,L  LLis the target sequence length, andS  SSis the source sequence length. A 2D mask will be\\nbroadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\\nBinary and float masks are supported. For a binary mask, a True value indicates that the\\ncorresponding position is not allowed to attend. For a float mask, the mask values will be added to\\nthe attention weight.\\nIf both attn_mask and key_padding_mask are supplied, their types should match.  average_attn_weights ( bool ) – If true, indicates that the returned attn_weights should be averaged across\\nheads. Otherwise, attn_weights are provided separately per head. Note that this flag only has an\\neffect when need_weights=True . Default: True (i.e. average weights across heads)  is_causal ( bool ) – If specified, applies a causal mask as attention mask.\\nDefault: False .\\nWarning: is_causal provides a hint that attn_mask is the\\ncausal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.  Return type  Tuple [ Tensor , Optional [ Tensor ]]  Outputs:  attn_output - Attention outputs of shape(  L  ,  E  )  (L, E)(L,E)when input is unbatched,(  L  ,  N  ,  E  )  (L, N, E)(L,N,E)when batch_first=False or(  N  ,  L  ,  E  )  (N, L, E)(N,L,E)when batch_first=True ,\\nwhereL  LLis the target sequence length,N  NNis the batch size, andE  EEis the\\nembedding dimension embed_dim .  attn_output_weights - Only returned when need_weights=True . If average_attn_weights=True ,\\nreturns attention weights averaged across heads of shape(  L  ,  S  )  (L, S)(L,S)when input is unbatched or(  N  ,  L  ,  S  )  (N, L, S)(N,L,S), whereN  NNis the batch size,L  LLis the target sequence length, andS  SSis the source sequence length. If average_attn_weights=False , returns attention weights per\\nhead of shape(  num_heads  ,  L  ,  S  )  (\\\\text{num\\\\_heads}, L, S)(num_heads,L,S)when input is unbatched or(  N  ,  num_heads  ,  L  ,  S  )  (N, \\\\text{num\\\\_heads}, L, S)(N,num_heads,L,S).  Note  batch_first argument is ignored for unbatched inputs.  merge_masks( attn_mask , key_padding_mask , query ) [source]  [source]  [LINK_4]  Determine mask type and combine masks if necessary.  If only one mask is provided, that mask\\nand the corresponding mask type will be returned. If both masks are provided, they will be both\\nexpanded to shape (batch_size,num_heads,seq_len,seq_len) , combined with logical or and mask type 2 will be returned\\n:param attn_mask: attention mask of shape (seq_len,seq_len) , mask type 0\\n:param key_padding_mask: padding mask of shape (batch_size,seq_len) , mask type 1\\n:param query: query embeddings of shape (batch_size,seq_len,embed_dim)  Returns  merged mask\\nmask_type: merged mask type (0, 1, or 2)  Return type  merged_mask\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#multiheadattention\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.merge_masks', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#multiheadattention', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L973', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention', 'https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html', 'https://arxiv.org/abs/1706.03762', 'https://pytorch.org/docs/stable/nested.html', 'https://pytorch.org/docs/stable/nested.html', 'https://pytorch.org/docs/stable/nested.html', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1139', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention.merge_masks', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1399', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.merge_masks', 'https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179cb'), 'title': 'nn.PReLU', 'page_text': 'PReLU [LINK_1]  class torch.nn.PReLU( num_parameters=1 , init=0.25 , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies the element-wise PReLU function.  PReLU  (  x  )  =  max  \\u2061  (  0  ,  x  )  +  a  ∗  min  \\u2061  (  0  ,  x  )  \\\\text{PReLU}(x) = \\\\max(0,x) + a * \\\\min(0,x)PReLU(x)=max(0,x)+a∗min(0,x)  or  PReLU  (  x  )  =  {  x  ,  if  x  ≥  0  a  x  ,  otherwise  \\\\text{PReLU}(x) =\\n\\\\begin{cases}\\nx, & \\\\text{ if } x \\\\ge 0 \\\\\\\\\\nax, & \\\\text{ otherwise }\\n\\\\end{cases}PReLU(x)={x,ax,\\u200bifx≥0otherwise\\u200b  Herea  aais a learnable parameter. When called without arguments, nn.PReLU() uses a single\\nparametera  aaacross all input channels. If called with nn.PReLU(nChannels) ,\\na separatea  aais used for each input channel.  Note  weight decay should not be used when learninga  aafor good performance.  Note  Channel dim is the 2nd dim of input. When input has dims < 2, then there is\\nno channel dim and the number of channels = 1.  Parameters  num_parameters ( int ) – number ofa  aato learn.\\nAlthough it takes an int as input, there is only two values are legitimate:\\n1, or the number of channels at input. Default: 1  init ( float ) – the initial value ofa  aa. Default: 0.25  Shape:  Input:(  ∗  )  ( *)(∗)where * means, any number of additional\\ndimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Variables  weight ( Tensor ) – the learnable weights of shape ( num_parameters ).  Examples:  >>>m=nn.PReLU()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#prelu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#torch.nn.PReLU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#prelu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#PReLU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1450', 'https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#torch.nn.PReLU', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179cc'), 'title': 'nn.ReLU', 'page_text': 'ReLU [LINK_1]  class torch.nn.ReLU( inplace=False ) [source]  [source]  [LINK_2]  Applies the rectified linear unit function element-wise.  ReLU  (  x  )  =  (  x  )  +  =  max  \\u2061  (  0  ,  x  )  \\\\text{ReLU}(x) = (x)^+ = \\\\max(0, x)ReLU(x)=(x)+=max(0,x)  Parameters  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.ReLU()>>>input=torch.randn(2)>>>output=m(input)AnimplementationofCReLU-https://arxiv.org/abs/1603.05201>>>m=nn.ReLU()>>>input=torch.randn(2).unsqueeze(0)>>>output=torch.cat((m(input),m(-input)))\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#ReLU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L97', 'https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html', 'https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179cd'), 'title': 'nn.ReLU6', 'page_text': 'ReLU6 [LINK_1]  class torch.nn.ReLU6( inplace=False ) [source]  [source]  [LINK_2]  Applies the ReLU6 function element-wise.  ReLU6  (  x  )  =  min  \\u2061  (  max  \\u2061  (  0  ,  x  )  ,  6  )  \\\\text{ReLU6}(x) = \\\\min(\\\\max(0,x), 6)ReLU6(x)=min(max(0,x),6)  Parameters  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.ReLU6()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#relu6\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#torch.nn.ReLU6', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#relu6', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#ReLU6', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L276', 'https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#torch.nn.ReLU6', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ce'), 'title': 'nn.RReLU', 'page_text': 'RReLU [LINK_1]  class torch.nn.RReLU( lower=0.125 , upper=0.3333333333333333 , inplace=False ) [source]  [source]  [LINK_2]  Applies the randomized leaky rectified linear unit function, element-wise.  Method described in the paper: Empirical Evaluation of Rectified Activations in Convolutional Network .  The function is defined as:  RReLU  (  x  )  =  {  x  if  x  ≥  0  a  x  otherwise  \\\\text{RReLU}(x) =\\n\\\\begin{cases}\\n    x & \\\\text{if } x \\\\geq 0 \\\\\\\\\\n    ax & \\\\text{ otherwise }\\n\\\\end{cases}RReLU(x)={xax\\u200bifx≥0otherwise\\u200b  wherea  aais randomly sampled from uniform distributionU  (  lower  ,  upper  )  \\\\mathcal{U}(\\\\text{lower}, \\\\text{upper})U(lower,upper)during training while during\\nevaluationa  aais fixed witha  =  lower  +  upper  2  a = \\\\frac{\\\\text{lower} + \\\\text{upper}}{2}a=2lower+upper\\u200b.  Parameters  lower ( float ) – lower bound of the uniform distribution. Default:1  8  \\\\frac{1}{8}81\\u200b  upper ( float ) – upper bound of the uniform distribution. Default:1  3  \\\\frac{1}{3}31\\u200b  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.RReLU(0.1,0.3)>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#rrelu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#torch.nn.RReLU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#rrelu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#RReLU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L140', 'https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#torch.nn.RReLU', 'https://arxiv.org/abs/1505.00853', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.SELU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179cf'), 'title': 'nn.SELU', 'page_text': 'SELU [LINK_1]  class torch.nn.SELU( inplace=False ) [source]  [source]  [LINK_2]  Applies the SELU function element-wise.  SELU  (  x  )  =  scale  ∗  (  max  \\u2061  (  0  ,  x  )  +  min  \\u2061  (  0  ,  α  ∗  (  exp  \\u2061  (  x  )  −  1  )  )  )  \\\\text{SELU}(x) = \\\\text{scale} * (\\\\max(0,x) + \\\\min(0, \\\\alpha * (\\\\exp(x) - 1)))SELU(x)=scale∗(max(0,x)+min(0,α∗(exp(x)−1)))  withα  =  1.6732632423543772848170429916717  \\\\alpha = 1.6732632423543772848170429916717α=1.6732632423543772848170429916717andscale  =  1.0507009873554804934193349852946  \\\\text{scale} = 1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.  Warning  When using kaiming_normal or kaiming_normal_ for initialisation, nonlinearity=\\'linear\\' should be used instead of nonlinearity=\\'selu\\' in order to get Self-Normalizing Neural Networks .\\nSee torch.nn.init.calculate_gain() for more information.  More details can be found in the paper Self-Normalizing Neural Networks .  Parameters  inplace ( bool  ,  optional ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.SELU()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#selu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#torch.nn.SELU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#selu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#SELU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L613', 'https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#torch.nn.SELU', 'https://arxiv.org/abs/1706.02515', 'https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain', 'https://arxiv.org/abs/1706.02515', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.CELU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d0'), 'title': 'nn.CELU', 'page_text': 'CELU [LINK_1]  class torch.nn.CELU( alpha=1.0 , inplace=False ) [source]  [source]  [LINK_2]  Applies the CELU function element-wise.  CELU  (  x  )  =  max  \\u2061  (  0  ,  x  )  +  min  \\u2061  (  0  ,  α  ∗  (  exp  \\u2061  (  x  /  α  )  −  1  )  )  \\\\text{CELU}(x) = \\\\max(0,x) + \\\\min(0, \\\\alpha * (\\\\exp(x/\\\\alpha) - 1))CELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1))  More details can be found in the paper Continuously Differentiable Exponential Linear Units .  Parameters  alpha ( float ) – theα  \\\\alphaαvalue for the CELU formulation. Default: 1.0  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.CELU()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#celu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#celu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#CELU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L568', 'https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU', 'https://arxiv.org/abs/1704.07483', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.GELU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.SELU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d1'), 'title': 'nn.GELU', 'page_text': 'GELU [LINK_1]  class torch.nn.GELU( approximate=\\'none\\' ) [source]  [source]  [LINK_2]  Applies the Gaussian Error Linear Units function.  GELU  (  x  )  =  x  ∗  Φ  (  x  )  \\\\text{GELU}(x) = x * \\\\Phi(x)GELU(x)=x∗Φ(x)  whereΦ  (  x  )  \\\\Phi(x)Φ(x)is the Cumulative Distribution Function for Gaussian Distribution.  When the approximate argument is ‘tanh’, Gelu is estimated with:  GELU  (  x  )  =  0.5  ∗  x  ∗  (  1  +  Tanh  (  2  /  π  ∗  (  x  +  0.044715  ∗  x  3  )  )  )  \\\\text{GELU}(x) = 0.5 * x * (1 + \\\\text{Tanh}(\\\\sqrt{2 / \\\\pi} * (x + 0.044715 * x^3)))GELU(x)=0.5∗x∗(1+Tanh(2/π\\u200b∗(x+0.044715∗x3)))  Parameters  approximate ( str  ,  optional ) – the gelu approximation algorithm to use: \\'none\\' | \\'tanh\\' . Default: \\'none\\'  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.GELU()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#gelu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#gelu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#GELU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L698', 'https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html', 'https://pytorch.org/docs/stable/generated/torch.nn.CELU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d2'), 'title': 'nn.Sigmoid', 'page_text': 'Sigmoid [LINK_1]  class torch.nn.Sigmoid( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the Sigmoid function element-wise.  Sigmoid  (  x  )  =  σ  (  x  )  =  1  1  +  exp  \\u2061  (  −  x  )  \\\\text{Sigmoid}(x) = \\\\sigma(x) = \\\\frac{1}{1 + \\\\exp(-x)}Sigmoid(x)=σ(x)=1+exp(−x)1\\u200b  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Sigmoid()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#sigmoid\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#sigmoid', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Sigmoid', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L306', 'https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid', 'https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.GELU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d3'), 'title': 'nn.SiLU', 'page_text': 'SiLU [LINK_1]  class torch.nn.SiLU( inplace=False ) [source]  [source]  [LINK_2]  Applies the Sigmoid Linear Unit (SiLU) function, element-wise.  The SiLU function is also known as the swish function.  silu  (  x  )  =  x  ∗  σ  (  x  )  ,  where  σ  (  x  )  is\\xa0the\\xa0logistic\\xa0sigmoid.  \\\\text{silu}(x) = x * \\\\sigma(x), \\\\text{where } \\\\sigma(x) \\\\text{ is the logistic sigmoid.}silu(x)=x∗σ(x),whereσ(x)is\\xa0the\\xa0logistic\\xa0sigmoid.  Note  See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation\\nin Reinforcement Learning and Swish:\\na Self-Gated Activation Function where the SiLU was experimented with later.  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.SiLU()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#silu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#torch.nn.SiLU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#silu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#SiLU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L395', 'https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#torch.nn.SiLU', 'https://arxiv.org/abs/1606.08415', 'https://arxiv.org/abs/1702.03118', 'https://arxiv.org/abs/1710.05941v1', 'https://pytorch.org/docs/stable/generated/torch.nn.Mish.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d4'), 'title': 'nn.Mish', 'page_text': 'Mish [LINK_1]  class torch.nn.Mish( inplace=False ) [source]  [source]  [LINK_2]  Applies the Mish function, element-wise.  Mish: A Self Regularized Non-Monotonic Neural Activation Function.  Mish  (  x  )  =  x  ∗  Tanh  (  Softplus  (  x  )  )  \\\\text{Mish}(x) = x * \\\\text{Tanh}(\\\\text{Softplus}(x))Mish(x)=x∗Tanh(Softplus(x))  Note  See Mish: A Self Regularized Non-Monotonic Neural Activation Function  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Mish()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#mish\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#torch.nn.Mish', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#mish', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Mish', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L439', 'https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#torch.nn.Mish', 'https://arxiv.org/abs/1908.08681', 'https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html', 'https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d5'), 'title': 'nn.Softplus', 'page_text': 'Softplus [LINK_1]  class torch.nn.Softplus( beta=1.0 , threshold=20.0 ) [source]  [source]  [LINK_2]  Applies the Softplus function element-wise.  Softplus  (  x  )  =  1  β  ∗  log  \\u2061  (  1  +  exp  \\u2061  (  β  ∗  x  )  )  \\\\text{Softplus}(x) = \\\\frac{1}{\\\\beta} * \\\\log(1 + \\\\exp(\\\\beta * x))Softplus(x)=β1\\u200b∗log(1+exp(β∗x))  SoftPlus is a smooth approximation to the ReLU function and can be used\\nto constrain the output of a machine to always be positive.  For numerical stability the implementation reverts to the linear function\\nwheni  n  p  u  t  ×  β  >  t  h  r  e  s  h  o  l  d  input \\\\times \\\\beta > thresholdinput×β>threshold.  Parameters  beta ( float ) – theβ  \\\\betaβvalue for the Softplus formulation. Default: 1  threshold ( float ) – values above this revert to a linear function. Default: 20  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Softplus()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#softplus\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#torch.nn.Softplus', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#softplus', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softplus', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L858', 'https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#torch.nn.Softplus', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Mish.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d6'), 'title': 'nn.Softshrink', 'page_text': 'Softshrink [LINK_1]  class torch.nn.Softshrink( lambd=0.5 ) [source]  [source]  [LINK_2]  Applies the soft shrinkage function element-wise.  SoftShrinkage  (  x  )  =  {  x  −  λ  ,  if  x  >  λ  x  +  λ  ,  if  x  <  −  λ  0  ,  otherwise  \\\\text{SoftShrinkage}(x) =\\n\\\\begin{cases}\\nx - \\\\lambda, & \\\\text{ if } x > \\\\lambda \\\\\\\\\\nx + \\\\lambda, & \\\\text{ if } x < -\\\\lambda \\\\\\\\\\n0, & \\\\text{ otherwise }\\n\\\\end{cases}SoftShrinkage(x)=⎩⎨⎧\\u200bx−λ,x+λ,0,\\u200bifx>λifx<−λotherwise\\u200b  Parameters  lambd ( float ) – theλ  \\\\lambdaλ(must be no less than zero) value for the Softshrink formulation. Default: 0.5  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Softshrink()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html#softshrink\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html#torch.nn.Softshrink', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html#softshrink', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softshrink', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L903', 'https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html#torch.nn.Softshrink', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d7'), 'title': 'nn.Softsign', 'page_text': 'Softsign [LINK_1]  class torch.nn.Softsign( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the element-wise Softsign function.  SoftSign  (  x  )  =  x  1  +  ∣  x  ∣  \\\\text{SoftSign}(x) = \\\\frac{x}{ 1 + |x|}SoftSign(x)=1+∣x∣x\\u200b  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Softsign()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html#softsign\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html#torch.nn.Softsign', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html#softsign', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softsign', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1523', 'https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html#torch.nn.Softsign', 'https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d8'), 'title': 'nn.Tanh', 'page_text': 'Tanh [LINK_1]  class torch.nn.Tanh( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the Hyperbolic Tangent (Tanh) function element-wise.  Tanh is defined as:  Tanh  (  x  )  =  tanh  \\u2061  (  x  )  =  exp  \\u2061  (  x  )  −  exp  \\u2061  (  −  x  )  exp  \\u2061  (  x  )  +  exp  \\u2061  (  −  x  )  \\\\text{Tanh}(x) = \\\\tanh(x) = \\\\frac{\\\\exp(x) - \\\\exp(-x)} {\\\\exp(x) + \\\\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(−x)exp(x)−exp(−x)\\u200b  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Tanh()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#tanh\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#tanh', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Tanh', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L370', 'https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh', 'https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179d9'), 'title': 'nn.Tanhshrink', 'page_text': 'Tanhshrink [LINK_1]  class torch.nn.Tanhshrink( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the element-wise Tanhshrink function.  Tanhshrink  (  x  )  =  x  −  tanh  \\u2061  (  x  )  \\\\text{Tanhshrink}(x) = x - \\\\tanh(x)Tanhshrink(x)=x−tanh(x)  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Tanhshrink()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html#tanhshrink\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html#tanhshrink', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Tanhshrink', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1546', 'https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink', 'https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179da'), 'title': 'nn.Threshold', 'page_text': 'Threshold [LINK_1]  class torch.nn.Threshold( threshold , value , inplace=False ) [source]  [source]  [LINK_2]  Thresholds each element of the input Tensor.  Threshold is defined as:  y  =  {  x  ,  if  x  >  threshold  value  ,  otherwise  y =\\n\\\\begin{cases}\\nx, &\\\\text{ if } x > \\\\text{threshold} \\\\\\\\\\n\\\\text{value}, &\\\\text{ otherwise }\\n\\\\end{cases}y={x,value,\\u200bifx>thresholdotherwise\\u200b  Parameters  threshold ( float ) – The value to threshold at  value ( float ) – The value to replace with  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Threshold(0.1,20)>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#threshold\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#torch.nn.Threshold', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#threshold', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Threshold', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L48', 'https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#torch.nn.Threshold', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.GLU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179db'), 'title': 'nn.GLU', 'page_text': 'GLU [LINK_1]  class torch.nn.GLU( dim=-1 ) [source]  [source]  [LINK_2]  Applies the gated linear unit function.  G  L  U  (  a  ,  b  )  =  a  ⊗  σ  (  b  )  {GLU}(a, b)= a \\\\otimes \\\\sigma(b)GLU(a,b)=a⊗σ(b)wherea  aais the first half\\nof the input matrices andb  bbis the second half.  Parameters  dim ( int ) – the dimension on which to split the input. Default: -1  Shape:  Input:(  ∗  1  ,  N  ,  ∗  2  )  (\\\\ast_1, N, \\\\ast_2)(∗1\\u200b,N,∗2\\u200b)where * means, any number of additional\\ndimensions  Output:(  ∗  1  ,  M  ,  ∗  2  )  (\\\\ast_1, M, \\\\ast_2)(∗1\\u200b,M,∗2\\u200b)whereM  =  N  /  2  M=N/2M=N/2  Examples:  >>>m=nn.GLU()>>>input=torch.randn(4,2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GLU.html#glu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GLU.html#torch.nn.GLU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.GLU.html#glu', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#GLU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L663', 'https://pytorch.org/docs/stable/generated/torch.nn.GLU.html#torch.nn.GLU', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179dc'), 'title': 'nn.Softmin', 'page_text': 'Softmin [LINK_1]  class torch.nn.Softmin( dim=None ) [source]  [source]  [LINK_2]  Applies the Softmin function to an n-dimensional input Tensor.  Rescales them so that the elements of the n-dimensional output Tensor\\nlie in the range [0, 1] and sum to 1.  Softmin is defined as:  Softmin  (  x  i  )  =  exp  \\u2061  (  −  x  i  )  ∑  j  exp  \\u2061  (  −  x  j  )  \\\\text{Softmin}(x_{i}) = \\\\frac{\\\\exp(-x_i)}{\\\\sum_j \\\\exp(-x_j)}Softmin(xi\\u200b)=∑j\\u200bexp(−xj\\u200b)exp(−xi\\u200b)\\u200b  Shape:  Input:(  ∗  )  (*)(∗)where * means, any number of additional\\ndimensions  Output:(  ∗  )  (*)(∗), same shape as the input  Parameters  dim ( int ) – A dimension along which Softmin will be computed (so every slice\\nalong dim will sum to 1).  Returns  a Tensor of the same dimension and shape as the input, with\\nvalues in the range [0, 1]  Return type  None  Examples:  >>>m=nn.Softmin(dim=1)>>>input=torch.randn(2,3)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html#softmin\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html#torch.nn.Softmin', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html#softmin', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softmin', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1569', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html#torch.nn.Softmin', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html', 'https://pytorch.org/docs/stable/generated/torch.nn.GLU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179dd'), 'title': 'nn.Softmax', 'page_text': 'Softmax [LINK_1]  class torch.nn.Softmax( dim=None ) [source]  [source]  [LINK_2]  Applies the Softmax function to an n-dimensional input Tensor.  Rescales them so that the elements of the n-dimensional output Tensor\\nlie in the range [0,1] and sum to 1.  Softmax is defined as:  Softmax  (  x  i  )  =  exp  \\u2061  (  x  i  )  ∑  j  exp  \\u2061  (  x  j  )  \\\\text{Softmax}(x_{i}) = \\\\frac{\\\\exp(x_i)}{\\\\sum_j \\\\exp(x_j)}Softmax(xi\\u200b)=∑j\\u200bexp(xj\\u200b)exp(xi\\u200b)\\u200b  When the input Tensor is a sparse tensor then the unspecified\\nvalues are treated as -inf .  Shape:  Input:(  ∗  )  (*)(∗)where * means, any number of additional\\ndimensions  Output:(  ∗  )  (*)(∗), same shape as the input  Returns  a Tensor of the same dimension and shape as the input with\\nvalues in the range [0, 1]  Parameters  dim ( int ) – A dimension along which Softmax will be computed (so every slice\\nalong dim will sum to 1).  Return type  None  Note  This module doesn’t work directly with NLLLoss,\\nwhich expects the Log to be computed between the Softmax and itself.\\nUse LogSoftmax instead (it’s faster and has better numerical properties).  Examples:  >>>m=nn.Softmax(dim=1)>>>input=torch.randn(2,3)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#softmax\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#softmax', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softmax', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1619', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179de'), 'title': 'nn.Softmax2d', 'page_text': 'Softmax2d [LINK_1]  class torch.nn.Softmax2d( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies SoftMax over features to each spatial location.  When given an image of ChannelsxHeightxWidth , it will\\napply Softmax to each location(  C  h  a  n  n  e  l  s  ,  h  i  ,  w  j  )  (Channels, h_i, w_j)(Channels,hi\\u200b,wj\\u200b)  Shape:  Input:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W).  Output:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W)(same shape as input)  Returns  a Tensor of the same dimension and shape as the input with\\nvalues in the range [0, 1]  Return type  None  Examples:  >>>m=nn.Softmax2d()>>># you softmax over the 2nd dimension>>>input=torch.randn(2,3,12,13)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html#softmax2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html#softmax2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softmax2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1678', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d', 'https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179df'), 'title': 'nn.LogSoftmax', 'page_text': 'LogSoftmax [LINK_1]  class torch.nn.LogSoftmax( dim=None ) [source]  [source]  [LINK_2]  Applies thelog  \\u2061  (  Softmax  (  x  )  )  \\\\log(\\\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor.  The LogSoftmax formulation can be simplified as:  LogSoftmax  (  x  i  )  =  log  \\u2061  (  exp  \\u2061  (  x  i  )  ∑  j  exp  \\u2061  (  x  j  )  )  \\\\text{LogSoftmax}(x_{i}) = \\\\log\\\\left(\\\\frac{\\\\exp(x_i) }{ \\\\sum_j \\\\exp(x_j)} \\\\right)LogSoftmax(xi\\u200b)=log(∑j\\u200bexp(xj\\u200b)exp(xi\\u200b)\\u200b)  Shape:  Input:(  ∗  )  (*)(∗)where * means, any number of additional\\ndimensions  Output:(  ∗  )  (*)(∗), same shape as the input  Parameters  dim ( int ) – A dimension along which LogSoftmax will be computed.  Returns  a Tensor of the same dimension and shape as the input with\\nvalues in the range [-inf, 0)  Return type  None  Examples:  >>>m=nn.LogSoftmax(dim=1)>>>input=torch.randn(2,3)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#logsoftmax\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#logsoftmax', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#LogSoftmax', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/activation.py#L1708', 'https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e0'), 'title': 'nn.AdaptiveLogSoftmaxWithLoss', 'page_text': 'AdaptiveLogSoftmaxWithLoss [LINK_1]  class torch.nn.AdaptiveLogSoftmaxWithLoss( in_features , n_classes , cutoffs , div_value=4.0 , head_bias=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Efficient softmax approximation.  As described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin,\\nMoustapha Cissé, David Grangier, and Hervé Jégou .  Adaptive softmax is an approximate strategy for training models with large\\noutput spaces. It is most effective when the label distribution is highly\\nimbalanced, for example in natural language modelling, where the word\\nfrequency distribution approximately follows the Zipf’s law .  Adaptive softmax partitions the labels into several clusters, according to\\ntheir frequency. These clusters may contain different number of targets\\neach.\\nAdditionally, clusters containing less frequent labels assign lower\\ndimensional embeddings to those labels, which speeds up the computation.\\nFor each minibatch, only clusters for which at least one target is\\npresent are evaluated.  The idea is that the clusters which are accessed frequently\\n(like the first one, containing most frequent labels), should also be cheap\\nto compute – that is, contain a small number of assigned labels.  We highly recommend taking a look at the original paper for more details.  cutoffs should be an ordered Sequence of integers sorted\\nin the increasing order.\\nIt controls number of clusters and the partitioning of targets into\\nclusters. For example setting cutoffs=[10,100,1000] means that first 10 targets will be assigned\\nto the ‘head’ of the adaptive softmax, targets 11, 12, …, 100 will be\\nassigned to the first cluster, and targets 101, 102, …, 1000 will be\\nassigned to the second cluster, while targets 1001, 1002, …, n_classes - 1 will be assigned\\nto the last, third cluster.  div_value is used to compute the size of each additional cluster,\\nwhich is given as⌊  in_features  div_value  i  d  x  ⌋  \\\\left\\\\lfloor\\\\frac{\\\\texttt{in\\\\_features}}{\\\\texttt{div\\\\_value}^{idx}}\\\\right\\\\rfloor⌊div_valueidxin_features\\u200b⌋,\\nwherei  d  x  idxidxis the cluster index (with clusters\\nfor less frequent words having larger indices,\\nand indices starting from1  11).  head_bias if set to True, adds a bias term to the ‘head’ of the\\nadaptive softmax. See paper for details. Set to False in the official\\nimplementation.  Warning  Labels passed as inputs to this module should be sorted according to\\ntheir frequency. This means that the most frequent label should be\\nrepresented by the index 0 , and the least frequent\\nlabel should be represented by the index n_classes - 1 .  Note  This module returns a NamedTuple with output and loss fields. See further documentation for details.  Note  To compute log-probabilities for all classes, the log_prob method can be used.  Parameters  in_features ( int ) – Number of features in the input tensor  n_classes ( int ) – Number of classes in the dataset  cutoffs ( Sequence ) – Cutoffs used to assign targets to their buckets  div_value ( float  ,  optional ) – value used as an exponent to compute sizes\\nof the clusters. Default: 4.0  head_bias ( bool  ,  optional ) – If True , adds a bias term to the ‘head’ of the\\nadaptive softmax. Default: False  Returns  output is a Tensor of size N containing computed target\\nlog probabilities for each example  loss is a Scalar representing the computed negative\\nlog likelihood loss  Return type  NamedTuple with output and loss fields  Shape:  input:(  N  ,  in_features  )  (N, \\\\texttt{in\\\\_features})(N,in_features)or(  in_features  )  (\\\\texttt{in\\\\_features})(in_features)  target:(  N  )  (N)(N)or(  )  ()()where each value satisfies0  <  =  target[i]  <  =  n_classes  0 <= \\\\texttt{target[i]} <= \\\\texttt{n\\\\_classes}0<=target[i]<=n_classes  output1:(  N  )  (N)(N)or(  )  ()()  output2: Scalar  log_prob( input ) [source]  [source]  [LINK_3]  Compute log probabilities for alln_classes  \\\\texttt{n\\\\_classes}n_classes.  Parameters  input ( Tensor ) – a minibatch of examples  Returns  log-probabilities of for each classc  ccin range0  <  =  c  <  =  n_classes  0 <= c <= \\\\texttt{n\\\\_classes}0<=c<=n_classes, wheren_classes  \\\\texttt{n\\\\_classes}n_classesis a\\nparameter passed to AdaptiveLogSoftmaxWithLoss constructor.  Return type  Tensor  Shape:  Input:(  N  ,  in_features  )  (N, \\\\texttt{in\\\\_features})(N,in_features)  Output:(  N  ,  n_classes  )  (N, \\\\texttt{n\\\\_classes})(N,n_classes)  predict( input ) [source]  [source]  [LINK_4]  Return the class with the highest probability for each example in the input minibatch.  This is equivalent to self.log_prob(input).argmax(dim=1) , but is more efficient in some cases.  Parameters  input ( Tensor ) – a minibatch of examples  Returns  a class with the highest probability for each example  Return type  output ( Tensor )  Shape:  Input:(  N  ,  in_features  )  (N, \\\\texttt{in\\\\_features})(N,in_features)  Output:(  N  )  (N)(N)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#adaptivelogsoftmaxwithloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.predict', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#adaptivelogsoftmaxwithloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/adaptive.py#L20', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss', 'https://arxiv.org/abs/1609.04309', 'https://en.wikipedia.org/wiki/Zipf%27s_law', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/adaptive.py#L279', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/adaptive.py#L298', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.predict', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e1'), 'title': 'nn.BatchNorm1d', 'page_text': 'BatchNorm1d [LINK_1]  class torch.nn.BatchNorm1d( num_features , eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Batch Normalization over a 2D or 3D input.  Method described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{\\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension over\\nthe mini-batches andγ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the number of features or channels of the input). By default, the\\nelements ofγ  \\\\gammaγare set to 1 and the elements ofβ  \\\\betaβare set to 0.\\nAt train time in the forward pass, the variance is calculated via the biased estimator,\\nequivalent to torch.var(input,unbiased=False) . However, the value stored in the\\nmoving average of the variance is calculated via the unbiased  estimator, equivalent to torch.var(input,unbiased=True) .  Also by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default momentum of 0.1.  If track_running_stats is set to False , this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Because the Batch Normalization is done over the C dimension, computing statistics\\non (N, L) slices, it’s common terminology to call this Temporal Batch Normalization.  Parameters  num_features ( int ) – number of features or channelsC  CCof the input  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average\\n(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.\\nin both training and eval modes. Default: True  Shape:  Input:(  N  ,  C  )  (N, C)(N,C)or(  N  ,  C  ,  L  )  (N, C, L)(N,C,L), whereN  NNis the batch size,C  CCis the number of features or channels, andL  LLis the sequence length  Output:(  N  ,  C  )  (N, C)(N,C)or(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)(same shape as input)  Examples:  >>># With Learnable Parameters>>>m=nn.BatchNorm1d(100)>>># Without Learnable Parameters>>>m=nn.BatchNorm1d(100,affine=False)>>>input=torch.randn(20,100)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L268', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d', 'https://arxiv.org/abs/1502.03167', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e2'), 'title': 'nn.BatchNorm2d', 'page_text': 'BatchNorm2d [LINK_1]  class torch.nn.BatchNorm2d( num_features , eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Batch Normalization over a 4D input.  4D is a mini-batch of 2D inputs\\nwith additional channel dimension. Method described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension over\\nthe mini-batches andγ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the input size). By default, the elements ofγ  \\\\gammaγare set\\nto 1 and the elements ofβ  \\\\betaβare set to 0. At train time in the forward pass, the\\nstandard-deviation is calculated via the biased estimator, equivalent to torch.var(input,unbiased=False) . However, the value stored in the moving average of the\\nstandard-deviation is calculated via the unbiased  estimator, equivalent to torch.var(input,unbiased=True) .  Also by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default momentum of 0.1.  If track_running_stats is set to False , this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Because the Batch Normalization is done over the C dimension, computing statistics\\non (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.  Parameters  num_features ( int ) –C  CCfrom an expected input of size(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average\\n(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.\\nin both training and eval modes. Default: True  Shape:  Input:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)  Output:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)(same shape as input)  Examples:  >>># With Learnable Parameters>>>m=nn.BatchNorm2d(100)>>># Without Learnable Parameters>>>m=nn.BatchNorm2d(100,affine=False)>>>input=torch.randn(20,100,35,45)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#batchnorm2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#batchnorm2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L378', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d', 'https://arxiv.org/abs/1502.03167', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e3'), 'title': 'nn.BatchNorm3d', 'page_text': 'BatchNorm3d [LINK_1]  class torch.nn.BatchNorm3d( num_features , eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Batch Normalization over a 5D input.  5D is a mini-batch of 3D inputs with additional channel dimension as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension over\\nthe mini-batches andγ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the input size). By default, the elements ofγ  \\\\gammaγare set\\nto 1 and the elements ofβ  \\\\betaβare set to 0. At train time in the forward pass, the\\nstandard-deviation is calculated via the biased estimator, equivalent to torch.var(input,unbiased=False) . However, the value stored in the moving average of the\\nstandard-deviation is calculated via the unbiased  estimator, equivalent to torch.var(input,unbiased=True) .  Also by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default momentum of 0.1.  If track_running_stats is set to False , this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Because the Batch Normalization is done over the C dimension, computing statistics\\non (N, D, H, W) slices, it’s common terminology to call this Volumetric Batch Normalization\\nor Spatio-temporal Batch Normalization.  Parameters  num_features ( int ) –C  CCfrom an expected input of size(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average\\n(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.\\nin both training and eval modes. Default: True  Shape:  Input:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)  Output:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)(same shape as input)  Examples:  >>># With Learnable Parameters>>>m=nn.BatchNorm3d(100)>>># Without Learnable Parameters>>>m=nn.BatchNorm3d(100,affine=False)>>>input=torch.randn(20,100,35,45,10)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#batchnorm3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#batchnorm3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L489', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d', 'https://arxiv.org/abs/1502.03167', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e4'), 'title': 'nn.LazyBatchNorm1d', 'page_text': 'LazyBatchNorm1d [LINK_1]  class torch.nn.LazyBatchNorm1d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.BatchNorm1d module with lazy initialization.  Lazy initialization based on the num_features argument of the BatchNorm1d that is inferred\\nfrom the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average\\n(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.\\nin both training and eval modes. Default: True  cls_to_become [source]  [LINK_3]  alias of BatchNorm1d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#lazybatchnorm1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#lazybatchnorm1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#LazyBatchNorm1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L344', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L268', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e5'), 'title': 'nn.LazyBatchNorm2d', 'page_text': 'LazyBatchNorm2d [LINK_1]  class torch.nn.LazyBatchNorm2d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.BatchNorm2d module with lazy initialization.  Lazy initialization is done for the num_features argument of the BatchNorm2d that is inferred\\nfrom the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average\\n(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.\\nin both training and eval modes. Default: True  cls_to_become [source]  [LINK_3]  alias of BatchNorm2d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#lazybatchnorm2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#lazybatchnorm2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#LazyBatchNorm2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L455', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L378', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e6'), 'title': 'nn.LazyBatchNorm3d', 'page_text': 'LazyBatchNorm3d [LINK_1]  class torch.nn.LazyBatchNorm3d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.BatchNorm3d module with lazy initialization.  Lazy initialization is done for the num_features argument of the BatchNorm3d that is inferred\\nfrom the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average\\n(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.\\nin both training and eval modes. Default: True  cls_to_become [source]  [LINK_3]  alias of BatchNorm3d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#lazybatchnorm3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#lazybatchnorm3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#LazyBatchNorm3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L566', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L489', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e7'), 'title': 'nn.GroupNorm', 'page_text': 'GroupNorm [LINK_1]  class torch.nn.GroupNorm( num_groups , num_channels , eps=1e-05 , affine=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Group Normalization over a mini-batch of inputs.  This layer implements the operation as described in\\nthe paper Group Normalization  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The input channels are separated into num_groups groups, each containing num_channels/num_groups channels. num_channels must be divisible by num_groups . The mean and standard-deviation are calculated\\nseparately over the each group.γ  \\\\gammaγandβ  \\\\betaβare learnable\\nper-channel affine transform parameter vectors of size num_channels if affine is True .\\nThe variance is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False) .  This layer uses statistics computed from input data in both training and\\nevaluation modes.  Parameters  num_groups ( int ) – number of groups to separate the channels into  num_channels ( int ) – number of channels expected in input  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  affine ( bool ) – a boolean value that when set to True , this module\\nhas learnable per-channel affine parameters initialized to ones (for weights)\\nand zeros (for biases). Default: True .  Shape:  Input:(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)whereC  =  num_channels  C=\\\\text{num\\\\_channels}C=num_channels  Output:(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)(same shape as input)  Examples:  >>>input=torch.randn(20,6,10,10)>>># Separate 6 channels into 3 groups>>>m=nn.GroupNorm(3,6)>>># Separate 6 channels into 6 groups (equivalent with InstanceNorm)>>>m=nn.GroupNorm(6,6)>>># Put all 6 channels into a single group (equivalent with LayerNorm)>>>m=nn.GroupNorm(1,6)>>># Activating the module>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html#groupnorm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html#groupnorm', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#GroupNorm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L228', 'https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm', 'https://arxiv.org/abs/1803.08494', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e8'), 'title': 'nn.SyncBatchNorm', 'page_text': 'SyncBatchNorm [LINK_1]  class torch.nn.SyncBatchNorm( num_features , eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , process_group=None , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Batch Normalization over a N-Dimensional input.  The N-D input is a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension over all\\nmini-batches of the same process groups.γ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors of size C (where C is the input size).\\nBy default, the elements ofγ  \\\\gammaγare sampled fromU  (  0  ,  1  )  \\\\mathcal{U}(0, 1)U(0,1)and the elements ofβ  \\\\betaβare set to 0.\\nThe standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False) .  Also by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default momentum of 0.1.  If track_running_stats is set to False , this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Because the Batch Normalization is done for each channel in the C dimension, computing\\nstatistics on (N,+) slices, it’s common terminology to call this Volumetric Batch\\nNormalization or Spatio-temporal Batch Normalization.  Currently SyncBatchNorm only supports DistributedDataParallel (DDP) with single GPU per process. Use torch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNorm*D layer to SyncBatchNorm before wrapping\\nNetwork with DDP.  Parameters  num_features ( int ) –C  CCfrom an expected input of size(  N  ,  C  ,  +  )  (N, C, +)(N,C,+)  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average\\n(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.\\nin both training and eval modes. Default: True  process_group ( Optional  [  Any  ] ) – synchronization of stats happen within each process group\\nindividually. Default behavior is synchronization across the whole\\nworld  Shape:  Input:(  N  ,  C  ,  +  )  (N, C, +)(N,C,+)  Output:(  N  ,  C  ,  +  )  (N, C, +)(N,C,+)(same shape as input)  Note  Synchronization of batchnorm statistics occurs only while training, i.e.\\nsynchronization is disabled when model.eval() is set or if self.training is otherwise False .  Examples:  >>># With Learnable Parameters>>>m=nn.SyncBatchNorm(100)>>># creating process group (optional)>>># ranks is a list of int identifying rank ids.>>>ranks=list(range(8))>>>r1,r2=ranks[:4],ranks[4:]>>># Note: every rank calls into new_group for every>>># process group created, even if that rank is not>>># part of the group.>>>process_groups=[torch.distributed.new_group(pids)forpidsin[r1,r2]]>>>process_group=process_groups[0ifdist.get_rank()<=3else1]>>># Without Learnable Parameters>>>m=nn.BatchNorm3d(100,affine=False,process_group=process_group)>>>input=torch.randn(20,100,35,45,10)>>>output=m(input)>>># network is nn.BatchNorm layer>>>sync_bn_network=nn.SyncBatchNorm.convert_sync_batchnorm(network,process_group)>>># only single gpu per process is currently supported>>>ddp_sync_bn_network=torch.nn.parallel.DistributedDataParallel(>>>sync_bn_network,>>>device_ids=[args.local_rank],>>>output_device=args.local_rank)  classmethod convert_sync_batchnorm( module , process_group=None ) [source]  [source]  [LINK_3]  Converts all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.  Parameters  module ( nn.Module ) – module containing one or more BatchNorm*D layers  process_group ( optional ) – process group to scope synchronization,\\ndefault is the whole world  Returns  The original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer,\\na new torch.nn.SyncBatchNorm layer object will be returned\\ninstead.  Example:  >>># Network with nn.BatchNorm layer>>>module=torch.nn.Sequential(>>>torch.nn.Linear(20,100),>>>torch.nn.BatchNorm1d(100),>>>).cuda()>>># creating process group (optional)>>># ranks is a list of int identifying rank ids.>>>ranks=list(range(8))>>>r1,r2=ranks[:4],ranks[4:]>>># Note: every rank calls into new_group for every>>># process group created, even if that rank is not>>># part of the group.>>>process_groups=[torch.distributed.new_group(pids)forpidsin[r1,r2]]>>>process_group=process_groups[0ifdist.get_rank()<=3else1]>>>sync_bn_module=torch.nn.SyncBatchNorm.convert_sync_batchnorm(module,process_group)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#syncbatchnorm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#syncbatchnorm', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L600', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm', 'https://arxiv.org/abs/1502.03167', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/batchnorm.py#L822', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179e9'), 'title': 'nn.InstanceNorm1d', 'page_text': 'InstanceNorm1d [LINK_1]  class torch.nn.InstanceNorm1d( num_features , eps=1e-05 , momentum=0.1 , affine=False , track_running_stats=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Instance Normalization.  This operation applies Instance Normalization\\nover a 2D (unbatched) or 3D (batched) input as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension separately\\nfor each object in a mini-batch.γ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the number of features or channels of the input) if affine is True .\\nThe variance is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False) .  By default, this layer uses instance statistics computed from input data in\\nboth training and evaluation modes.  If track_running_stats is set to True , during training this\\nlayer keeps running estimates of its computed mean and variance, which are\\nthen used for normalization during evaluation. The running estimates are\\nkept with a default momentum of 0.1.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Note  InstanceNorm1d and LayerNorm are very similar, but\\nhave some subtle differences. InstanceNorm1d is applied\\non each channel of channeled data like multidimensional time series, but LayerNorm is usually applied on entire sample and often in NLP\\ntasks. Additionally, LayerNorm applies elementwise affine\\ntransform, while InstanceNorm1d usually don’t apply affine\\ntransform.  Parameters  num_features ( int ) – number of features or channelsC  CCof the input  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var computation. Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault: False .  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default: False  Shape:  Input:(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)or(  C  ,  L  )  (C, L)(C,L)  Output:(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)or(  C  ,  L  )  (C, L)(C,L)(same shape as input)  Examples:  >>># Without Learnable Parameters>>>m=nn.InstanceNorm1d(100)>>># With Learnable Parameters>>>m=nn.InstanceNorm1d(100,affine=True)>>>input=torch.randn(20,100,40)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#instancenorm1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#instancenorm1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#InstanceNorm1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L127', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d', 'https://arxiv.org/abs/1607.08022', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ea'), 'title': 'nn.InstanceNorm2d', 'page_text': 'InstanceNorm2d [LINK_1]  class torch.nn.InstanceNorm2d( num_features , eps=1e-05 , momentum=0.1 , affine=False , track_running_stats=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Instance Normalization.  This operation applies Instance Normalization\\nover a 4D input (a mini-batch of 2D inputs\\nwith additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension separately\\nfor each object in a mini-batch.γ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the input size) if affine is True .\\nThe standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False) .  By default, this layer uses instance statistics computed from input data in\\nboth training and evaluation modes.  If track_running_stats is set to True , during training this\\nlayer keeps running estimates of its computed mean and variance, which are\\nthen used for normalization during evaluation. The running estimates are\\nkept with a default momentum of 0.1.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Note  InstanceNorm2d and LayerNorm are very similar, but\\nhave some subtle differences. InstanceNorm2d is applied\\non each channel of channeled data like RGB images, but LayerNorm is usually applied on entire sample and often in NLP\\ntasks. Additionally, LayerNorm applies elementwise affine\\ntransform, while InstanceNorm2d usually don’t apply affine\\ntransform.  Parameters  num_features ( int ) –C  CCfrom an expected input of size(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W)  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var computation. Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault: False .  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default: False  Shape:  Input:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W)  Output:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W)(same shape as input)  Examples:  >>># Without Learnable Parameters>>>m=nn.InstanceNorm2d(100)>>># With Learnable Parameters>>>m=nn.InstanceNorm2d(100,affine=True)>>>input=torch.randn(20,100,35,45)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#instancenorm2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#instancenorm2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#InstanceNorm2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L241', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d', 'https://arxiv.org/abs/1607.08022', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179eb'), 'title': 'nn.InstanceNorm3d', 'page_text': 'InstanceNorm3d [LINK_1]  class torch.nn.InstanceNorm3d( num_features , eps=1e-05 , momentum=0.1 , affine=False , track_running_stats=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Instance Normalization.  This operation applies Instance Normalization\\nover a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension separately\\nfor each object in a mini-batch.γ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the input size) if affine is True .\\nThe standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False) .  By default, this layer uses instance statistics computed from input data in\\nboth training and evaluation modes.  If track_running_stats is set to True , during training this\\nlayer keeps running estimates of its computed mean and variance, which are\\nthen used for normalization during evaluation. The running estimates are\\nkept with a default momentum of 0.1.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Note  InstanceNorm3d and LayerNorm are very similar, but\\nhave some subtle differences. InstanceNorm3d is applied\\non each channel of channeled data like 3D models with RGB color, but LayerNorm is usually applied on entire sample and often in NLP\\ntasks. Additionally, LayerNorm applies elementwise affine\\ntransform, while InstanceNorm3d usually don’t apply affine\\ntransform.  Parameters  num_features ( int ) –C  CCfrom an expected input of size(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W)  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var computation. Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault: False .  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default: False  Shape:  Input:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W)  Output:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W)(same shape as input)  Examples:  >>># Without Learnable Parameters>>>m=nn.InstanceNorm3d(100)>>># With Learnable Parameters>>>m=nn.InstanceNorm3d(100,affine=True)>>>input=torch.randn(20,100,35,45,10)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#instancenorm3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#instancenorm3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#InstanceNorm3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L358', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d', 'https://arxiv.org/abs/1607.08022', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ec'), 'title': 'nn.LazyInstanceNorm1d', 'page_text': 'LazyInstanceNorm1d [LINK_1]  class torch.nn.LazyInstanceNorm1d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument.  The num_features argument of the InstanceNorm1d is inferred from the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  num_features –C  CCfrom an expected input of size(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)or(  C  ,  L  )  (C, L)(C,L)  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var computation. Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault: False .  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default: False  Shape:  Input:(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)or(  C  ,  L  )  (C, L)(C,L)  Output:(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)or(  C  ,  L  )  (C, L)(C,L)(same shape as input)  cls_to_become [source]  [LINK_3]  alias of InstanceNorm1d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html#lazyinstancenorm1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html#lazyinstancenorm1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#LazyInstanceNorm1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L127', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ed'), 'title': 'nn.LazyInstanceNorm2d', 'page_text': 'LazyInstanceNorm2d [LINK_1]  class torch.nn.LazyInstanceNorm2d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument.  The num_features argument of the InstanceNorm2d is inferred from the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  num_features –C  CCfrom an expected input of size(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W)  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var computation. Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault: False .  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default: False  Shape:  Input:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W)  Output:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W)(same shape as input)  cls_to_become [source]  [LINK_3]  alias of InstanceNorm2d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html#lazyinstancenorm2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html#lazyinstancenorm2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#LazyInstanceNorm2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L320', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L241', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ee'), 'title': 'nn.LazyInstanceNorm3d', 'page_text': 'LazyInstanceNorm3d [LINK_1]  class torch.nn.LazyInstanceNorm3d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument.  The num_features argument of the InstanceNorm3d is inferred from the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  num_features –C  CCfrom an expected input of size(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W)  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var computation. Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault: False .  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default: False  Shape:  Input:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W)  Output:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W)(same shape as input)  cls_to_become [source]  [LINK_3]  alias of InstanceNorm3d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html#lazyinstancenorm3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html#lazyinstancenorm3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#LazyInstanceNorm3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L436', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/instancenorm.py#L358', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ef'), 'title': 'nn.LayerNorm', 'page_text': 'LayerNorm [LINK_1]  class torch.nn.LayerNorm( normalized_shape , eps=1e-05 , elementwise_affine=True , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Layer Normalization over a mini-batch of inputs.  This layer implements the operation as described in\\nthe paper Layer Normalization  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated over the last D dimensions, where D is the dimension of normalized_shape . For example, if normalized_shape is (3,5) (a 2-dimensional shape), the mean and standard-deviation are computed over\\nthe last 2 dimensions of the input (i.e. input.mean((-2,-1)) ).γ  \\\\gammaγandβ  \\\\betaβare learnable affine transform parameters of normalized_shape if elementwise_affine is True .\\nThe variance is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False) .  Note  Unlike Batch Normalization and Instance Normalization, which applies\\nscalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and\\nbias with elementwise_affine .  This layer uses statistics computed from input data in both training and\\nevaluation modes.  Parameters  normalized_shape ( int  or  list  or  torch.Size ) – input shape from an expected input\\nof size  [  ∗  ×  normalized_shape  [  0  ]  ×  normalized_shape  [  1  ]  ×  …  ×  normalized_shape  [  −  1  ]  ]  [* \\\\times \\\\text{normalized\\\\_shape}[0] \\\\times \\\\text{normalized\\\\_shape}[1]\\n    \\\\times \\\\ldots \\\\times \\\\text{normalized\\\\_shape}[-1]][∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]  If a single integer is used, it is treated as a singleton list, and this module will\\nnormalize over the last dimension which is expected to be of that specific size.  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  elementwise_affine ( bool ) – a boolean value that when set to True , this module\\nhas learnable per-element affine parameters initialized to ones (for weights)\\nand zeros (for biases). Default: True .  bias ( bool ) – If set to False , the layer will not learn an additive bias (only relevant if elementwise_affine is True ). Default: True .  Variables  weight – the learnable weights of the module of shapenormalized_shape  \\\\text{normalized\\\\_shape}normalized_shapewhen elementwise_affine is set to True .\\nThe values are initialized to 1.  bias – the learnable bias of the module of shapenormalized_shape  \\\\text{normalized\\\\_shape}normalized_shapewhen elementwise_affine is set to True .\\nThe values are initialized to 0.  Shape:  Input:(  N  ,  ∗  )  (N, *)(N,∗)  Output:(  N  ,  ∗  )  (N, *)(N,∗)(same shape as input)  Examples:  >>># NLP Example>>>batch,sentence_length,embedding_dim=20,5,10>>>embedding=torch.randn(batch,sentence_length,embedding_dim)>>>layer_norm=nn.LayerNorm(embedding_dim)>>># Activate module>>>layer_norm(embedding)>>>>>># Image Example>>>N,C,H,W=20,5,10,10>>>input=torch.randn(N,C,H,W)>>># Normalize over the last three dimensions (i.e. the channel and spatial dimensions)>>># as shown in the image below>>>layer_norm=nn.LayerNorm([C,H,W])>>>output=layer_norm(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#layernorm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#layernorm', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#LayerNorm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L94', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm', 'https://arxiv.org/abs/1607.06450', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://pytorch.org/docs/stable/size.html#torch.Size', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_images/layer_norm.jpg', 'https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyInstanceNorm3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f0'), 'title': 'nn.LocalResponseNorm', 'page_text': 'LocalResponseNorm [LINK_1]  class torch.nn.LocalResponseNorm( size , alpha=0.0001 , beta=0.75 , k=1.0 ) [source]  [source]  [LINK_2]  Applies local response normalization over an input signal.  The input signal is composed of several input planes, where channels occupy the second dimension.\\nApplies normalization across channels.  b  c  =  a  c  (  k  +  α  n  ∑  c  ′  =  max  \\u2061  (  0  ,  c  −  n  /  2  )  min  \\u2061  (  N  −  1  ,  c  +  n  /  2  )  a  c  ′  2  )  −  β  b_{c} = a_{c}\\\\left(k + \\\\frac{\\\\alpha}{n}\\n\\\\sum_{c\\'=\\\\max(0, c-n/2)}^{\\\\min(N-1,c+n/2)}a_{c\\'}^2\\\\right)^{-\\\\beta}bc\\u200b=ac\\u200b\\u200bk+nα\\u200bc′=max(0,c−n/2)∑min(N−1,c+n/2)\\u200bac′2\\u200b\\u200b−β  Parameters  size ( int ) – amount of neighbouring channels used for normalization  alpha ( float ) – multiplicative factor. Default: 0.0001  beta ( float ) – exponent. Default: 0.75  k ( float ) – additive factor. Default: 1  Shape:  Input:(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)  Output:(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)(same shape as input)  Examples:  >>>lrn=nn.LocalResponseNorm(2)>>>signal_2d=torch.randn(32,5,24,24)>>>signal_4d=torch.randn(16,5,7,7,7,7)>>>output_2d=lrn(signal_2d)>>>output_4d=lrn(signal_4d)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html#localresponsenorm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html#localresponsenorm', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#LocalResponseNorm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L17', 'https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f1'), 'title': 'nn.RMSNorm', 'page_text': 'RMSNorm [LINK_1]  class torch.nn.RMSNorm( normalized_shape , eps=None , elementwise_affine=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Root Mean Square Layer Normalization over a mini-batch of inputs.  This layer implements the operation as described in\\nthe paper Root Mean Square Layer Normalization  y  i  =  x  i  R  M  S  (  x  )  ∗  γ  i  ,  where  RMS  (  x  )  =  ϵ  +  1  n  ∑  i  =  1  n  x  i  2  y_i = \\\\frac{x_i}{\\\\mathrm{RMS}(x)} * \\\\gamma_i, \\\\quad\\n\\\\text{where} \\\\quad \\\\text{RMS}(x) = \\\\sqrt{\\\\epsilon + \\\\frac{1}{n} \\\\sum_{i=1}^{n} x_i^2}yi\\u200b=RMS(x)xi\\u200b\\u200b∗γi\\u200b,whereRMS(x)=ϵ+n1\\u200bi=1∑n\\u200bxi2\\u200b\\u200b  The RMS is taken over the last D dimensions, where D is the dimension of normalized_shape . For example, if normalized_shape is (3,5) (a 2-dimensional shape), the RMS is computed over\\nthe last 2 dimensions of the input.  Parameters  normalized_shape ( int  or  list  or  torch.Size ) – input shape from an expected input\\nof size  [  ∗  ×  normalized_shape  [  0  ]  ×  normalized_shape  [  1  ]  ×  …  ×  normalized_shape  [  −  1  ]  ]  [* \\\\times \\\\text{normalized\\\\_shape}[0] \\\\times \\\\text{normalized\\\\_shape}[1]\\n    \\\\times \\\\ldots \\\\times \\\\text{normalized\\\\_shape}[-1]][∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]  If a single integer is used, it is treated as a singleton list, and this module will\\nnormalize over the last dimension which is expected to be of that specific size.  eps ( Optional  [  float  ] ) – a value added to the denominator for numerical stability. Default: torch.finfo(x.dtype).eps()  elementwise_affine ( bool ) – a boolean value that when set to True , this module\\nhas learnable per-element affine parameters initialized to ones (for weights). Default: True .  Shape:  Input:(  N  ,  ∗  )  (N, *)(N,∗)  Output:(  N  ,  ∗  )  (N, *)(N,∗)(same shape as input)  Examples:  >>>rms_norm=nn.RMSNorm([2,3])>>>input=torch.randn(2,2,3)>>>rms_norm(input)  extra_repr() [source]  [source]  [LINK_3]  Extra information about the module.  Return type  str  forward( x ) [source]  [source]  [LINK_4]  Runs forward pass.  Return type  Tensor  reset_parameters() [source]  [source]  [LINK_5]  Resets parameters based on their initialization used in __init__.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#rmsnorm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.extra_repr\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.forward\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.reset_parameters', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#rmsnorm', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#RMSNorm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L321', 'https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm', 'https://arxiv.org/pdf/1910.07467.pdf', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://pytorch.org/docs/stable/size.html#torch.Size', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#RMSNorm.extra_repr', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L403', 'https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.extra_repr', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#RMSNorm.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L397', 'https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.forward', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#RMSNorm.reset_parameters', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L390', 'https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.reset_parameters', 'https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f2'), 'title': 'nn.RNNBase', 'page_text': 'RNNBase [LINK_1]  class torch.nn.RNNBase( mode , input_size , hidden_size , num_layers=1 , bias=True , batch_first=False , dropout=0.0 , bidirectional=False , proj_size=0 , device=None , dtype=None ) [source]  [source]  [LINK_2]  Base class for RNN modules (RNN, LSTM, GRU).  Implements aspects of RNNs shared by the RNN, LSTM, and GRU classes, such as module initialization\\nand utility methods for parameter storage management.  Note  The forward method is not implemented by the RNNBase class.  Note  LSTM and GRU classes override some methods implemented by RNNBase.  flatten_parameters() [source]  [source]  [LINK_3]  Reset parameter data pointer so that they can use faster code paths.  Right now, this works only if the module is on the GPU and cuDNN is enabled.\\nOtherwise, it’s a no-op.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html#rnnbase\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html#torch.nn.RNNBase\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html#torch.nn.RNNBase.flatten_parameters', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html#rnnbase', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNNBase', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/rnn.py#L48', 'https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html#torch.nn.RNNBase', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNNBase.flatten_parameters', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/rnn.py#L224', 'https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html#torch.nn.RNNBase.flatten_parameters', 'https://pytorch.org/docs/stable/generated/torch.nn.RNN.html', 'https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f3'), 'title': 'nn.RNN', 'page_text': 'RNN [LINK_1]  class torch.nn.RNN( input_size , hidden_size , num_layers=1 , nonlinearity=\\'tanh\\' , bias=True , batch_first=False , dropout=0.0 , bidirectional=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Apply a multi-layer Elman RNN withtanh  \\u2061  \\\\tanhtanhorReLU  \\\\text{ReLU}ReLUnon-linearity to an input sequence. For each element in the input sequence,\\neach layer computes the following function:  h  t  =  tanh  \\u2061  (  x  t  W  i  h  T  +  b  i  h  +  h  t  −  1  W  h  h  T  +  b  h  h  )  h_t = \\\\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})ht\\u200b=tanh(xt\\u200bWihT\\u200b+bih\\u200b+ht−1\\u200bWhhT\\u200b+bhh\\u200b)  whereh  t  h_tht\\u200bis the hidden state at time t ,x  t  x_txt\\u200bis\\nthe input at time t , andh  (  t  −  1  )  h_{(t-1)}h(t−1)\\u200bis the hidden state of the\\nprevious layer at time t-1 or the initial hidden state at time 0 .\\nIf nonlinearity is \\'relu\\' , thenReLU  \\\\text{ReLU}ReLUis used instead oftanh  \\u2061  \\\\tanhtanh.  # Efficient implementation equivalent to the following with bidirectional=Falsedefforward(x,hx=None):ifbatch_first:x=x.transpose(0,1)seq_len,batch_size,_=x.size()ifhxisNone:hx=torch.zeros(num_layers,batch_size,hidden_size)h_t_minus_1=hxh_t=hxoutput=[]fortinrange(seq_len):forlayerinrange(num_layers):h_t[layer]=torch.tanh(x[t]@weight_ih[layer].T+bias_ih[layer]+h_t_minus_1[layer]@weight_hh[layer].T+bias_hh[layer])output.append(h_t[-1])h_t_minus_1=h_toutput=torch.stack(output)ifbatch_first:output=output.transpose(0,1)returnoutput,h_t  Parameters  input_size – The number of expected features in the input x  hidden_size – The number of features in the hidden state h  num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN ,\\nwith the second RNN taking in outputs of the first RNN and\\ncomputing the final results. Default: 1  nonlinearity – The non-linearity to use. Can be either \\'tanh\\' or \\'relu\\' . Default: \\'tanh\\'  bias – If False , then the layer does not use bias weights b_ih and b_hh .\\nDefault: True  batch_first – If True , then the input and output tensors are provided\\nas (batch, seq, feature) instead of (seq, batch, feature) .\\nNote that this does not apply to hidden or cell states. See the\\nInputs/Outputs sections below for details.  Default: False  dropout – If non-zero, introduces a Dropout layer on the outputs of each\\nRNN layer except the last layer, with dropout probability equal to dropout . Default: 0  bidirectional – If True , becomes a bidirectional RNN. Default: False  Inputs: input, hx  input : tensor of shape(  L  ,  H  i  n  )  (L, H_{in})(L,Hin\\u200b)for unbatched input,(  L  ,  N  ,  H  i  n  )  (L, N, H_{in})(L,N,Hin\\u200b)when batch_first=False or(  N  ,  L  ,  H  i  n  )  (N, L, H_{in})(N,L,Hin\\u200b)when batch_first=True containing the features of\\nthe input sequence.  The input can also be a packed variable length sequence.\\nSee torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.  hx : tensor of shape(  D  ∗  num_layers  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, H_{out})(D∗num_layers,Hout\\u200b)for unbatched input or(  D  ∗  num_layers  ,  N  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, N, H_{out})(D∗num_layers,N,Hout\\u200b)containing the initial hidden\\nstate for the input sequence batch. Defaults to zeros if not provided.  where:  N  =  batch\\xa0size  L  =  sequence\\xa0length  D  =  2  if\\xa0bidirectional=True\\xa0otherwise  1  H  i  n  =  input_size  H  o  u  t  =  hidden_size  \\\\begin{aligned}\\n    N ={} & \\\\text{batch size} \\\\\\\\\\n    L ={} & \\\\text{sequence length} \\\\\\\\\\n    D ={} & 2 \\\\text{ if bidirectional=True otherwise } 1 \\\\\\\\\\n    H_{in} ={} & \\\\text{input\\\\_size} \\\\\\\\\\n    H_{out} ={} & \\\\text{hidden\\\\_size}\\n\\\\end{aligned}N=L=D=Hin\\u200b=Hout\\u200b=\\u200bbatch\\xa0sizesequence\\xa0length2if\\xa0bidirectional=True\\xa0otherwise1input_sizehidden_size\\u200b  Outputs: output, h_n  output : tensor of shape(  L  ,  D  ∗  H  o  u  t  )  (L, D * H_{out})(L,D∗Hout\\u200b)for unbatched input,(  L  ,  N  ,  D  ∗  H  o  u  t  )  (L, N, D * H_{out})(L,N,D∗Hout\\u200b)when batch_first=False or(  N  ,  L  ,  D  ∗  H  o  u  t  )  (N, L, D * H_{out})(N,L,D∗Hout\\u200b)when batch_first=True containing the output features (h_t) from the last layer of the RNN, for each t . If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output\\nwill also be a packed sequence.  h_n : tensor of shape(  D  ∗  num_layers  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, H_{out})(D∗num_layers,Hout\\u200b)for unbatched input or(  D  ∗  num_layers  ,  N  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, N, H_{out})(D∗num_layers,N,Hout\\u200b)containing the final hidden state\\nfor each element in the batch.  Variables  weight_ih_l[k] – the learnable input-hidden weights of the k-th layer,\\nof shape (hidden_size, input_size) for k = 0 . Otherwise, the shape is (hidden_size, num_directions * hidden_size)  weight_hh_l[k] – the learnable hidden-hidden weights of the k-th layer,\\nof shape (hidden_size, hidden_size)  bias_ih_l[k] – the learnable input-hidden bias of the k-th layer,\\nof shape (hidden_size)  bias_hh_l[k] – the learnable hidden-hidden bias of the k-th layer,\\nof shape (hidden_size)  Note  All the weights and biases are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  1  hidden_size  k = \\\\frac{1}{\\\\text{hidden\\\\_size}}k=hidden_size1\\u200b  Note  For bidirectional RNNs, forward and backward are directions 0 and 1 respectively.\\nExample of splitting the output layers when batch_first=False : output.view(seq_len,batch,num_directions,hidden_size) .  Note  batch_first argument is ignored for unbatched inputs.  Warning  There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA.\\nYou can enforce deterministic behavior by setting the following environment variables:  On CUDA 10.1, set environment variable CUDA_LAUNCH_BLOCKING=1 .\\nThis may affect performance.  On CUDA 10.2 or later, set environment variable\\n(note the leading colon symbol) CUBLAS_WORKSPACE_CONFIG=:16:8 or CUBLAS_WORKSPACE_CONFIG=:4096:2 .  See the cuDNN 8 Release Notes for more information.  Note  If the following conditions are satisfied:\\n1) cudnn is enabled,\\n2) input data is on the GPU\\n3) input data has dtype torch.float16 4) V100 GPU is used,\\n5) input data is not in PackedSequence format\\npersistent algorithm can be selected to improve performance.  Examples:  >>>rnn=nn.RNN(10,20,2)>>>input=torch.randn(5,3,10)>>>h0=torch.randn(2,3,20)>>>output,hn=rnn(input,h0)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#rnn\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#rnn', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNN', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/rnn.py#L469', 'https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html', 'https://pytorch.org/docs/stable/generated/torch.nn.RNNBase.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f4'), 'title': 'nn.LSTM', 'page_text': 'LSTM [LINK_1]  class torch.nn.LSTM( input_size , hidden_size , num_layers=1 , bias=True , batch_first=False , dropout=0.0 , bidirectional=False , proj_size=0 , device=None , dtype=None ) [source]  [source]  [LINK_2]  Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence.\\nFor each element in the input sequence, each layer computes the following\\nfunction:  i  t  =  σ  (  W  i  i  x  t  +  b  i  i  +  W  h  i  h  t  −  1  +  b  h  i  )  f  t  =  σ  (  W  i  f  x  t  +  b  i  f  +  W  h  f  h  t  −  1  +  b  h  f  )  g  t  =  tanh  \\u2061  (  W  i  g  x  t  +  b  i  g  +  W  h  g  h  t  −  1  +  b  h  g  )  o  t  =  σ  (  W  i  o  x  t  +  b  i  o  +  W  h  o  h  t  −  1  +  b  h  o  )  c  t  =  f  t  ⊙  c  t  −  1  +  i  t  ⊙  g  t  h  t  =  o  t  ⊙  tanh  \\u2061  (  c  t  )  \\\\begin{array}{ll} \\\\\\\\\\n    i_t = \\\\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\\\\\\n    f_t = \\\\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\\\\\\n    g_t = \\\\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\\\\\\n    o_t = \\\\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\\\\\\n    c_t = f_t \\\\odot c_{t-1} + i_t \\\\odot g_t \\\\\\\\\\n    h_t = o_t \\\\odot \\\\tanh(c_t) \\\\\\\\\\n\\\\end{array}it\\u200b=σ(Wii\\u200bxt\\u200b+bii\\u200b+Whi\\u200bht−1\\u200b+bhi\\u200b)ft\\u200b=σ(Wif\\u200bxt\\u200b+bif\\u200b+Whf\\u200bht−1\\u200b+bhf\\u200b)gt\\u200b=tanh(Wig\\u200bxt\\u200b+big\\u200b+Whg\\u200bht−1\\u200b+bhg\\u200b)ot\\u200b=σ(Wio\\u200bxt\\u200b+bio\\u200b+Who\\u200bht−1\\u200b+bho\\u200b)ct\\u200b=ft\\u200b⊙ct−1\\u200b+it\\u200b⊙gt\\u200bht\\u200b=ot\\u200b⊙tanh(ct\\u200b)\\u200b  whereh  t  h_tht\\u200bis the hidden state at time t ,c  t  c_tct\\u200bis the cell\\nstate at time t ,x  t  x_txt\\u200bis the input at time t ,h  t  −  1  h_{t-1}ht−1\\u200bis the hidden state of the layer at time t-1 or the initial hidden\\nstate at time 0 , andi  t  i_tit\\u200b,f  t  f_tft\\u200b,g  t  g_tgt\\u200b,o  t  o_tot\\u200bare the input, forget, cell, and output gates, respectively.σ  \\\\sigmaσis the sigmoid function, and⊙  \\\\odot⊙is the Hadamard product.  In a multilayer LSTM, the inputx  t  (  l  )  x^{(l)}_txt(l)\\u200bof thel  ll-th layer\\n(l  ≥  2  l \\\\ge 2l≥2) is the hidden stateh  t  (  l  −  1  )  h^{(l-1)}_tht(l−1)\\u200bof the previous layer multiplied by\\ndropoutδ  t  (  l  −  1  )  \\\\delta^{(l-1)}_tδt(l−1)\\u200bwhere eachδ  t  (  l  −  1  )  \\\\delta^{(l-1)}_tδt(l−1)\\u200bis a Bernoulli random\\nvariable which is0  00with probability dropout .  If proj_size>0 is specified, LSTM with projections will be used. This changes\\nthe LSTM cell in the following way. First, the dimension ofh  t  h_tht\\u200bwill be changed from hidden_size to proj_size (dimensions ofW  h  i  W_{hi}Whi\\u200bwill be changed accordingly).\\nSecond, the output hidden state of each layer will be multiplied by a learnable projection\\nmatrix:h  t  =  W  h  r  h  t  h_t = W_{hr}h_tht\\u200b=Whr\\u200bht\\u200b. Note that as a consequence of this, the output\\nof LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact\\ndimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128 .  Parameters  input_size – The number of expected features in the input x  hidden_size – The number of features in the hidden state h  num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM ,\\nwith the second LSTM taking in outputs of the first LSTM and\\ncomputing the final results. Default: 1  bias – If False , then the layer does not use bias weights b_ih and b_hh .\\nDefault: True  batch_first – If True , then the input and output tensors are provided\\nas (batch, seq, feature) instead of (seq, batch, feature) .\\nNote that this does not apply to hidden or cell states. See the\\nInputs/Outputs sections below for details.  Default: False  dropout – If non-zero, introduces a Dropout layer on the outputs of each\\nLSTM layer except the last layer, with dropout probability equal to dropout . Default: 0  bidirectional – If True , becomes a bidirectional LSTM. Default: False  proj_size – If >0 , will use LSTM with projections of corresponding size. Default: 0  Inputs: input, (h_0, c_0)  input : tensor of shape(  L  ,  H  i  n  )  (L, H_{in})(L,Hin\\u200b)for unbatched input,(  L  ,  N  ,  H  i  n  )  (L, N, H_{in})(L,N,Hin\\u200b)when batch_first=False or(  N  ,  L  ,  H  i  n  )  (N, L, H_{in})(N,L,Hin\\u200b)when batch_first=True containing the features of\\nthe input sequence.  The input can also be a packed variable length sequence.\\nSee torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.  h_0 : tensor of shape(  D  ∗  num_layers  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, H_{out})(D∗num_layers,Hout\\u200b)for unbatched input or(  D  ∗  num_layers  ,  N  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, N, H_{out})(D∗num_layers,N,Hout\\u200b)containing the\\ninitial hidden state for each element in the input sequence.\\nDefaults to zeros if (h_0, c_0) is not provided.  c_0 : tensor of shape(  D  ∗  num_layers  ,  H  c  e  l  l  )  (D * \\\\text{num\\\\_layers}, H_{cell})(D∗num_layers,Hcell\\u200b)for unbatched input or(  D  ∗  num_layers  ,  N  ,  H  c  e  l  l  )  (D * \\\\text{num\\\\_layers}, N, H_{cell})(D∗num_layers,N,Hcell\\u200b)containing the\\ninitial cell state for each element in the input sequence.\\nDefaults to zeros if (h_0, c_0) is not provided.  where:  N  =  batch\\xa0size  L  =  sequence\\xa0length  D  =  2  if\\xa0bidirectional=True\\xa0otherwise  1  H  i  n  =  input_size  H  c  e  l  l  =  hidden_size  H  o  u  t  =  proj_size\\xa0if\\xa0proj_size  >  0  otherwise\\xa0hidden_size  \\\\begin{aligned}\\n    N ={} & \\\\text{batch size} \\\\\\\\\\n    L ={} & \\\\text{sequence length} \\\\\\\\\\n    D ={} & 2 \\\\text{ if bidirectional=True otherwise } 1 \\\\\\\\\\n    H_{in} ={} & \\\\text{input\\\\_size} \\\\\\\\\\n    H_{cell} ={} & \\\\text{hidden\\\\_size} \\\\\\\\\\n    H_{out} ={} & \\\\text{proj\\\\_size if } \\\\text{proj\\\\_size}>0 \\\\text{ otherwise hidden\\\\_size} \\\\\\\\\\n\\\\end{aligned}N=L=D=Hin\\u200b=Hcell\\u200b=Hout\\u200b=\\u200bbatch\\xa0sizesequence\\xa0length2if\\xa0bidirectional=True\\xa0otherwise1input_sizehidden_sizeproj_size\\xa0ifproj_size>0otherwise\\xa0hidden_size\\u200b  Outputs: output, (h_n, c_n)  output : tensor of shape(  L  ,  D  ∗  H  o  u  t  )  (L, D * H_{out})(L,D∗Hout\\u200b)for unbatched input,(  L  ,  N  ,  D  ∗  H  o  u  t  )  (L, N, D * H_{out})(L,N,D∗Hout\\u200b)when batch_first=False or(  N  ,  L  ,  D  ∗  H  o  u  t  )  (N, L, D * H_{out})(N,L,D∗Hout\\u200b)when batch_first=True containing the output features (h_t) from the last layer of the LSTM, for each t . If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output\\nwill also be a packed sequence. When bidirectional=True , output will contain\\na concatenation of the forward and reverse hidden states at each time step in the sequence.  h_n : tensor of shape(  D  ∗  num_layers  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, H_{out})(D∗num_layers,Hout\\u200b)for unbatched input or(  D  ∗  num_layers  ,  N  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, N, H_{out})(D∗num_layers,N,Hout\\u200b)containing the\\nfinal hidden state for each element in the sequence. When bidirectional=True , h_n will contain a concatenation of the final forward and reverse hidden states, respectively.  c_n : tensor of shape(  D  ∗  num_layers  ,  H  c  e  l  l  )  (D * \\\\text{num\\\\_layers}, H_{cell})(D∗num_layers,Hcell\\u200b)for unbatched input or(  D  ∗  num_layers  ,  N  ,  H  c  e  l  l  )  (D * \\\\text{num\\\\_layers}, N, H_{cell})(D∗num_layers,N,Hcell\\u200b)containing the\\nfinal cell state for each element in the sequence. When bidirectional=True , c_n will contain a concatenation of the final forward and reverse cell states, respectively.  Variables  weight_ih_l[k] – the learnable input-hidden weights of thek  t  h  \\\\text{k}^{th}kthlayer (W_ii|W_if|W_ig|W_io) , of shape (4*hidden_size, input_size) for k = 0 .\\nOtherwise, the shape is (4*hidden_size, num_directions * hidden_size) . If proj_size>0 was specified, the shape will be (4*hidden_size, num_directions * proj_size) for k > 0  weight_hh_l[k] – the learnable hidden-hidden weights of thek  t  h  \\\\text{k}^{th}kthlayer (W_hi|W_hf|W_hg|W_ho) , of shape (4*hidden_size, hidden_size) . If proj_size>0 was specified, the shape will be (4*hidden_size, proj_size) .  bias_ih_l[k] – the learnable input-hidden bias of thek  t  h  \\\\text{k}^{th}kthlayer (b_ii|b_if|b_ig|b_io) , of shape (4*hidden_size)  bias_hh_l[k] – the learnable hidden-hidden bias of thek  t  h  \\\\text{k}^{th}kthlayer (b_hi|b_hf|b_hg|b_ho) , of shape (4*hidden_size)  weight_hr_l[k] – the learnable projection weights of thek  t  h  \\\\text{k}^{th}kthlayer\\nof shape (proj_size, hidden_size) . Only present when proj_size>0 was\\nspecified.  weight_ih_l[k]_reverse – Analogous to weight_ih_l[k] for the reverse direction.\\nOnly present when bidirectional=True .  weight_hh_l[k]_reverse – Analogous to weight_hh_l[k] for the reverse direction.\\nOnly present when bidirectional=True .  bias_ih_l[k]_reverse – Analogous to bias_ih_l[k] for the reverse direction.\\nOnly present when bidirectional=True .  bias_hh_l[k]_reverse – Analogous to bias_hh_l[k] for the reverse direction.\\nOnly present when bidirectional=True .  weight_hr_l[k]_reverse – Analogous to weight_hr_l[k] for the reverse direction.\\nOnly present when bidirectional=True and proj_size>0 was specified.  Note  All the weights and biases are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  1  hidden_size  k = \\\\frac{1}{\\\\text{hidden\\\\_size}}k=hidden_size1\\u200b  Note  For bidirectional LSTMs, forward and backward are directions 0 and 1 respectively.\\nExample of splitting the output layers when batch_first=False : output.view(seq_len,batch,num_directions,hidden_size) .  Note  For bidirectional LSTMs, h_n is not equivalent to the last element of output ; the\\nformer contains the final forward and reverse hidden states, while the latter contains the\\nfinal forward hidden state and the initial reverse hidden state.  Note  batch_first argument is ignored for unbatched inputs.  Note  proj_size should be smaller than hidden_size .  Warning  There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA.\\nYou can enforce deterministic behavior by setting the following environment variables:  On CUDA 10.1, set environment variable CUDA_LAUNCH_BLOCKING=1 .\\nThis may affect performance.  On CUDA 10.2 or later, set environment variable\\n(note the leading colon symbol) CUBLAS_WORKSPACE_CONFIG=:16:8 or CUBLAS_WORKSPACE_CONFIG=:4096:2 .  See the cuDNN 8 Release Notes for more information.  Note  If the following conditions are satisfied:\\n1) cudnn is enabled,\\n2) input data is on the GPU\\n3) input data has dtype torch.float16 4) V100 GPU is used,\\n5) input data is not in PackedSequence format\\npersistent algorithm can be selected to improve performance.  Examples:  >>>rnn=nn.LSTM(10,20,2)>>>input=torch.randn(5,3,10)>>>h0=torch.randn(2,3,20)>>>c0=torch.randn(2,3,20)>>>output,(hn,cn)=rnn(input,(h0,c0))\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/rnn.py#L795', 'https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM', 'https://arxiv.org/abs/1402.1128', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html', 'https://pytorch.org/docs/stable/generated/torch.nn.GRU.html', 'https://pytorch.org/docs/stable/generated/torch.nn.RNN.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f5'), 'title': 'nn.GRU', 'page_text': 'GRU [LINK_1]  class torch.nn.GRU( input_size , hidden_size , num_layers=1 , bias=True , batch_first=False , dropout=0.0 , bidirectional=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Apply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\\nFor each element in the input sequence, each layer computes the following\\nfunction:  r  t  =  σ  (  W  i  r  x  t  +  b  i  r  +  W  h  r  h  (  t  −  1  )  +  b  h  r  )  z  t  =  σ  (  W  i  z  x  t  +  b  i  z  +  W  h  z  h  (  t  −  1  )  +  b  h  z  )  n  t  =  tanh  \\u2061  (  W  i  n  x  t  +  b  i  n  +  r  t  ⊙  (  W  h  n  h  (  t  −  1  )  +  b  h  n  )  )  h  t  =  (  1  −  z  t  )  ⊙  n  t  +  z  t  ⊙  h  (  t  −  1  )  \\\\begin{array}{ll}\\n    r_t = \\\\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\\\\\\n    z_t = \\\\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\\\\\\n    n_t = \\\\tanh(W_{in} x_t + b_{in} + r_t \\\\odot (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\\\\\\n    h_t = (1 - z_t) \\\\odot n_t + z_t \\\\odot h_{(t-1)}\\n\\\\end{array}rt\\u200b=σ(Wir\\u200bxt\\u200b+bir\\u200b+Whr\\u200bh(t−1)\\u200b+bhr\\u200b)zt\\u200b=σ(Wiz\\u200bxt\\u200b+biz\\u200b+Whz\\u200bh(t−1)\\u200b+bhz\\u200b)nt\\u200b=tanh(Win\\u200bxt\\u200b+bin\\u200b+rt\\u200b⊙(Whn\\u200bh(t−1)\\u200b+bhn\\u200b))ht\\u200b=(1−zt\\u200b)⊙nt\\u200b+zt\\u200b⊙h(t−1)\\u200b\\u200b  whereh  t  h_tht\\u200bis the hidden state at time t ,x  t  x_txt\\u200bis the input\\nat time t ,h  (  t  −  1  )  h_{(t-1)}h(t−1)\\u200bis the hidden state of the layer\\nat time t-1 or the initial hidden state at time 0 , andr  t  r_trt\\u200b,z  t  z_tzt\\u200b,n  t  n_tnt\\u200bare the reset, update, and new gates, respectively.σ  \\\\sigmaσis the sigmoid function, and⊙  \\\\odot⊙is the Hadamard product.  In a multilayer GRU, the inputx  t  (  l  )  x^{(l)}_txt(l)\\u200bof thel  ll-th layer\\n(l  ≥  2  l \\\\ge 2l≥2) is the hidden stateh  t  (  l  −  1  )  h^{(l-1)}_tht(l−1)\\u200bof the previous layer multiplied by\\ndropoutδ  t  (  l  −  1  )  \\\\delta^{(l-1)}_tδt(l−1)\\u200bwhere eachδ  t  (  l  −  1  )  \\\\delta^{(l-1)}_tδt(l−1)\\u200bis a Bernoulli random\\nvariable which is0  00with probability dropout .  Parameters  input_size – The number of expected features in the input x  hidden_size – The number of features in the hidden state h  num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU ,\\nwith the second GRU taking in outputs of the first GRU and\\ncomputing the final results. Default: 1  bias – If False , then the layer does not use bias weights b_ih and b_hh .\\nDefault: True  batch_first – If True , then the input and output tensors are provided\\nas (batch, seq, feature) instead of (seq, batch, feature) .\\nNote that this does not apply to hidden or cell states. See the\\nInputs/Outputs sections below for details.  Default: False  dropout – If non-zero, introduces a Dropout layer on the outputs of each\\nGRU layer except the last layer, with dropout probability equal to dropout . Default: 0  bidirectional – If True , becomes a bidirectional GRU. Default: False  Inputs: input, h_0  input : tensor of shape(  L  ,  H  i  n  )  (L, H_{in})(L,Hin\\u200b)for unbatched input,(  L  ,  N  ,  H  i  n  )  (L, N, H_{in})(L,N,Hin\\u200b)when batch_first=False or(  N  ,  L  ,  H  i  n  )  (N, L, H_{in})(N,L,Hin\\u200b)when batch_first=True containing the features of\\nthe input sequence.  The input can also be a packed variable length sequence.\\nSee torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.  h_0 : tensor of shape(  D  ∗  num_layers  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, H_{out})(D∗num_layers,Hout\\u200b)or(  D  ∗  num_layers  ,  N  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, N, H_{out})(D∗num_layers,N,Hout\\u200b)containing the initial hidden state for the input sequence. Defaults to zeros if not provided.  where:  N  =  batch\\xa0size  L  =  sequence\\xa0length  D  =  2  if\\xa0bidirectional=True\\xa0otherwise  1  H  i  n  =  input_size  H  o  u  t  =  hidden_size  \\\\begin{aligned}\\n    N ={} & \\\\text{batch size} \\\\\\\\\\n    L ={} & \\\\text{sequence length} \\\\\\\\\\n    D ={} & 2 \\\\text{ if bidirectional=True otherwise } 1 \\\\\\\\\\n    H_{in} ={} & \\\\text{input\\\\_size} \\\\\\\\\\n    H_{out} ={} & \\\\text{hidden\\\\_size}\\n\\\\end{aligned}N=L=D=Hin\\u200b=Hout\\u200b=\\u200bbatch\\xa0sizesequence\\xa0length2if\\xa0bidirectional=True\\xa0otherwise1input_sizehidden_size\\u200b  Outputs: output, h_n  output : tensor of shape(  L  ,  D  ∗  H  o  u  t  )  (L, D * H_{out})(L,D∗Hout\\u200b)for unbatched input,(  L  ,  N  ,  D  ∗  H  o  u  t  )  (L, N, D * H_{out})(L,N,D∗Hout\\u200b)when batch_first=False or(  N  ,  L  ,  D  ∗  H  o  u  t  )  (N, L, D * H_{out})(N,L,D∗Hout\\u200b)when batch_first=True containing the output features (h_t) from the last layer of the GRU, for each t . If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output\\nwill also be a packed sequence.  h_n : tensor of shape(  D  ∗  num_layers  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, H_{out})(D∗num_layers,Hout\\u200b)or(  D  ∗  num_layers  ,  N  ,  H  o  u  t  )  (D * \\\\text{num\\\\_layers}, N, H_{out})(D∗num_layers,N,Hout\\u200b)containing the final hidden state\\nfor the input sequence.  Variables  weight_ih_l[k] – the learnable input-hidden weights of thek  t  h  \\\\text{k}^{th}kthlayer\\n(W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0 .\\nOtherwise, the shape is (3*hidden_size, num_directions * hidden_size)  weight_hh_l[k] – the learnable hidden-hidden weights of thek  t  h  \\\\text{k}^{th}kthlayer\\n(W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size)  bias_ih_l[k] – the learnable input-hidden bias of thek  t  h  \\\\text{k}^{th}kthlayer\\n(b_ir|b_iz|b_in), of shape (3*hidden_size)  bias_hh_l[k] – the learnable hidden-hidden bias of thek  t  h  \\\\text{k}^{th}kthlayer\\n(b_hr|b_hz|b_hn), of shape (3*hidden_size)  Note  All the weights and biases are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  1  hidden_size  k = \\\\frac{1}{\\\\text{hidden\\\\_size}}k=hidden_size1\\u200b  Note  For bidirectional GRUs, forward and backward are directions 0 and 1 respectively.\\nExample of splitting the output layers when batch_first=False : output.view(seq_len,batch,num_directions,hidden_size) .  Note  batch_first argument is ignored for unbatched inputs.  Note  The calculation of new gaten  t  n_tnt\\u200bsubtly differs from the original paper and other frameworks.\\nIn the original implementation, the Hadamard product(  ⊙  )  (\\\\odot)(⊙)betweenr  t  r_trt\\u200band the\\nprevious hidden stateh  (  t  −  1  )  h_{(t-1)}h(t−1)\\u200bis done before the multiplication with the weight matrix W and addition of bias:  n  t  =  tanh  \\u2061  (  W  i  n  x  t  +  b  i  n  +  W  h  n  (  r  t  ⊙  h  (  t  −  1  )  )  +  b  h  n  )  \\\\begin{aligned}\\n    n_t = \\\\tanh(W_{in} x_t + b_{in} + W_{hn} ( r_t \\\\odot h_{(t-1)} ) + b_{hn})\\n\\\\end{aligned}nt\\u200b=tanh(Win\\u200bxt\\u200b+bin\\u200b+Whn\\u200b(rt\\u200b⊙h(t−1)\\u200b)+bhn\\u200b)\\u200b  This is in contrast to PyTorch implementation, which is done afterW  h  n  h  (  t  −  1  )  W_{hn} h_{(t-1)}Whn\\u200bh(t−1)\\u200b  n  t  =  tanh  \\u2061  (  W  i  n  x  t  +  b  i  n  +  r  t  ⊙  (  W  h  n  h  (  t  −  1  )  +  b  h  n  )  )  \\\\begin{aligned}\\n    n_t = \\\\tanh(W_{in} x_t + b_{in} + r_t \\\\odot (W_{hn} h_{(t-1)}+ b_{hn}))\\n\\\\end{aligned}nt\\u200b=tanh(Win\\u200bxt\\u200b+bin\\u200b+rt\\u200b⊙(Whn\\u200bh(t−1)\\u200b+bhn\\u200b))\\u200b  This implementation differs on purpose for efficiency.  Note  If the following conditions are satisfied:\\n1) cudnn is enabled,\\n2) input data is on the GPU\\n3) input data has dtype torch.float16 4) V100 GPU is used,\\n5) input data is not in PackedSequence format\\npersistent algorithm can be selected to improve performance.  Examples:  >>>rnn=nn.GRU(10,20,2)>>>input=torch.randn(5,3,10)>>>h0=torch.randn(2,3,20)>>>output,hn=rnn(input,h0)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#gru\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#gru', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#GRU', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/rnn.py#L1162', 'https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f6'), 'title': 'nn.RNNCell', 'page_text': 'RNNCell [LINK_1]  class torch.nn.RNNCell( input_size , hidden_size , bias=True , nonlinearity=\\'tanh\\' , device=None , dtype=None ) [source]  [source]  [LINK_2]  An Elman RNN cell with tanh or ReLU non-linearity.  h  ′  =  tanh  \\u2061  (  W  i  h  x  +  b  i  h  +  W  h  h  h  +  b  h  h  )  h\\' = \\\\tanh(W_{ih} x + b_{ih}  +  W_{hh} h + b_{hh})h′=tanh(Wih\\u200bx+bih\\u200b+Whh\\u200bh+bhh\\u200b)  If nonlinearity is ‘relu’ , then ReLU is used in place of tanh.  Parameters  input_size ( int ) – The number of expected features in the input x  hidden_size ( int ) – The number of features in the hidden state h  bias ( bool ) – If False , then the layer does not use bias weights b_ih and b_hh .\\nDefault: True  nonlinearity ( str ) – The non-linearity to use. Can be either \\'tanh\\' or \\'relu\\' . Default: \\'tanh\\'  Inputs: input, hidden  input : tensor containing input features  hidden : tensor containing the initial hidden state\\nDefaults to zero if not provided.  Outputs: h’  h’ of shape (batch, hidden_size) : tensor containing the next hidden state\\nfor each element in the batch  Shape:  input:(  N  ,  H  i  n  )  (N, H_{in})(N,Hin\\u200b)or(  H  i  n  )  (H_{in})(Hin\\u200b)tensor containing input features whereH  i  n  H_{in}Hin\\u200b= input_size .  hidden:(  N  ,  H  o  u  t  )  (N, H_{out})(N,Hout\\u200b)or(  H  o  u  t  )  (H_{out})(Hout\\u200b)tensor containing the initial hidden\\nstate whereH  o  u  t  H_{out}Hout\\u200b= hidden_size . Defaults to zero if not provided.  output:(  N  ,  H  o  u  t  )  (N, H_{out})(N,Hout\\u200b)or(  H  o  u  t  )  (H_{out})(Hout\\u200b)tensor containing the next hidden state.  Variables  weight_ih ( torch.Tensor ) – the learnable input-hidden weights, of shape (hidden_size, input_size)  weight_hh ( torch.Tensor ) – the learnable hidden-hidden weights, of shape (hidden_size, hidden_size)  bias_ih – the learnable input-hidden bias, of shape (hidden_size)  bias_hh – the learnable hidden-hidden bias, of shape (hidden_size)  Note  All the weights and biases are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  1  hidden_size  k = \\\\frac{1}{\\\\text{hidden\\\\_size}}k=hidden_size1\\u200b  Examples:  >>>rnn=nn.RNNCell(10,20)>>>input=torch.randn(6,3,10)>>>hx=torch.randn(3,20)>>>output=[]>>>foriinrange(6):...hx=rnn(input[i],hx)...output.append(hx)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html#rnncell\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html#torch.nn.RNNCell', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html#rnncell', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNNCell', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/rnn.py#L1491', 'https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html#torch.nn.RNNCell', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html', 'https://pytorch.org/docs/stable/generated/torch.nn.GRU.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f7'), 'title': 'nn.LSTMCell', 'page_text': 'LSTMCell [LINK_1]  class torch.nn.LSTMCell( input_size , hidden_size , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A long short-term memory (LSTM) cell.  i  =  σ  (  W  i  i  x  +  b  i  i  +  W  h  i  h  +  b  h  i  )  f  =  σ  (  W  i  f  x  +  b  i  f  +  W  h  f  h  +  b  h  f  )  g  =  tanh  \\u2061  (  W  i  g  x  +  b  i  g  +  W  h  g  h  +  b  h  g  )  o  =  σ  (  W  i  o  x  +  b  i  o  +  W  h  o  h  +  b  h  o  )  c  ′  =  f  ⊙  c  +  i  ⊙  g  h  ′  =  o  ⊙  tanh  \\u2061  (  c  ′  )  \\\\begin{array}{ll}\\ni = \\\\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\\\\\\nf = \\\\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\\\\\\ng = \\\\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\\\\\\no = \\\\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\\\\\\nc\\' = f \\\\odot c + i \\\\odot g \\\\\\\\\\nh\\' = o \\\\odot \\\\tanh(c\\') \\\\\\\\\\n\\\\end{array}i=σ(Wii\\u200bx+bii\\u200b+Whi\\u200bh+bhi\\u200b)f=σ(Wif\\u200bx+bif\\u200b+Whf\\u200bh+bhf\\u200b)g=tanh(Wig\\u200bx+big\\u200b+Whg\\u200bh+bhg\\u200b)o=σ(Wio\\u200bx+bio\\u200b+Who\\u200bh+bho\\u200b)c′=f⊙c+i⊙gh′=o⊙tanh(c′)\\u200b  whereσ  \\\\sigmaσis the sigmoid function, and⊙  \\\\odot⊙is the Hadamard product.  Parameters  input_size ( int ) – The number of expected features in the input x  hidden_size ( int ) – The number of features in the hidden state h  bias ( bool ) – If False , then the layer does not use bias weights b_ih and b_hh . Default: True  Inputs: input, (h_0, c_0)  input of shape (batch, input_size) or (input_size) : tensor containing input features  h_0 of shape (batch, hidden_size) or (hidden_size) : tensor containing the initial hidden state  c_0 of shape (batch, hidden_size) or (hidden_size) : tensor containing the initial cell state  If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.  Outputs: (h_1, c_1)  h_1 of shape (batch, hidden_size) or (hidden_size) : tensor containing the next hidden state  c_1 of shape (batch, hidden_size) or (hidden_size) : tensor containing the next cell state  Variables  weight_ih ( torch.Tensor ) – the learnable input-hidden weights, of shape (4*hidden_size, input_size)  weight_hh ( torch.Tensor ) – the learnable hidden-hidden weights, of shape (4*hidden_size, hidden_size)  bias_ih – the learnable input-hidden bias, of shape (4*hidden_size)  bias_hh – the learnable hidden-hidden bias, of shape (4*hidden_size)  Note  All the weights and biases are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  1  hidden_size  k = \\\\frac{1}{\\\\text{hidden\\\\_size}}k=hidden_size1\\u200b  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  Examples:  >>>rnn=nn.LSTMCell(10,20)# (input_size, hidden_size)>>>input=torch.randn(2,3,10)# (time_steps, batch, input_size)>>>hx=torch.randn(3,20)# (batch, hidden_size)>>>cx=torch.randn(3,20)>>>output=[]>>>foriinrange(input.size()[0]):...hx,cx=rnn(input[i],(hx,cx))...output.append(hx)>>>output=torch.stack(output,dim=0)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html#lstmcell\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html#lstmcell', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTMCell', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/rnn.py#L1610', 'https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html', 'https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f8'), 'title': 'nn.GRUCell', 'page_text': 'GRUCell [LINK_1]  class torch.nn.GRUCell( input_size , hidden_size , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A gated recurrent unit (GRU) cell.  r  =  σ  (  W  i  r  x  +  b  i  r  +  W  h  r  h  +  b  h  r  )  z  =  σ  (  W  i  z  x  +  b  i  z  +  W  h  z  h  +  b  h  z  )  n  =  tanh  \\u2061  (  W  i  n  x  +  b  i  n  +  r  ⊙  (  W  h  n  h  +  b  h  n  )  )  h  ′  =  (  1  −  z  )  ⊙  n  +  z  ⊙  h  \\\\begin{array}{ll}\\nr = \\\\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\\\\\\nz = \\\\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\\\\\\nn = \\\\tanh(W_{in} x + b_{in} + r \\\\odot (W_{hn} h + b_{hn})) \\\\\\\\\\nh\\' = (1 - z) \\\\odot n + z \\\\odot h\\n\\\\end{array}r=σ(Wir\\u200bx+bir\\u200b+Whr\\u200bh+bhr\\u200b)z=σ(Wiz\\u200bx+biz\\u200b+Whz\\u200bh+bhz\\u200b)n=tanh(Win\\u200bx+bin\\u200b+r⊙(Whn\\u200bh+bhn\\u200b))h′=(1−z)⊙n+z⊙h\\u200b  whereσ  \\\\sigmaσis the sigmoid function, and⊙  \\\\odot⊙is the Hadamard product.  Parameters  input_size ( int ) – The number of expected features in the input x  hidden_size ( int ) – The number of features in the hidden state h  bias ( bool ) – If False , then the layer does not use bias weights b_ih and b_hh . Default: True  Inputs: input, hidden  input : tensor containing input features  hidden : tensor containing the initial hidden\\nstate for each element in the batch.\\nDefaults to zero if not provided.  Outputs: h’  h’ : tensor containing the next hidden state\\nfor each element in the batch  Shape:  input:(  N  ,  H  i  n  )  (N, H_{in})(N,Hin\\u200b)or(  H  i  n  )  (H_{in})(Hin\\u200b)tensor containing input features whereH  i  n  H_{in}Hin\\u200b= input_size .  hidden:(  N  ,  H  o  u  t  )  (N, H_{out})(N,Hout\\u200b)or(  H  o  u  t  )  (H_{out})(Hout\\u200b)tensor containing the initial hidden\\nstate whereH  o  u  t  H_{out}Hout\\u200b= hidden_size . Defaults to zero if not provided.  output:(  N  ,  H  o  u  t  )  (N, H_{out})(N,Hout\\u200b)or(  H  o  u  t  )  (H_{out})(Hout\\u200b)tensor containing the next hidden state.  Variables  weight_ih ( torch.Tensor ) – the learnable input-hidden weights, of shape (3*hidden_size, input_size)  weight_hh ( torch.Tensor ) – the learnable hidden-hidden weights, of shape (3*hidden_size, hidden_size)  bias_ih – the learnable input-hidden bias, of shape (3*hidden_size)  bias_hh – the learnable hidden-hidden bias, of shape (3*hidden_size)  Note  All the weights and biases are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  1  hidden_size  k = \\\\frac{1}{\\\\text{hidden\\\\_size}}k=hidden_size1\\u200b  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  Examples:  >>>rnn=nn.GRUCell(10,20)>>>input=torch.randn(6,3,10)>>>hx=torch.randn(3,20)>>>output=[]>>>foriinrange(6):...hx=rnn(input[i],hx)...output.append(hx)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html#grucell\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html#torch.nn.GRUCell', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html#grucell', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#GRUCell', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/rnn.py#L1720', 'https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html#torch.nn.GRUCell', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179f9'), 'title': 'nn.Transformer', 'page_text': 'Transformer [LINK_1]  class torch.nn.Transformer( d_model=512 , nhead=8 , num_encoder_layers=6 , num_decoder_layers=6 , dim_feedforward=2048 , dropout=0.1 , activation=<functionrelu> , custom_encoder=None , custom_decoder=None , layer_norm_eps=1e-05 , batch_first=False , norm_first=False , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A transformer model.  Note  See this tutorial for an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.  User is able to modify the attributes as needed. The architecture\\nis based on the paper “Attention Is All You Need”. Ashish Vaswani, Noam Shazeer,\\nNiki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\\nIllia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\\nProcessing Systems, pages 6000-6010.  Parameters  d_model ( int ) – the number of expected features in the encoder/decoder inputs (default=512).  nhead ( int ) – the number of heads in the multiheadattention models (default=8).  num_encoder_layers ( int ) – the number of sub-encoder-layers in the encoder (default=6).  num_decoder_layers ( int ) – the number of sub-decoder-layers in the decoder (default=6).  dim_feedforward ( int ) – the dimension of the feedforward network model (default=2048).  dropout ( float ) – the dropout value (default=0.1).  activation ( Union  [  str  ,  Callable  [  [  Tensor  ]  ,  Tensor  ]  ] ) – the activation function of encoder/decoder intermediate layer, can be a string\\n(“relu” or “gelu”) or a unary callable. Default: relu  custom_encoder ( Optional  [  Any  ] ) – custom encoder (default=None).  custom_decoder ( Optional  [  Any  ] ) – custom decoder (default=None).  layer_norm_eps ( float ) – the eps value in layer normalization components (default=1e-5).  batch_first ( bool ) – If True , then the input and output tensors are provided\\nas (batch, seq, feature). Default: False (seq, batch, feature).  norm_first ( bool ) – if True , encoder and decoder layers will perform LayerNorms before\\nother attention and feedforward operations, otherwise after. Default: False (after).  bias ( bool ) – If set to False , Linear and LayerNorm layers will not learn an additive\\nbias. Default: True .  Examples::  >>>transformer_model=nn.Transformer(nhead=16,num_encoder_layers=12)>>>src=torch.rand((10,32,512))>>>tgt=torch.rand((20,32,512))>>>out=transformer_model(src,tgt)  Note: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model  forward( src , tgt , src_mask=None , tgt_mask=None , memory_mask=None , src_key_padding_mask=None , tgt_key_padding_mask=None , memory_key_padding_mask=None , src_is_causal=None , tgt_is_causal=None , memory_is_causal=False ) [source]  [source]  [LINK_3]  Take in and process masked source/target sequences.  Note  If a boolean tensor is provided for any of the [src/tgt/memory]_mask arguments, positions with a True value are\\nnot allowed to participate in the attention,\\nwhich is the opposite of the definition for attn_mask in torch.nn.functional.scaled_dot_product_attention() .  Parameters  src ( Tensor ) – the sequence to the encoder (required).  tgt ( Tensor ) – the sequence to the decoder (required).  src_mask ( Optional  [  Tensor  ] ) – the additive mask for the src sequence (optional).  tgt_mask ( Optional  [  Tensor  ] ) – the additive mask for the tgt sequence (optional).  memory_mask ( Optional  [  Tensor  ] ) – the additive mask for the encoder output (optional).  src_key_padding_mask ( Optional  [  Tensor  ] ) – the Tensor mask for src keys per batch (optional).  tgt_key_padding_mask ( Optional  [  Tensor  ] ) – the Tensor mask for tgt keys per batch (optional).  memory_key_padding_mask ( Optional  [  Tensor  ] ) – the Tensor mask for memory keys per batch (optional).  src_is_causal ( Optional  [  bool  ] ) – If specified, applies a causal mask as src_mask .\\nDefault: None ; try to detect a causal mask.\\nWarning: src_is_causal provides a hint that src_mask is\\nthe causal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.  tgt_is_causal ( Optional  [  bool  ] ) – If specified, applies a causal mask as tgt_mask .\\nDefault: None ; try to detect a causal mask.\\nWarning: tgt_is_causal provides a hint that tgt_mask is\\nthe causal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.  memory_is_causal ( bool ) – If specified, applies a causal mask as memory_mask .\\nDefault: False .\\nWarning: memory_is_causal provides a hint that memory_mask is the causal mask. Providing incorrect\\nhints can result in incorrect execution, including\\nforward and backward compatibility.  Return type  Tensor  Shape:  src:(  S  ,  E  )  (S, E)(S,E)for unbatched input,(  S  ,  N  ,  E  )  (S, N, E)(S,N,E)if batch_first=False or (N, S, E) if batch_first=True .  tgt:(  T  ,  E  )  (T, E)(T,E)for unbatched input,(  T  ,  N  ,  E  )  (T, N, E)(T,N,E)if batch_first=False or (N, T, E) if batch_first=True .  src_mask:(  S  ,  S  )  (S, S)(S,S)or(  N  ⋅  num_heads  ,  S  ,  S  )  (N\\\\cdot\\\\text{num\\\\_heads}, S, S)(N⋅num_heads,S,S).  tgt_mask:(  T  ,  T  )  (T, T)(T,T)or(  N  ⋅  num_heads  ,  T  ,  T  )  (N\\\\cdot\\\\text{num\\\\_heads}, T, T)(N⋅num_heads,T,T).  memory_mask:(  T  ,  S  )  (T, S)(T,S).  src_key_padding_mask:(  S  )  (S)(S)for unbatched input otherwise(  N  ,  S  )  (N, S)(N,S).  tgt_key_padding_mask:(  T  )  (T)(T)for unbatched input otherwise(  N  ,  T  )  (N, T)(N,T).  memory_key_padding_mask:(  S  )  (S)(S)for unbatched input otherwise(  N  ,  S  )  (N, S)(N,S).  Note: [src/tgt/memory]_mask ensures that positioni  iiis allowed to attend the unmasked\\npositions. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor\\nis provided, it will be added to the attention weight.\\n[src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\\nthe attention. If a BoolTensor is provided, the positions with the\\nvalue of True will be ignored while the position with the value of False will be unchanged.  output:(  T  ,  E  )  (T, E)(T,E)for unbatched input,(  T  ,  N  ,  E  )  (T, N, E)(T,N,E)if batch_first=False or (N, T, E) if batch_first=True .  Note: Due to the multi-head attention architecture in the transformer model,\\nthe output sequence length of a transformer is same as the input sequence\\n(i.e. target) length of the decoder.  whereS  SSis the source sequence length,T  TTis the target sequence length,N  NNis the\\nbatch size,E  EEis the feature number  Examples  >>>output=transformer_model(src,tgt,src_mask=src_mask,tgt_mask=tgt_mask)  static generate_square_subsequent_mask( sz , device=None , dtype=None ) [source]  [source]  [LINK_4]  Generate a square causal mask for the sequence.  The masked positions are filled with float(‘-inf’). Unmasked positions are filled with float(0.0).  Return type  Tensor\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#transformer\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.generate_square_subsequent_mask', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#transformer', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L57', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer', 'https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/typing.html#typing.Callable', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://github.com/pytorch/examples/tree/master/word_language_model', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L173', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer.generate_square_subsequent_mask', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L291', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.generate_square_subsequent_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html', 'https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179fa'), 'title': 'nn.TransformerEncoder', 'page_text': 'TransformerEncoder [LINK_1]  class torch.nn.TransformerEncoder( encoder_layer , num_layers , norm=None , enable_nested_tensor=True , mask_check=True ) [source]  [source]  [LINK_2]  TransformerEncoder is a stack of N encoder layers.  Note  See this tutorial for an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.  Users can build the BERT( https://arxiv.org/abs/1810.04805 ) model with corresponding parameters.  Parameters  encoder_layer ( TransformerEncoderLayer ) – an instance of the TransformerEncoderLayer() class (required).  num_layers ( int ) – the number of sub-encoder-layers in the encoder (required).  norm ( Optional  [  Module  ] ) – the layer normalization component (optional).  enable_nested_tensor ( bool ) – if True, input will automatically convert to nested tensor\\n(and convert back on output). This will improve the overall performance of\\nTransformerEncoder when padding rate is high. Default: True (enabled).  Examples::  >>>encoder_layer=nn.TransformerEncoderLayer(d_model=512,nhead=8)>>>transformer_encoder=nn.TransformerEncoder(encoder_layer,num_layers=6)>>>src=torch.rand(10,32,512)>>>out=transformer_encoder(src)  forward( src , mask=None , src_key_padding_mask=None , is_causal=None ) [source]  [source]  [LINK_3]  Pass the input through the encoder layers in turn.  Parameters  src ( Tensor ) – the sequence to the encoder (required).  mask ( Optional  [  Tensor  ] ) – the mask for the src sequence (optional).  src_key_padding_mask ( Optional  [  Tensor  ] ) – the mask for the src keys per batch (optional).  is_causal ( Optional  [  bool  ] ) – If specified, applies a causal mask as mask .\\nDefault: None ; try to detect a causal mask.\\nWarning: is_causal provides a hint that mask is the\\ncausal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.  Return type  Tensor  Shape:  see the docs in Transformer .\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#transformerencoder\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder.forward', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#transformerencoder', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoder', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L310', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder', 'https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html', 'https://arxiv.org/abs/1810.04805', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoder.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L390', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder.forward', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179fb'), 'title': 'nn.TransformerDecoder', 'page_text': 'TransformerDecoder [LINK_1]  class torch.nn.TransformerDecoder( decoder_layer , num_layers , norm=None ) [source]  [source]  [LINK_2]  TransformerDecoder is a stack of N decoder layers.  Note  See this tutorial for an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.  Parameters  decoder_layer ( TransformerDecoderLayer ) – an instance of the TransformerDecoderLayer() class (required).  num_layers ( int ) – the number of sub-decoder-layers in the decoder (required).  norm ( Optional  [  Module  ] ) – the layer normalization component (optional).  Examples::  >>>decoder_layer=nn.TransformerDecoderLayer(d_model=512,nhead=8)>>>transformer_decoder=nn.TransformerDecoder(decoder_layer,num_layers=6)>>>memory=torch.rand(10,32,512)>>>tgt=torch.rand(20,32,512)>>>out=transformer_decoder(tgt,memory)  forward( tgt , memory , tgt_mask=None , memory_mask=None , tgt_key_padding_mask=None , memory_key_padding_mask=None , tgt_is_causal=None , memory_is_causal=False ) [source]  [source]  [LINK_3]  Pass the inputs (and mask) through the decoder layer in turn.  Parameters  tgt ( Tensor ) – the sequence to the decoder (required).  memory ( Tensor ) – the sequence from the last layer of the encoder (required).  tgt_mask ( Optional  [  Tensor  ] ) – the mask for the tgt sequence (optional).  memory_mask ( Optional  [  Tensor  ] ) – the mask for the memory sequence (optional).  tgt_key_padding_mask ( Optional  [  Tensor  ] ) – the mask for the tgt keys per batch (optional).  memory_key_padding_mask ( Optional  [  Tensor  ] ) – the mask for the memory keys per batch (optional).  tgt_is_causal ( Optional  [  bool  ] ) – If specified, applies a causal mask as tgtmask .\\nDefault: None ; try to detect a causal mask.\\nWarning: tgt_is_causal provides a hint that tgt_mask is\\nthe causal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.  memory_is_causal ( bool ) – If specified, applies a causal mask as memorymask .\\nDefault: False .\\nWarning: memory_is_causal provides a hint that memory_mask is the causal mask. Providing incorrect\\nhints can result in incorrect execution, including\\nforward and backward compatibility.  Return type  Tensor  Shape:  see the docs in Transformer .\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#transformerdecoder\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder.forward', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#transformerdecoder', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoder', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L533', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder', 'https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoder.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L568', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder.forward', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179fc'), 'title': 'nn.TransformerEncoderLayer', 'page_text': 'TransformerEncoderLayer [LINK_1]  class torch.nn.TransformerEncoderLayer( d_model , nhead , dim_feedforward=2048 , dropout=0.1 , activation=<functionrelu> , layer_norm_eps=1e-05 , batch_first=False , norm_first=False , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  TransformerEncoderLayer is made up of self-attn and feedforward network.  Note  See this tutorial for an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.  This standard encoder layer is based on the paper “Attention Is All You Need”.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nLukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\\nNeural Information Processing Systems, pages 6000-6010. Users may modify or implement\\nin a different way during application.  TransformerEncoderLayer can handle either traditional torch.tensor inputs,\\nor Nested Tensor inputs.  Derived classes are expected to similarly accept\\nboth input formats.  (Not all combinations of inputs are currently\\nsupported by TransformerEncoderLayer while Nested Tensor is in prototype\\nstate.)  If you are implementing a custom layer, you may derive it either from\\nthe Module or TransformerEncoderLayer class.  If your custom layer\\nsupports both torch.Tensors and Nested Tensors inputs, make its\\nimplementation a derived class of TransformerEncoderLayer. If your custom\\nLayer supports only torch.Tensor inputs, derive its implementation from\\nModule.  Parameters  d_model ( int ) – the number of expected features in the input (required).  nhead ( int ) – the number of heads in the multiheadattention models (required).  dim_feedforward ( int ) – the dimension of the feedforward network model (default=2048).  dropout ( float ) – the dropout value (default=0.1).  activation ( Union  [  str  ,  Callable  [  [  Tensor  ]  ,  Tensor  ]  ] ) – the activation function of the intermediate layer, can be a string\\n(“relu” or “gelu”) or a unary callable. Default: relu  layer_norm_eps ( float ) – the eps value in layer normalization components (default=1e-5).  batch_first ( bool ) – If True , then the input and output tensors are provided\\nas (batch, seq, feature). Default: False (seq, batch, feature).  norm_first ( bool ) – if True , layer norm is done prior to attention and feedforward\\noperations, respectively. Otherwise it’s done after. Default: False (after).  bias ( bool ) – If set to False , Linear and LayerNorm layers will not learn an additive\\nbias. Default: True .  Examples::  >>>encoder_layer=nn.TransformerEncoderLayer(d_model=512,nhead=8)>>>src=torch.rand(10,32,512)>>>out=encoder_layer(src)  Alternatively, when batch_first is True :  >>>encoder_layer=nn.TransformerEncoderLayer(d_model=512,nhead=8,batch_first=True)>>>src=torch.rand(32,10,512)>>>out=encoder_layer(src)  Fast path:  forward() will use a special optimized implementation described in FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness if all of the following\\nconditions are met:  Either autograd is disabled (using torch.inference_mode or torch.no_grad ) or no tensor\\nargument requires_grad  training is disabled (using .eval() )  batch_first is True and the input is batched (i.e., src.dim()==3 )  activation is one of: \"relu\" , \"gelu\" , torch.functional.relu , or torch.functional.gelu  at most one of src_mask and src_key_padding_mask is passed  if src is a NestedTensor , neither src_mask nor src_key_padding_mask is passed  the two LayerNorm instances have a consistent eps value (this will naturally be the case\\nunless the caller has manually modified one without modifying the other)  If the optimized implementation is in use, a NestedTensor can be\\npassed for src to represent padding more efficiently than using a padding\\nmask. In this case, a NestedTensor will be\\nreturned, and an additional speedup proportional to the fraction of the input that\\nis padding can be expected.  forward( src , src_mask=None , src_key_padding_mask=None , is_causal=False ) [source]  [source]  [LINK_3]  Pass the input through the encoder layer.  Parameters  src ( Tensor ) – the sequence to the encoder layer (required).  src_mask ( Optional  [  Tensor  ] ) – the mask for the src sequence (optional).  src_key_padding_mask ( Optional  [  Tensor  ] ) – the mask for the src keys per batch (optional).  is_causal ( bool ) – If specified, applies a causal mask as srcmask .\\nDefault: False .\\nWarning: is_causal provides a hint that src_mask is the\\ncausal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.  Return type  Tensor  Shape:  see the docs in Transformer .\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#transformerencoderlayer\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer.forward', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#transformerencoderlayer', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L630', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer', 'https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/typing.html#typing.Callable', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://arxiv.org/abs/2205.14135', 'https://pytorch.org/docs/stable/nested.html', 'https://pytorch.org/docs/stable/nested.html', 'https://pytorch.org/docs/stable/nested.html', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L766', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer.forward', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179fd'), 'title': 'nn.TransformerDecoderLayer', 'page_text': 'TransformerDecoderLayer [LINK_1]  class torch.nn.TransformerDecoderLayer( d_model , nhead , dim_feedforward=2048 , dropout=0.1 , activation=<functionrelu> , layer_norm_eps=1e-05 , batch_first=False , norm_first=False , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.  Note  See this tutorial for an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.  This standard decoder layer is based on the paper “Attention Is All You Need”.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nLukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\\nNeural Information Processing Systems, pages 6000-6010. Users may modify or implement\\nin a different way during application.  Parameters  d_model ( int ) – the number of expected features in the input (required).  nhead ( int ) – the number of heads in the multiheadattention models (required).  dim_feedforward ( int ) – the dimension of the feedforward network model (default=2048).  dropout ( float ) – the dropout value (default=0.1).  activation ( Union  [  str  ,  Callable  [  [  Tensor  ]  ,  Tensor  ]  ] ) – the activation function of the intermediate layer, can be a string\\n(“relu” or “gelu”) or a unary callable. Default: relu  layer_norm_eps ( float ) – the eps value in layer normalization components (default=1e-5).  batch_first ( bool ) – If True , then the input and output tensors are provided\\nas (batch, seq, feature). Default: False (seq, batch, feature).  norm_first ( bool ) – if True , layer norm is done prior to self attention, multihead\\nattention and feedforward operations, respectively. Otherwise it’s done after.\\nDefault: False (after).  bias ( bool ) – If set to False , Linear and LayerNorm layers will not learn an additive\\nbias. Default: True .  Examples::  >>>decoder_layer=nn.TransformerDecoderLayer(d_model=512,nhead=8)>>>memory=torch.rand(10,32,512)>>>tgt=torch.rand(20,32,512)>>>out=decoder_layer(tgt,memory)  Alternatively, when batch_first is True :  >>>decoder_layer=nn.TransformerDecoderLayer(d_model=512,nhead=8,batch_first=True)>>>memory=torch.rand(32,10,512)>>>tgt=torch.rand(32,20,512)>>>out=decoder_layer(tgt,memory)  forward( tgt , memory , tgt_mask=None , memory_mask=None , tgt_key_padding_mask=None , memory_key_padding_mask=None , tgt_is_causal=False , memory_is_causal=False ) [source]  [source]  [LINK_3]  Pass the inputs (and mask) through the decoder layer.  Parameters  tgt ( Tensor ) – the sequence to the decoder layer (required).  memory ( Tensor ) – the sequence from the last layer of the encoder (required).  tgt_mask ( Optional  [  Tensor  ] ) – the mask for the tgt sequence (optional).  memory_mask ( Optional  [  Tensor  ] ) – the mask for the memory sequence (optional).  tgt_key_padding_mask ( Optional  [  Tensor  ] ) – the mask for the tgt keys per batch (optional).  memory_key_padding_mask ( Optional  [  Tensor  ] ) – the mask for the memory keys per batch (optional).  tgt_is_causal ( bool ) – If specified, applies a causal mask as tgtmask .\\nDefault: False .\\nWarning: tgt_is_causal provides a hint that tgt_mask is\\nthe causal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.  memory_is_causal ( bool ) – If specified, applies a causal mask as memorymask .\\nDefault: False .\\nWarning: memory_is_causal provides a hint that memory_mask is the causal mask. Providing incorrect\\nhints can result in incorrect execution, including\\nforward and backward compatibility.  Return type  Tensor  Shape:  see the docs in Transformer .\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#transformerdecoderlayer\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer.forward', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#transformerdecoderlayer', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L951', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer', 'https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/typing.html#typing.Callable', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/transformer.py#L1052', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer.forward', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer', 'https://pytorch.org/docs/stable/generated/torch.nn.Identity.html', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179fe'), 'title': 'nn.Identity', 'page_text': 'Identity [LINK_1]  class torch.nn.Identity( *args , **kwargs ) [source]  [source]  [LINK_2]  A placeholder identity operator that is argument-insensitive.  Parameters  args ( Any ) – any argument (unused)  kwargs ( Any ) – any keyword argument (unused)  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Identity(54,unused_argument1=0.1,unused_argument2=False)>>>input=torch.randn(128,20)>>>output=m(input)>>>print(output.size())torch.Size([128, 20])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#identity\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#torch.nn.Identity', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#identity', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Identity', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/linear.py#L22', 'https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#torch.nn.Identity', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://docs.python.org/3/library/typing.html#typing.Any', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html', 'https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe0179ff'), 'title': 'nn.Linear', 'page_text': 'Linear [LINK_1]  class torch.nn.Linear( in_features , out_features , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies an affine linear transformation to the incoming data:y  =  x  A  T  +  b  y = xA^T + by=xAT+b.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  Parameters  in_features ( int ) – size of each input sample  out_features ( int ) – size of each output sample  bias ( bool ) – If set to False , the layer will not learn an additive bias.\\nDefault: True  Shape:  Input:(  ∗  ,  H  i  n  )  (*, H_{in})(∗,Hin\\u200b)where∗  *∗means any number of\\ndimensions including none andH  i  n  =  in_features  H_{in} = \\\\text{in\\\\_features}Hin\\u200b=in_features.  Output:(  ∗  ,  H  o  u  t  )  (*, H_{out})(∗,Hout\\u200b)where all but the last dimension\\nare the same shape as the input andH  o  u  t  =  out_features  H_{out} = \\\\text{out\\\\_features}Hout\\u200b=out_features.  Variables  weight ( torch.Tensor ) – the learnable weights of the module of shape(  out_features  ,  in_features  )  (\\\\text{out\\\\_features}, \\\\text{in\\\\_features})(out_features,in_features). The values are\\ninitialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b), wherek  =  1  in_features  k = \\\\frac{1}{\\\\text{in\\\\_features}}k=in_features1\\u200b  bias – the learnable bias of the module of shape(  out_features  )  (\\\\text{out\\\\_features})(out_features).\\nIf bias is True , the values are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  1  in_features  k = \\\\frac{1}{\\\\text{in\\\\_features}}k=in_features1\\u200b  Examples:  >>>m=nn.Linear(20,30)>>>input=torch.randn(128,20)>>>output=m(input)>>>print(output.size())torch.Size([128, 30])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/linear.py#L50', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere', 'https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Identity.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a00'), 'title': 'nn.Bilinear', 'page_text': 'Bilinear [LINK_1]  class torch.nn.Bilinear( in1_features , in2_features , out_features , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a bilinear transformation to the incoming data:y  =  x  1  T  A  x  2  +  b  y = x_1^T A x_2 + by=x1T\\u200bAx2\\u200b+b.  Parameters  in1_features ( int ) – size of each first input sample  in2_features ( int ) – size of each second input sample  out_features ( int ) – size of each output sample  bias ( bool ) – If set to False, the layer will not learn an additive bias.\\nDefault: True  Shape:  Input1:(  ∗  ,  H  i  n  1  )  (*, H_{in1})(∗,Hin1\\u200b)whereH  i  n  1  =  in1_features  H_{in1}=\\\\text{in1\\\\_features}Hin1\\u200b=in1_featuresand∗  *∗means any number of additional dimensions including none. All but the last dimension\\nof the inputs should be the same.  Input2:(  ∗  ,  H  i  n  2  )  (*, H_{in2})(∗,Hin2\\u200b)whereH  i  n  2  =  in2_features  H_{in2}=\\\\text{in2\\\\_features}Hin2\\u200b=in2_features.  Output:(  ∗  ,  H  o  u  t  )  (*, H_{out})(∗,Hout\\u200b)whereH  o  u  t  =  out_features  H_{out}=\\\\text{out\\\\_features}Hout\\u200b=out_featuresand all but the last dimension are the same shape as the input.  Variables  weight ( torch.Tensor ) – the learnable weights of the module of shape(  out_features  ,  in1_features  ,  in2_features  )  (\\\\text{out\\\\_features}, \\\\text{in1\\\\_features}, \\\\text{in2\\\\_features})(out_features,in1_features,in2_features).\\nThe values are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b), wherek  =  1  in1_features  k = \\\\frac{1}{\\\\text{in1\\\\_features}}k=in1_features1\\u200b  bias – the learnable bias of the module of shape(  out_features  )  (\\\\text{out\\\\_features})(out_features).\\nIf bias is True , the values are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b), wherek  =  1  in1_features  k = \\\\frac{1}{\\\\text{in1\\\\_features}}k=in1_features1\\u200b  Examples:  >>>m=nn.Bilinear(20,30,40)>>>input1=torch.randn(128,20)>>>input2=torch.randn(128,30)>>>output=m(input1,input2)>>>print(output.size())torch.Size([128, 40])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html#bilinear\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html#torch.nn.Bilinear', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html#bilinear', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Bilinear', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/linear.py#L150', 'https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html#torch.nn.Bilinear', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a01'), 'title': 'nn.LazyLinear', 'page_text': 'LazyLinear [LINK_1]  class torch.nn.LazyLinear( out_features , bias=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.Linear module where in_features is inferred.  In this module, the weight and bias are of torch.nn.UninitializedParameter class. They will be initialized after the first call to forward is done and the\\nmodule will become a regular torch.nn.Linear module. The in_features argument\\nof the Linear is inferred from the input.shape[-1] .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_features ( int ) – size of each output sample  bias ( UninitializedParameter ) – If set to False , the layer will not learn an additive bias.\\nDefault: True  Variables  weight ( torch.nn.parameter.UninitializedParameter ) – the learnable weights of the module of shape(  out_features  ,  in_features  )  (\\\\text{out\\\\_features}, \\\\text{in\\\\_features})(out_features,in_features). The values are\\ninitialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b), wherek  =  1  in_features  k = \\\\frac{1}{\\\\text{in\\\\_features}}k=in_features1\\u200b  bias ( torch.nn.parameter.UninitializedParameter ) – the learnable bias of the module of shape(  out_features  )  (\\\\text{out\\\\_features})(out_features).\\nIf bias is True , the values are initialized fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  1  in_features  k = \\\\frac{1}{\\\\text{in\\\\_features}}k=in_features1\\u200b  cls_to_become [source]  [LINK_3]  alias of Linear\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#lazylinear\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear.cls_to_become', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#lazylinear', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#LazyLinear', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/linear.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/linear.py#L50', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear.cls_to_become', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a02'), 'title': 'nn.Dropout', 'page_text': 'Dropout [LINK_1]  class torch.nn.Dropout( p=0.5 , inplace=False ) [source]  [source]  [LINK_2]  During training, randomly zeroes some of the elements of the input tensor with probability p .  The zeroed elements are chosen independently for each forward call and are sampled from a Bernoulli distribution.  Each channel will be zeroed out independently on every forward call.  This has proven to be an effective technique for regularization and\\npreventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature\\ndetectors .  Furthermore, the outputs are scaled by a factor of1  1  −  p  \\\\frac{1}{1-p}1−p1\\u200bduring\\ntraining. This means that during evaluation the module simply computes an\\nidentity function.  Parameters  p ( float ) – probability of an element to be zeroed. Default: 0.5  inplace ( bool ) – If set to True , will do this operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗). Input can be of any shape  Output:(  ∗  )  (*)(∗). Output is of the same shape as input  Examples:  >>>m=nn.Dropout(p=0.2)>>>input=torch.randn(20,16)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#Dropout', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L35', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout', 'https://arxiv.org/abs/1207.0580', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout1d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a03'), 'title': 'nn.Dropout1d', 'page_text': 'Dropout1d [LINK_1]  class torch.nn.Dropout1d( p=0.5 , inplace=False ) [source]  [source]  [LINK_2]  Randomly zero out entire channels.  A channel is a 1D feature map,\\ne.g., thej  jj-th channel of thei  ii-th sample in the\\nbatched input is a 1D tensorinput  [  i  ,  j  ]  \\\\text{input}[i, j]input[i,j].  Each channel will be zeroed out independently on every forward call with\\nprobability p using samples from a Bernoulli distribution.  Usually the input comes from nn.Conv1d modules.  As described in the paper Efficient Object Localization Using Convolutional Networks ,\\nif adjacent pixels within feature maps are strongly correlated\\n(as is normally the case in early convolution layers) then i.i.d. dropout\\nwill not regularize the activations and will otherwise just result\\nin an effective learning rate decrease.  In this case, nn.Dropout1d() will help promote independence between\\nfeature maps and should be used instead.  Parameters  p ( float  ,  optional ) – probability of an element to be zero-ed.  inplace ( bool  ,  optional ) – If set to True , will do this operation\\nin-place  Shape:  Input:(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)or(  C  ,  L  )  (C, L)(C,L).  Output:(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)or(  C  ,  L  )  (C, L)(C,L)(same shape as input).  Examples:  >>>m=nn.Dropout1d(p=0.2)>>>input=torch.randn(20,16,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Dropout1d.html#dropout1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Dropout1d.html#torch.nn.Dropout1d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Dropout1d.html#dropout1d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#Dropout1d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L73', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout1d.html#torch.nn.Dropout1d', 'https://arxiv.org/abs/1411.4280', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a04'), 'title': 'nn.Dropout2d', 'page_text': 'Dropout2d [LINK_1]  class torch.nn.Dropout2d( p=0.5 , inplace=False ) [source]  [source]  [LINK_2]  Randomly zero out entire channels.  A channel is a 2D feature map,\\ne.g., thej  jj-th channel of thei  ii-th sample in the\\nbatched input is a 2D tensorinput  [  i  ,  j  ]  \\\\text{input}[i, j]input[i,j].  Each channel will be zeroed out independently on every forward call with\\nprobability p using samples from a Bernoulli distribution.  Usually the input comes from nn.Conv2d modules.  As described in the paper Efficient Object Localization Using Convolutional Networks ,\\nif adjacent pixels within feature maps are strongly correlated\\n(as is normally the case in early convolution layers) then i.i.d. dropout\\nwill not regularize the activations and will otherwise just result\\nin an effective learning rate decrease.  In this case, nn.Dropout2d() will help promote independence between\\nfeature maps and should be used instead.  Parameters  p ( float  ,  optional ) – probability of an element to be zero-ed.  inplace ( bool  ,  optional ) – If set to True , will do this operation\\nin-place  Warning  Due to historical reasons, this class will perform 1D channel-wise dropout\\nfor 3D inputs (as done by nn.Dropout1d ). Thus, it currently does NOT\\nsupport inputs without a batch dimension of shape(  C  ,  H  ,  W  )  (C, H, W)(C,H,W). This\\nbehavior will change in a future release to interpret 3D inputs as no-batch-dim\\ninputs. To maintain the old behavior, switch to nn.Dropout1d .  Shape:  Input:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  N  ,  C  ,  L  )  (N, C, L)(N,C,L).  Output:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)(same shape as input).  Examples:  >>>m=nn.Dropout2d(p=0.2)>>>input=torch.randn(20,16,32,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#dropout2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#dropout2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#Dropout2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L118', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d', 'https://arxiv.org/abs/1411.4280', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout3d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout1d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a05'), 'title': 'nn.Dropout3d', 'page_text': 'Dropout3d [LINK_1]  class torch.nn.Dropout3d( p=0.5 , inplace=False ) [source]  [source]  [LINK_2]  Randomly zero out entire channels.  A channel is a 3D feature map,\\ne.g., thej  jj-th channel of thei  ii-th sample in the\\nbatched input is a 3D tensorinput  [  i  ,  j  ]  \\\\text{input}[i, j]input[i,j].  Each channel will be zeroed out independently on every forward call with\\nprobability p using samples from a Bernoulli distribution.  Usually the input comes from nn.Conv3d modules.  As described in the paper Efficient Object Localization Using Convolutional Networks ,\\nif adjacent pixels within feature maps are strongly correlated\\n(as is normally the case in early convolution layers) then i.i.d. dropout\\nwill not regularize the activations and will otherwise just result\\nin an effective learning rate decrease.  In this case, nn.Dropout3d() will help promote independence between\\nfeature maps and should be used instead.  Parameters  p ( float  ,  optional ) – probability of an element to be zeroed.  inplace ( bool  ,  optional ) – If set to True , will do this operation\\nin-place  Shape:  Input:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W).  Output:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W)(same shape as input).  Examples:  >>>m=nn.Dropout3d(p=0.2)>>>input=torch.randn(20,16,4,32,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Dropout3d.html#dropout3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Dropout3d.html#dropout3d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#Dropout3d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L170', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d', 'https://arxiv.org/abs/1411.4280', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.AlphaDropout.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a06'), 'title': 'nn.AlphaDropout', 'page_text': 'AlphaDropout [LINK_1]  class torch.nn.AlphaDropout( p=0.5 , inplace=False ) [source]  [source]  [LINK_2]  Applies Alpha Dropout over the input.  Alpha Dropout is a type of Dropout that maintains the self-normalizing\\nproperty.\\nFor an input with zero mean and unit standard deviation, the output of\\nAlpha Dropout maintains the original mean and standard deviation of the\\ninput.\\nAlpha Dropout goes hand-in-hand with SELU activation function, which ensures\\nthat the outputs have zero mean and unit standard deviation.  During training, it randomly masks some of the elements of the input\\ntensor with probability p using samples from a bernoulli distribution.\\nThe elements to masked are randomized on every forward call, and scaled\\nand shifted to maintain zero mean and unit standard deviation.  During evaluation the module simply computes an identity function.  More details can be found in the paper Self-Normalizing Neural Networks .  Parameters  p ( float ) – probability of an element to be dropped. Default: 0.5  inplace ( bool  ,  optional ) – If set to True , will do this operation\\nin-place  Shape:  Input:(  ∗  )  (*)(∗). Input can be of any shape  Output:(  ∗  )  (*)(∗). Output is of the same shape as input  Examples:  >>>m=nn.AlphaDropout(p=0.2)>>>input=torch.randn(20,16)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AlphaDropout.html#alphadropout\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.AlphaDropout.html#alphadropout', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#AlphaDropout', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L215', 'https://pytorch.org/docs/stable/generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout', 'https://arxiv.org/abs/1706.02515', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.FeatureAlphaDropout.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Dropout3d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a07'), 'title': 'nn.FeatureAlphaDropout', 'page_text': 'FeatureAlphaDropout [LINK_1]  class torch.nn.FeatureAlphaDropout( p=0.5 , inplace=False ) [source]  [source]  [LINK_2]  Randomly masks out entire channels.  A channel is a feature map,\\ne.g. thej  jj-th channel of thei  ii-th sample in the batch input\\nis a tensorinput  [  i  ,  j  ]  \\\\text{input}[i, j]input[i,j]of the input tensor). Instead of\\nsetting activations to zero, as in regular Dropout, the activations are set\\nto the negative saturation value of the SELU activation function. More details\\ncan be found in the paper Self-Normalizing Neural Networks .  Each element will be masked independently for each sample on every forward\\ncall with probability p using samples from a Bernoulli distribution.\\nThe elements to be masked are randomized on every forward call, and scaled\\nand shifted to maintain zero mean and unit variance.  Usually the input comes from nn.AlphaDropout modules.  As described in the paper Efficient Object Localization Using Convolutional Networks ,\\nif adjacent pixels within feature maps are strongly correlated\\n(as is normally the case in early convolution layers) then i.i.d. dropout\\nwill not regularize the activations and will otherwise just result\\nin an effective learning rate decrease.  In this case, nn.AlphaDropout() will help promote independence between\\nfeature maps and should be used instead.  Parameters  p ( float  ,  optional ) – probability of an element to be zeroed. Default: 0.5  inplace ( bool  ,  optional ) – If set to True , will do this operation\\nin-place  Shape:  Input:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W).  Output:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)or(  C  ,  D  ,  H  ,  W  )  (C, D, H, W)(C,D,H,W)(same shape as input).  Examples:  >>>m=nn.FeatureAlphaDropout(p=0.2)>>>input=torch.randn(20,16,4,32,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.FeatureAlphaDropout.html#featurealphadropout\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.FeatureAlphaDropout.html#featurealphadropout', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#FeatureAlphaDropout', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L257', 'https://pytorch.org/docs/stable/generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout', 'https://arxiv.org/abs/1706.02515', 'https://arxiv.org/abs/1411.4280', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html', 'https://pytorch.org/docs/stable/generated/torch.nn.AlphaDropout.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a08'), 'title': 'nn.Embedding', 'page_text': 'Embedding [LINK_1]  class torch.nn.Embedding( num_embeddings , embedding_dim , padding_idx=None , max_norm=None , norm_type=2.0 , scale_grad_by_freq=False , sparse=False , _weight=None , _freeze=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  A simple lookup table that stores embeddings of a fixed dictionary and size.  This module is often used to store word embeddings and retrieve them using indices.\\nThe input to the module is a list of indices, and the output is the corresponding\\nword embeddings.  Parameters  num_embeddings ( int ) – size of the dictionary of embeddings  embedding_dim ( int ) – the size of each embedding vector  padding_idx ( int  ,  optional ) – If specified, the entries at padding_idx do not contribute to the gradient;\\ntherefore, the embedding vector at padding_idx is not updated during training,\\ni.e. it remains as a fixed “pad”. For a newly constructed Embedding,\\nthe embedding vector at padding_idx will default to all zeros,\\nbut can be updated to another value to be used as the padding vector.  max_norm ( float  ,  optional ) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm .  norm_type ( float  ,  optional ) – The p of the p-norm to compute for the max_norm option. Default 2 .  scale_grad_by_freq ( bool  ,  optional ) – If given, this will scale gradients by the inverse of frequency of\\nthe words in the mini-batch. Default False .  sparse ( bool  ,  optional ) – If True , gradient w.r.t. weight matrix will be a sparse tensor.\\nSee Notes for more details regarding sparse gradients.  Variables  weight ( Tensor ) – the learnable weights of the module of shape (num_embeddings, embedding_dim)\\ninitialized fromN  (  0  ,  1  )  \\\\mathcal{N}(0, 1)N(0,1)  Shape:  Input:(  ∗  )  (*)(∗), IntTensor or LongTensor of arbitrary shape containing the indices to extract  Output:(  ∗  ,  H  )  (*, H)(∗,H), where * is the input shape andH  =  embedding_dim  H=\\\\text{embedding\\\\_dim}H=embedding_dim  Note  Keep in mind that only a limited number of optimizers support\\nsparse gradients: currently it’s optim.SGD ( CUDA and CPU ), optim.SparseAdam ( CUDA and CPU ) and optim.Adagrad ( CPU )  Note  When max_norm is not None , Embedding ’s forward method will modify the weight tensor in-place. Since tensors needed for gradient computations cannot be\\nmodified in-place, performing a differentiable operation on Embedding.weight before\\ncalling Embedding ’s forward method requires cloning Embedding.weight when max_norm is not None . For example:  n,d,m=3,5,7embedding=nn.Embedding(n,d,max_norm=1.0)W=torch.randn((m,d),requires_grad=True)idx=torch.tensor([1,2])a=embedding.weight.clone()@W.t()# weight must be cloned for this to be differentiableb=embedding(idx)@W.t()# modifies weight in-placeout=(a.unsqueeze(0)+b.unsqueeze(1))loss=out.sigmoid().prod()loss.backward()  Examples:  >>># an Embedding module containing 10 tensors of size 3>>>embedding=nn.Embedding(10,3)>>># a batch of 2 samples of 4 indices each>>>input=torch.LongTensor([[1,2,4,5],[4,3,2,9]])>>>embedding(input)tensor([[[-0.0251, -1.6902,  0.7172],[-0.6431,  0.0748,  0.6969],[ 1.4970,  1.3448, -0.9685],[-0.3677, -2.7265, -0.1685]],[[ 1.4970,  1.3448, -0.9685],[ 0.4362, -0.4004,  0.9400],[-0.6431,  0.0748,  0.6969],[ 0.9124, -2.3616,  1.1151]]])>>># example with padding_idx>>>embedding=nn.Embedding(10,3,padding_idx=0)>>>input=torch.LongTensor([[0,2,0,5]])>>>embedding(input)tensor([[[ 0.0000,  0.0000,  0.0000],[ 0.1535, -2.0309,  0.9315],[ 0.0000,  0.0000,  0.0000],[-0.1655,  0.9897,  0.0635]]])>>># example of changing `pad` vector>>>padding_idx=0>>>embedding=nn.Embedding(3,3,padding_idx=padding_idx)>>>embedding.weightParameter containing:tensor([[ 0.0000,  0.0000,  0.0000],[-0.7895, -0.7089, -0.0364],[ 0.6778,  0.5803,  0.2678]], requires_grad=True)>>>withtorch.no_grad():...embedding.weight[padding_idx]=torch.ones(3)>>>embedding.weightParameter containing:tensor([[ 1.0000,  1.0000,  1.0000],[-0.7895, -0.7089, -0.0364],[ 0.6778,  0.5803,  0.2678]], requires_grad=True)  classmethod from_pretrained( embeddings , freeze=True , padding_idx=None , max_norm=None , norm_type=2.0 , scale_grad_by_freq=False , sparse=False ) [source]  [source]  [LINK_3]  Create Embedding instance from given 2-dimensional FloatTensor.  Parameters  embeddings ( Tensor ) – FloatTensor containing weights for the Embedding.\\nFirst dimension is being passed to Embedding as num_embeddings , second as embedding_dim .  freeze ( bool  ,  optional ) – If True , the tensor does not get updated in the learning process.\\nEquivalent to embedding.weight.requires_grad=False . Default: True  padding_idx ( int  ,  optional ) – If specified, the entries at padding_idx do not contribute to the gradient;\\ntherefore, the embedding vector at padding_idx is not updated during training,\\ni.e. it remains as a fixed “pad”.  max_norm ( float  ,  optional ) – See module initialization documentation.  norm_type ( float  ,  optional ) – See module initialization documentation. Default 2 .  scale_grad_by_freq ( bool  ,  optional ) – See module initialization documentation. Default False .  sparse ( bool  ,  optional ) – See module initialization documentation.  Examples:  >>># FloatTensor containing pretrained weights>>>weight=torch.FloatTensor([[1,2.3,3],[4,5.1,6.3]])>>>embedding=nn.Embedding.from_pretrained(weight)>>># Get embeddings for index 1>>>input=torch.LongTensor([1])>>>embedding(input)tensor([[ 4.0000,  5.1000,  6.3000]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#embedding\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#embedding', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/sparse.py#L15', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding.from_pretrained', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/sparse.py#L214', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html', 'https://pytorch.org/docs/stable/generated/torch.nn.FeatureAlphaDropout.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a09'), 'title': 'nn.EmbeddingBag', 'page_text': 'EmbeddingBag [LINK_1]  class torch.nn.EmbeddingBag( num_embeddings , embedding_dim , max_norm=None , norm_type=2.0 , scale_grad_by_freq=False , mode=\\'mean\\' , sparse=False , _weight=None , include_last_offset=False , padding_idx=None , device=None , dtype=None ) [source]  [source]  [LINK_2]  Compute sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings.  For bags of constant length, no per_sample_weights , no indices equal to padding_idx ,\\nand with 2D inputs, this class  with mode=\"sum\" is equivalent to Embedding followed by torch.sum(dim=1) ,  with mode=\"mean\" is equivalent to Embedding followed by torch.mean(dim=1) ,  with mode=\"max\" is equivalent to Embedding followed by torch.max(dim=1) .  However, EmbeddingBag is much more time and memory efficient than using a chain of these\\noperations.  EmbeddingBag also supports per-sample weights as an argument to the forward\\npass. This scales the output of the Embedding before performing a weighted\\nreduction as specified by mode . If per_sample_weights is passed, the\\nonly supported mode is \"sum\" , which computes a weighted sum according to per_sample_weights .  Parameters  num_embeddings ( int ) – size of the dictionary of embeddings  embedding_dim ( int ) – the size of each embedding vector  max_norm ( float  ,  optional ) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm .  norm_type ( float  ,  optional ) – The p of the p-norm to compute for the max_norm option. Default 2 .  scale_grad_by_freq ( bool  ,  optional ) – if given, this will scale gradients by the inverse of frequency of\\nthe words in the mini-batch. Default False .\\nNote: this option is not supported when mode=\"max\" .  mode ( str  ,  optional ) – \"sum\" , \"mean\" or \"max\" . Specifies the way to reduce the bag. \"sum\" computes the weighted sum, taking per_sample_weights into consideration. \"mean\" computes the average of the values\\nin the bag, \"max\" computes the max value over each bag.\\nDefault: \"mean\"  sparse ( bool  ,  optional ) – if True , gradient w.r.t. weight matrix will be a sparse tensor. See\\nNotes for more details regarding sparse gradients. Note: this option is not\\nsupported when mode=\"max\" .  include_last_offset ( bool  ,  optional ) – if True , offsets has one additional element, where the last element\\nis equivalent to the size of indices . This matches the CSR format.  padding_idx ( int  ,  optional ) – If specified, the entries at padding_idx do not contribute to the\\ngradient; therefore, the embedding vector at padding_idx is not updated\\nduring training, i.e. it remains as a fixed “pad”. For a newly constructed\\nEmbeddingBag, the embedding vector at padding_idx will default to all\\nzeros, but can be updated to another value to be used as the padding vector.\\nNote that the embedding vector at padding_idx is excluded from the\\nreduction.  Variables  weight ( Tensor ) – the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized fromN  (  0  ,  1  )  \\\\mathcal{N}(0, 1)N(0,1).  Examples:  >>># an EmbeddingBag module containing 10 tensors of size 3>>>embedding_sum=nn.EmbeddingBag(10,3,mode=\\'sum\\')>>># a batch of 2 samples of 4 indices each>>>input=torch.tensor([1,2,4,5,4,3,2,9],dtype=torch.long)>>>offsets=torch.tensor([0,4],dtype=torch.long)>>>embedding_sum(input,offsets)tensor([[-0.8861, -5.4350, -0.0523],[ 1.1306, -2.5798, -1.0044]])>>># Example with padding_idx>>>embedding_sum=nn.EmbeddingBag(10,3,mode=\\'sum\\',padding_idx=2)>>>input=torch.tensor([2,2,2,2,4,3,2,9],dtype=torch.long)>>>offsets=torch.tensor([0,4],dtype=torch.long)>>>embedding_sum(input,offsets)tensor([[ 0.0000,  0.0000,  0.0000],[-0.7082,  3.2145, -2.6251]])>>># An EmbeddingBag can be loaded from an Embedding like so>>>embedding=nn.Embedding(10,3,padding_idx=2)>>>embedding_sum=nn.EmbeddingBag.from_pretrained(embedding.weight,padding_idx=embedding.padding_idx,mode=\\'sum\\')  forward( input , offsets=None , per_sample_weights=None ) [source]  [source]  [LINK_3]  Forward pass of EmbeddingBag.  Parameters  input ( Tensor ) – Tensor containing bags of indices into the embedding matrix.  offsets ( Tensor  ,  optional ) – Only used when input is 1D. offsets determines\\nthe starting index position of each bag (sequence) in input .  per_sample_weights ( Tensor  ,  optional ) – a tensor of float / double weights, or None\\nto indicate all weights should be taken to be 1 . If specified, per_sample_weights must have exactly the same shape as input and is treated as having the same offsets , if those are not None . Only supported for mode=\\'sum\\' .  Returns  Tensor output shape of (B, embedding_dim) .  Return type  Tensor  Note  A few notes about input and offsets :  input and offsets have to be of the same type, either int or long  If input is 2D of shape (B, N) , it will be treated as B bags (sequences)\\neach of fixed length N , and this will return B values aggregated in a way\\ndepending on the mode . offsets is ignored and required to be None in this case.  If input is 1D of shape (N) , it will be treated as a concatenation of\\nmultiple bags (sequences). offsets is required to be a 1D tensor containing the\\nstarting index positions of each bag in input . Therefore, for offsets of shape (B) , input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have\\nreturned vectors filled by zeros.  classmethod from_pretrained( embeddings , freeze=True , max_norm=None , norm_type=2.0 , scale_grad_by_freq=False , mode=\\'mean\\' , sparse=False , include_last_offset=False , padding_idx=None ) [source]  [source]  [LINK_4]  Create EmbeddingBag instance from given 2-dimensional FloatTensor.  Parameters  embeddings ( Tensor ) – FloatTensor containing weights for the EmbeddingBag.\\nFirst dimension is being passed to EmbeddingBag as ‘num_embeddings’, second as ‘embedding_dim’.  freeze ( bool  ,  optional ) – If True , the tensor does not get updated in the learning process.\\nEquivalent to embeddingbag.weight.requires_grad=False . Default: True  max_norm ( float  ,  optional ) – See module initialization documentation. Default: None  norm_type ( float  ,  optional ) – See module initialization documentation. Default 2 .  scale_grad_by_freq ( bool  ,  optional ) – See module initialization documentation. Default False .  mode ( str  ,  optional ) – See module initialization documentation. Default: \"mean\"  sparse ( bool  ,  optional ) – See module initialization documentation. Default: False .  include_last_offset ( bool  ,  optional ) – See module initialization documentation. Default: False .  padding_idx ( int  ,  optional ) – See module initialization documentation. Default: None .  Return type  EmbeddingBag  Examples:  >>># FloatTensor containing pretrained weights>>>weight=torch.FloatTensor([[1,2.3,3],[4,5.1,6.3]])>>>embeddingbag=nn.EmbeddingBag.from_pretrained(weight)>>># Get embeddings for index 1>>>input=torch.LongTensor([[1,0]])>>>embeddingbag(input)tensor([[ 2.5000,  3.7000,  4.6500]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#embeddingbag\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag.forward\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag.from_pretrained', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#embeddingbag', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#EmbeddingBag', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/sparse.py#L269', 'https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding', 'https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#EmbeddingBag.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/sparse.py#L427', 'https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag.forward', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#EmbeddingBag.from_pretrained', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/sparse.py#L490', 'https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag.from_pretrained', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag', 'https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a0a'), 'title': 'nn.CosineSimilarity', 'page_text': 'CosineSimilarity [LINK_1]  class torch.nn.CosineSimilarity( dim=1 , eps=1e-08 ) [source]  [source]  [LINK_2]  Returns cosine similarity betweenx  1  x_1x1\\u200bandx  2  x_2x2\\u200b, computed along dim .  similarity  =  x  1  ⋅  x  2  max  \\u2061  (  ∥  x  1  ∥  2  ⋅  ∥  x  2  ∥  2  ,  ϵ  )  .  \\\\text{similarity} = \\\\dfrac{x_1 \\\\cdot x_2}{\\\\max(\\\\Vert x_1 \\\\Vert _2 \\\\cdot \\\\Vert x_2 \\\\Vert _2, \\\\epsilon)}.similarity=max(∥x1\\u200b∥2\\u200b⋅∥x2\\u200b∥2\\u200b,ϵ)x1\\u200b⋅x2\\u200b\\u200b.  Parameters  dim ( int  ,  optional ) – Dimension where cosine similarity is computed. Default: 1  eps ( float  ,  optional ) – Small value to avoid division by zero.\\nDefault: 1e-8  Shape:  Input1:(  ∗  1  ,  D  ,  ∗  2  )  (\\\\ast_1, D, \\\\ast_2)(∗1\\u200b,D,∗2\\u200b)where D is at position dim  Input2:(  ∗  1  ,  D  ,  ∗  2  )  (\\\\ast_1, D, \\\\ast_2)(∗1\\u200b,D,∗2\\u200b), same number of dimensions as x1, matching x1 size at dimension dim ,  and broadcastable with x1 at other dimensions.  Output:(  ∗  1  ,  ∗  2  )  (\\\\ast_1, \\\\ast_2)(∗1\\u200b,∗2\\u200b)  Examples::  >>>input1=torch.randn(100,128)>>>input2=torch.randn(100,128)>>>cos=nn.CosineSimilarity(dim=1,eps=1e-6)>>>output=cos(input1,input2)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html#cosinesimilarity\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html#cosinesimilarity', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/distance.html#CosineSimilarity', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/distance.py#L61', 'https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html', 'https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a0b'), 'title': 'nn.PairwiseDistance', 'page_text': 'PairwiseDistance [LINK_1]  class torch.nn.PairwiseDistance( p=2.0 , eps=1e-06 , keepdim=False ) [source]  [source]  [LINK_2]  Computes the pairwise distance between input vectors, or between columns of input matrices.  Distances are computed using p -norm, with constant eps added to avoid division by zero\\nif p is negative, i.e.:  d  i  s  t  (  x  ,  y  )  =  ∥  x  −  y  +  ϵ  e  ∥  p  ,  \\\\mathrm{dist}\\\\left(x, y\\\\right) = \\\\left\\\\Vert x-y + \\\\epsilon e \\\\right\\\\Vert_p,dist(x,y)=∥x−y+ϵe∥p\\u200b,  wheree  eeis the vector of ones and the p -norm is given by.  ∥  x  ∥  p  =  (  ∑  i  =  1  n  ∣  x  i  ∣  p  )  1  /  p  .  \\\\Vert x \\\\Vert _p = \\\\left( \\\\sum_{i=1}^n  \\\\vert x_i \\\\vert ^ p \\\\right) ^ {1/p}.∥x∥p\\u200b=(i=1∑n\\u200b∣xi\\u200b∣p)1/p.  Parameters  p ( real  ,  optional ) – the norm degree. Can be negative. Default: 2  eps ( float  ,  optional ) – Small value to avoid division by zero.\\nDefault: 1e-6  keepdim ( bool  ,  optional ) – Determines whether or not to keep the vector dimension.\\nDefault: False  Shape:  Input1:(  N  ,  D  )  (N, D)(N,D)or(  D  )  (D)(D)where N = batch dimension and D = vector dimension  Input2:(  N  ,  D  )  (N, D)(N,D)or(  D  )  (D)(D), same shape as the Input1  Output:(  N  )  (N)(N)or(  )  ()()based on input dimension.\\nIf keepdim is True , then(  N  ,  1  )  (N, 1)(N,1)or(  1  )  (1)(1)based on input dimension.  Examples::  >>>pdist=nn.PairwiseDistance(p=2)>>>input1=torch.randn(100,128)>>>input2=torch.randn(100,128)>>>output=pdist(input1,input2)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html#pairwisedistance\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html#pairwisedistance', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/distance.html#PairwiseDistance', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/distance.py#L10', 'https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a0c'), 'title': 'nn.L1Loss', 'page_text': 'L1Loss [LINK_1]  class torch.nn.L1Loss( size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that measures the mean absolute error (MAE) between each element in\\nthe inputx  xxand targety  yy.  The unreduced (i.e. with reduction set to \\'none\\' ) loss can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  …  ,  l  N  }  ⊤  ,  l  n  =  ∣  x  n  −  y  n  ∣  ,  \\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = \\\\left| x_n - y_n \\\\right|,ℓ(x,y)=L={l1\\u200b,…,lN\\u200b}⊤,ln\\u200b=∣xn\\u200b−yn\\u200b∣,  whereN  NNis the batch size. If reduction is not \\'none\\' (default \\'mean\\' ), then:  ℓ  (  x  ,  y  )  =  {  mean  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘mean’;  sum  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) =\\n\\\\begin{cases}\\n    \\\\operatorname{mean}(L), & \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  & \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={mean(L),sum(L),\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  x  xxandy  yyare tensors of arbitrary shapes with a total\\nofN  NNelements each.  The sum operation still operates over all the elements, and divides byN  NN.  The division byN  NNcan be avoided if one sets reduction=\\'sum\\' .  Supports real-valued and complex-valued inputs.  Parameters  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Output: scalar. If reduction is \\'none\\' , then(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>loss=nn.L1Loss()>>>input=torch.randn(3,5,requires_grad=True)>>>target=torch.randn(3,5)>>>output=loss(input,target)>>>output.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#l1loss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#l1loss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#L1Loss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L62', 'https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a0d'), 'title': 'nn.MSELoss', 'page_text': 'MSELoss [LINK_1]  class torch.nn.MSELoss( size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that measures the mean squared error (squared L2 norm) between\\neach element in the inputx  xxand targety  yy.  The unreduced (i.e. with reduction set to \\'none\\' ) loss can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  …  ,  l  N  }  ⊤  ,  l  n  =  (  x  n  −  y  n  )  2  ,  \\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = \\\\left( x_n - y_n \\\\right)^2,ℓ(x,y)=L={l1\\u200b,…,lN\\u200b}⊤,ln\\u200b=(xn\\u200b−yn\\u200b)2,  whereN  NNis the batch size. If reduction is not \\'none\\' (default \\'mean\\' ), then:  ℓ  (  x  ,  y  )  =  {  mean  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘mean’;  sum  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) =\\n\\\\begin{cases}\\n    \\\\operatorname{mean}(L), &  \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  &  \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={mean(L),sum(L),\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  x  xxandy  yyare tensors of arbitrary shapes with a total\\nofN  NNelements each.  The mean operation still operates over all the elements, and divides byN  NN.  The division byN  NNcan be avoided if one sets reduction=\\'sum\\' .  Parameters  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>loss=nn.MSELoss()>>>input=torch.randn(3,5,requires_grad=True)>>>target=torch.randn(3,5)>>>output=loss(input,target)>>>output.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#mseloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#mseloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MSELoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L548', 'https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a0e'), 'title': 'nn.CrossEntropyLoss', 'page_text': 'CrossEntropyLoss [LINK_1]  class torch.nn.CrossEntropyLoss( weight=None , size_average=None , ignore_index=-100 , reduce=None , reduction=\\'mean\\' , label_smoothing=0.0 ) [source]  [source]  [LINK_2]  This criterion computes the cross entropy loss between input logits\\nand target.  It is useful when training a classification problem with C classes.\\nIf provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes.\\nThis is particularly useful when you have an unbalanced training set.  The input is expected to contain the unnormalized logits for each class (which do not need\\nto be positive or sum to 1, in general). input has to be a Tensor of size(  C  )  (C)(C)for unbatched input,(  m  i  n  i  b  a  t  c  h  ,  C  )  (minibatch, C)(minibatch,C)or(  m  i  n  i  b  a  t  c  h  ,  C  ,  d  1  ,  d  2  ,  .  .  .  ,  d  K  )  (minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\\u200b,d2\\u200b,...,dK\\u200b)withK  ≥  1  K \\\\geq 1K≥1for the K -dimensional case. The last being useful for higher dimension inputs, such\\nas computing cross entropy loss per-pixel for 2D images.  The target that this criterion expects should contain either:  Class indices in the range[  0  ,  C  )  [0, C)[0,C)whereC  CCis the number of classes; if ignore_index is specified, this loss also accepts this class index (this index\\nmay not necessarily be in the class range). The unreduced (i.e. with reduction set to \\'none\\' ) loss for this case can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  …  ,  l  N  }  ⊤  ,  l  n  =  −  w  y  n  log  \\u2061  exp  \\u2061  (  x  n  ,  y  n  )  ∑  c  =  1  C  exp  \\u2061  (  x  n  ,  c  )  ⋅  1  {  y  n  ≠  ignore_index  }  \\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - w_{y_n} \\\\log \\\\frac{\\\\exp(x_{n,y_n})}{\\\\sum_{c=1}^C \\\\exp(x_{n,c})}\\n\\\\cdot \\\\mathbb{1}\\\\{y_n \\\\not= \\\\text{ignore\\\\_index}\\\\}ℓ(x,y)=L={l1\\u200b,…,lN\\u200b}⊤,ln\\u200b=−wyn\\u200b\\u200blog∑c=1C\\u200bexp(xn,c\\u200b)exp(xn,yn\\u200b\\u200b)\\u200b⋅1{yn\\u200b\\ue020=ignore_index}  wherex  xxis the input,y  yyis the target,w  wwis the weight,C  CCis the number of classes, andN  NNspans the minibatch dimension as well asd  1  ,  .  .  .  ,  d  k  d_1, ..., d_kd1\\u200b,...,dk\\u200bfor the K -dimensional case. If reduction is not \\'none\\' (default \\'mean\\' ), then  ℓ  (  x  ,  y  )  =  {  ∑  n  =  1  N  1  ∑  n  =  1  N  w  y  n  ⋅  1  {  y  n  ≠  ignore_index  }  l  n  ,  if\\xa0reduction  =  ‘mean’;  ∑  n  =  1  N  l  n  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) = \\\\begin{cases}\\n    \\\\sum_{n=1}^N \\\\frac{1}{\\\\sum_{n=1}^N w_{y_n} \\\\cdot \\\\mathbb{1}\\\\{y_n \\\\not= \\\\text{ignore\\\\_index}\\\\}} l_n, &\\n     \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n      \\\\sum_{n=1}^N l_n,  &\\n      \\\\text{if reduction} = \\\\text{`sum\\'.}\\n  \\\\end{cases}ℓ(x,y)={∑n=1N\\u200b∑n=1N\\u200bwyn\\u200b\\u200b⋅1{yn\\u200b\\ue020=ignore_index}1\\u200bln\\u200b,∑n=1N\\u200bln\\u200b,\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  Note that this case is equivalent to applying LogSoftmax on an input, followed by NLLLoss .  Probabilities for each class; useful when labels beyond a single class per minibatch item\\nare required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with reduction set to \\'none\\' ) loss for this case can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  …  ,  l  N  }  ⊤  ,  l  n  =  −  ∑  c  =  1  C  w  c  log  \\u2061  exp  \\u2061  (  x  n  ,  c  )  ∑  i  =  1  C  exp  \\u2061  (  x  n  ,  i  )  y  n  ,  c  \\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - \\\\sum_{c=1}^C w_c \\\\log \\\\frac{\\\\exp(x_{n,c})}{\\\\sum_{i=1}^C \\\\exp(x_{n,i})} y_{n,c}ℓ(x,y)=L={l1\\u200b,…,lN\\u200b}⊤,ln\\u200b=−c=1∑C\\u200bwc\\u200blog∑i=1C\\u200bexp(xn,i\\u200b)exp(xn,c\\u200b)\\u200byn,c\\u200b  wherex  xxis the input,y  yyis the target,w  wwis the weight,C  CCis the number of classes, andN  NNspans the minibatch dimension as well asd  1  ,  .  .  .  ,  d  k  d_1, ..., d_kd1\\u200b,...,dk\\u200bfor the K -dimensional case. If reduction is not \\'none\\' (default \\'mean\\' ), then  ℓ  (  x  ,  y  )  =  {  ∑  n  =  1  N  l  n  N  ,  if\\xa0reduction  =  ‘mean’;  ∑  n  =  1  N  l  n  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) = \\\\begin{cases}\\n    \\\\frac{\\\\sum_{n=1}^N l_n}{N}, &\\n     \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n      \\\\sum_{n=1}^N l_n,  &\\n      \\\\text{if reduction} = \\\\text{`sum\\'.}\\n  \\\\end{cases}ℓ(x,y)={N∑n=1N\\u200bln\\u200b\\u200b,∑n=1N\\u200bln\\u200b,\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  Note  The performance of this criterion is generally better when target contains class\\nindices, as this allows for optimized computation. Consider providing target as\\nclass probabilities only when a single class label per minibatch item is too restrictive.  Parameters  weight ( Tensor  ,  optional ) – a manual rescaling weight given to each class.\\nIf given, has to be a Tensor of size C and floating point dtype  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  ignore_index ( int  ,  optional ) – Specifies a target value that is ignored\\nand does not contribute to the input gradient. When size_average is True , the loss is averaged over non-ignored targets. Note that ignore_index is only applicable when the target contains class indices.  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will\\nbe applied, \\'mean\\' : the weighted mean of the output is taken, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in\\nthe meantime, specifying either of those two args will override reduction . Default: \\'mean\\'  label_smoothing ( float  ,  optional ) – A float in [0.0, 1.0]. Specifies the amount\\nof smoothing when computing the loss, where 0.0 means no smoothing. The targets\\nbecome a mixture of the original ground truth and a uniform distribution as described in Rethinking the Inception Architecture for Computer Vision . Default:0.0  0.00.0.  Shape:  Input: Shape(  C  )  (C)(C),(  N  ,  C  )  (N, C)(N,C)or(  N  ,  C  ,  d  1  ,  d  2  ,  .  .  .  ,  d  K  )  (N, C, d_1, d_2, ..., d_K)(N,C,d1\\u200b,d2\\u200b,...,dK\\u200b)withK  ≥  1  K \\\\geq 1K≥1in the case of K -dimensional loss.  Target: If containing class indices, shape(  )  ()(),(  N  )  (N)(N)or(  N  ,  d  1  ,  d  2  ,  .  .  .  ,  d  K  )  (N, d_1, d_2, ..., d_K)(N,d1\\u200b,d2\\u200b,...,dK\\u200b)withK  ≥  1  K \\\\geq 1K≥1in the case of K-dimensional loss where each value should be between[  0  ,  C  )  [0, C)[0,C).\\nIf containing class probabilities, same shape as the input and each value should be between[  0  ,  1  ]  [0, 1][0,1].  Output: If reduction is ‘none’, shape(  )  ()(),(  N  )  (N)(N)or(  N  ,  d  1  ,  d  2  ,  .  .  .  ,  d  K  )  (N, d_1, d_2, ..., d_K)(N,d1\\u200b,d2\\u200b,...,dK\\u200b)withK  ≥  1  K \\\\geq 1K≥1in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.  where:  C  =  number\\xa0of\\xa0classes  N  =  batch\\xa0size  \\\\begin{aligned}\\n    C ={} & \\\\text{number of classes} \\\\\\\\\\n    N ={} & \\\\text{batch size} \\\\\\\\\\n\\\\end{aligned}C=N=\\u200bnumber\\xa0of\\xa0classesbatch\\xa0size\\u200b  Examples:  >>># Example of target with class indices>>>loss=nn.CrossEntropyLoss()>>>input=torch.randn(3,5,requires_grad=True)>>>target=torch.empty(3,dtype=torch.long).random_(5)>>>output=loss(input,target)>>>output.backward()>>>>>># Example of target with class probabilities>>>input=torch.randn(3,5,requires_grad=True)>>>target=torch.randn(3,5).softmax(dim=1)>>>output=loss(input,target)>>>output.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1146', 'https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss', 'https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax', 'https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#float', 'https://arxiv.org/abs/1512.00567', 'https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a0f'), 'title': 'nn.CTCLoss', 'page_text': 'CTCLoss [LINK_1]  class torch.nn.CTCLoss( blank=0 , reduction=\\'mean\\' , zero_infinity=False ) [source]  [source]  [LINK_2]  The Connectionist Temporal Classification loss.  Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the\\nprobability of possible alignments of input to target, producing a loss value which is differentiable\\nwith respect to each input node. The alignment of input to target is assumed to be “many-to-one”, which\\nlimits the length of the target sequence such that it must be≤  \\\\leq≤the input length.  Parameters  blank ( int  ,  optional ) – blank label. Default0  00.  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the output losses will be divided by the target lengths and\\nthen the mean over the batch is taken, \\'sum\\' : the output losses will be summed.\\nDefault: \\'mean\\'  zero_infinity ( bool  ,  optional ) – Whether to zero infinite losses and the associated gradients.\\nDefault: False Infinite losses mainly occur when the inputs are too short\\nto be aligned to the targets.  Shape:  Log_probs: Tensor of size(  T  ,  N  ,  C  )  (T, N, C)(T,N,C)or(  T  ,  C  )  (T, C)(T,C),\\nwhereT  =  input\\xa0length  T = \\\\text{input length}T=input\\xa0length,N  =  batch\\xa0size  N = \\\\text{batch size}N=batch\\xa0size, andC  =  number\\xa0of\\xa0classes\\xa0(including\\xa0blank)  C = \\\\text{number of classes (including blank)}C=number\\xa0of\\xa0classes\\xa0(including\\xa0blank).\\nThe logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax() ).  Targets: Tensor of size(  N  ,  S  )  (N, S)(N,S)or(  sum  \\u2061  (  target_lengths  )  )  (\\\\operatorname{sum}(\\\\text{target\\\\_lengths}))(sum(target_lengths)),\\nwhereN  =  batch\\xa0size  N = \\\\text{batch size}N=batch\\xa0sizeandS  =  max\\xa0target\\xa0length,\\xa0if\\xa0shape\\xa0is  (  N  ,  S  )  S = \\\\text{max target length, if shape is } (N, S)S=max\\xa0target\\xa0length,\\xa0if\\xa0shape\\xa0is(N,S).\\nIt represents the target sequences. Each element in the target\\nsequence is a class index. And the target index cannot be blank (default=0).\\nIn the(  N  ,  S  )  (N, S)(N,S)form, targets are padded to the\\nlength of the longest sequence, and stacked.\\nIn the(  sum  \\u2061  (  target_lengths  )  )  (\\\\operatorname{sum}(\\\\text{target\\\\_lengths}))(sum(target_lengths))form,\\nthe targets are assumed to be un-padded and\\nconcatenated within 1 dimension.  Input_lengths: Tuple or tensor of size(  N  )  (N)(N)or(  )  ()(),\\nwhereN  =  batch\\xa0size  N = \\\\text{batch size}N=batch\\xa0size. It represents the lengths of the\\ninputs (must each be≤  T  \\\\leq T≤T). And the lengths are specified\\nfor each sequence to achieve masking under the assumption that sequences\\nare padded to equal lengths.  Target_lengths: Tuple or tensor of size(  N  )  (N)(N)or(  )  ()(),\\nwhereN  =  batch\\xa0size  N = \\\\text{batch size}N=batch\\xa0size. It represents lengths of the targets.\\nLengths are specified for each sequence to achieve masking under the\\nassumption that sequences are padded to equal lengths. If target shape is(  N  ,  S  )  (N,S)(N,S), target_lengths are effectively the stop indexs  n  s_nsn\\u200bfor each target sequence, such that target_n=targets[n,0:s_n] for\\neach target in a batch. Lengths must each be≤  S  \\\\leq S≤SIf the targets are given as a 1d tensor that is the concatenation of individual\\ntargets, the target_lengths must add up to the total length of the tensor.  Output: scalar if reduction is \\'mean\\' (default) or \\'sum\\' . If reduction is \\'none\\' , then(  N  )  (N)(N)if input is batched or(  )  ()()if input is unbatched, whereN  =  batch\\xa0size  N = \\\\text{batch size}N=batch\\xa0size.  Examples:  >>># Target are to be padded>>>T=50# Input sequence length>>>C=20# Number of classes (including blank)>>>N=16# Batch size>>>S=30# Target sequence length of longest target in batch (padding length)>>>S_min=10# Minimum target length, for demonstration purposes>>>>>># Initialize random batch of input vectors, for *size = (T,N,C)>>>input=torch.randn(T,N,C).log_softmax(2).detach().requires_grad_()>>>>>># Initialize random batch of targets (0 = blank, 1:C = classes)>>>target=torch.randint(low=1,high=C,size=(N,S),dtype=torch.long)>>>>>>input_lengths=torch.full(size=(N,),fill_value=T,dtype=torch.long)>>>target_lengths=torch.randint(low=S_min,high=S,size=(N,),dtype=torch.long)>>>ctc_loss=nn.CTCLoss()>>>loss=ctc_loss(input,target,input_lengths,target_lengths)>>>loss.backward()>>>>>>>>># Target are to be un-padded>>>T=50# Input sequence length>>>C=20# Number of classes (including blank)>>>N=16# Batch size>>>>>># Initialize random batch of input vectors, for *size = (T,N,C)>>>input=torch.randn(T,N,C).log_softmax(2).detach().requires_grad_()>>>input_lengths=torch.full(size=(N,),fill_value=T,dtype=torch.long)>>>>>># Initialize random batch of targets (0 = blank, 1:C = classes)>>>target_lengths=torch.randint(low=1,high=T,size=(N,),dtype=torch.long)>>>target=torch.randint(low=1,high=C,size=(sum(target_lengths),),dtype=torch.long)>>>ctc_loss=nn.CTCLoss()>>>loss=ctc_loss(input,target,input_lengths,target_lengths)>>>loss.backward()>>>>>>>>># Target are to be un-padded and unbatched (effectively N=1)>>>T=50# Input sequence length>>>C=20# Number of classes (including blank)>>>>>># Initialize random batch of input vectors, for *size = (T,C)>>>input=torch.randn(T,C).log_softmax(1).detach().requires_grad_()>>>input_lengths=torch.tensor(T,dtype=torch.long)>>>>>># Initialize random batch of targets (0 = blank, 1:C = classes)>>>target_lengths=torch.randint(low=1,high=T,size=(),dtype=torch.long)>>>target=torch.randint(low=1,high=C,size=(target_lengths,),dtype=torch.long)>>>ctc_loss=nn.CTCLoss()>>>loss=ctc_loss(input,target,input_lengths,target_lengths)>>>loss.backward()  Reference:  A. Graves et al.: Connectionist Temporal Classification:\\nLabelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf  Note  In order to use CuDNN, the following must be satisfied: targets must be\\nin concatenated format, all input_lengths must be T .b  l  a  n  k  =  0  blank=0blank=0, target_lengths ≤  256  \\\\leq 256≤256, the integer arguments must be of\\ndtype torch.int32 .  The regular implementation uses the (more common in PyTorch) torch.long dtype.  Note  In some circumstances when using the CUDA backend with CuDNN, this operator\\nmay select a nondeterministic algorithm to increase performance. If this is\\nundesirable, you can try to make the operation deterministic (potentially at\\na performance cost) by setting torch.backends.cudnn.deterministic=True .\\nPlease see the notes on Reproducibility for background.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#ctcloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#ctcloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CTCLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1830', 'https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax', 'https://www.cs.toronto.edu/~graves/icml_2006.pdf', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a10'), 'title': 'nn.NLLLoss', 'page_text': 'NLLLoss [LINK_1]  class torch.nn.NLLLoss( weight=None , size_average=None , ignore_index=-100 , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  The negative log likelihood loss. It is useful to train a classification\\nproblem with C classes.  If provided, the optional argument weight should be a 1D Tensor assigning\\nweight to each of the classes. This is particularly useful when you have an\\nunbalanced training set.  The input given through a forward call is expected to contain\\nlog-probabilities of each class. input has to be a Tensor of size either(  m  i  n  i  b  a  t  c  h  ,  C  )  (minibatch, C)(minibatch,C)or(  m  i  n  i  b  a  t  c  h  ,  C  ,  d  1  ,  d  2  ,  .  .  .  ,  d  K  )  (minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\\u200b,d2\\u200b,...,dK\\u200b)withK  ≥  1  K \\\\geq 1K≥1for the K -dimensional case. The latter is useful for\\nhigher dimension inputs, such as computing NLL loss per-pixel for 2D images.  Obtaining log-probabilities in a neural network is easily achieved by\\nadding a LogSoftmax layer in the last layer of your network.\\nYou may use CrossEntropyLoss instead, if you prefer not to add an extra\\nlayer.  The target that this loss expects should be a class index in the range[  0  ,  C  −  1  ]  [0, C-1][0,C−1]where C = number of classes ; if ignore_index is specified, this loss also accepts\\nthis class index (this index may not necessarily be in the class range).  The unreduced (i.e. with reduction set to \\'none\\' ) loss can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  …  ,  l  N  }  ⊤  ,  l  n  =  −  w  y  n  x  n  ,  y  n  ,  w  c  =  weight  [  c  ]  ⋅  1  {  c  ≠  ignore_index  }  ,  \\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - w_{y_n} x_{n,y_n}, \\\\quad\\nw_{c} = \\\\text{weight}[c] \\\\cdot \\\\mathbb{1}\\\\{c \\\\not= \\\\text{ignore\\\\_index}\\\\},ℓ(x,y)=L={l1\\u200b,…,lN\\u200b}⊤,ln\\u200b=−wyn\\u200b\\u200bxn,yn\\u200b\\u200b,wc\\u200b=weight[c]⋅1{c\\ue020=ignore_index},  wherex  xxis the input,y  yyis the target,w  wwis the weight, andN  NNis the batch size. If reduction is not \\'none\\' (default \\'mean\\' ), then  ℓ  (  x  ,  y  )  =  {  ∑  n  =  1  N  1  ∑  n  =  1  N  w  y  n  l  n  ,  if\\xa0reduction  =  ‘mean’;  ∑  n  =  1  N  l  n  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) = \\\\begin{cases}\\n    \\\\sum_{n=1}^N \\\\frac{1}{\\\\sum_{n=1}^N w_{y_n}} l_n, &\\n    \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\sum_{n=1}^N l_n,  &\\n    \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={∑n=1N\\u200b∑n=1N\\u200bwyn\\u200b\\u200b1\\u200bln\\u200b,∑n=1N\\u200bln\\u200b,\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  Parameters  weight ( Tensor  ,  optional ) – a manual rescaling weight given to each\\nclass. If given, it has to be a Tensor of size C . Otherwise, it is\\ntreated as if having all ones.  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: None  ignore_index ( int  ,  optional ) – Specifies a target value that is ignored\\nand does not contribute to the input gradient. When size_average is True , the loss is averaged over\\nnon-ignored targets.  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: None  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will\\nbe applied, \\'mean\\' : the weighted mean of the output is taken, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in\\nthe meantime, specifying either of those two args will override reduction . Default: \\'mean\\'  Shape::  Input:(  N  ,  C  )  (N, C)(N,C)or(  C  )  (C)(C), where C = number of classes , N = batch size , or(  N  ,  C  ,  d  1  ,  d  2  ,  .  .  .  ,  d  K  )  (N, C, d_1, d_2, ..., d_K)(N,C,d1\\u200b,d2\\u200b,...,dK\\u200b)withK  ≥  1  K \\\\geq 1K≥1in the case of K -dimensional loss.  Target:(  N  )  (N)(N)or(  )  ()(), where each value is0  ≤  targets  [  i  ]  ≤  C  −  1  0 \\\\leq \\\\text{targets}[i] \\\\leq C-10≤targets[i]≤C−1, or(  N  ,  d  1  ,  d  2  ,  .  .  .  ,  d  K  )  (N, d_1, d_2, ..., d_K)(N,d1\\u200b,d2\\u200b,...,dK\\u200b)withK  ≥  1  K \\\\geq 1K≥1in the case of\\nK-dimensional loss.  Output: If reduction is \\'none\\' , shape(  N  )  (N)(N)or(  N  ,  d  1  ,  d  2  ,  .  .  .  ,  d  K  )  (N, d_1, d_2, ..., d_K)(N,d1\\u200b,d2\\u200b,...,dK\\u200b)withK  ≥  1  K \\\\geq 1K≥1in the case of K-dimensional loss.\\nOtherwise, scalar.  Examples:  >>>log_softmax=nn.LogSoftmax(dim=1)>>>loss_fn=nn.NLLLoss()>>># input to NLLLoss is of size N x C = 3 x 5>>>input=torch.randn(3,5,requires_grad=True)>>># each element in target must have 0 <= value < C>>>target=torch.tensor([1,0,4])>>>loss=loss_fn(log_softmax(input),target)>>>loss.backward()>>>>>>>>># 2D loss example (used, for example, with image inputs)>>>N,C=5,4>>>loss_fn=nn.NLLLoss()>>>data=torch.randn(N,16,10,10)>>>conv=nn.Conv2d(16,C,(3,3))>>>log_softmax=nn.LogSoftmax(dim=1)>>># output of conv forward is of shape [N, C, 8, 8]>>>output=log_softmax(conv(data))>>># each element in target must have 0 <= value < C>>>target=torch.empty(N,8,8,dtype=torch.long).random_(0,C)>>># input to NLLLoss is of size N x C x height (8) x width (8)>>>loss=loss_fn(output,target)>>>loss.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#nllloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#nllloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#NLLLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L131', 'https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a11'), 'title': 'nn.PoissonNLLLoss', 'page_text': 'PoissonNLLLoss [LINK_1]  class torch.nn.PoissonNLLLoss( log_input=True , full=False , size_average=None , eps=1e-08 , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Negative log likelihood loss with Poisson distribution of target.  The loss can be described as:  target  ∼  P  o  i  s  s  o  n  (  input  )  loss  (  input  ,  target  )  =  input  −  target  ∗  log  \\u2061  (  input  )  +  log  \\u2061  (  target!  )  \\\\text{target} \\\\sim \\\\mathrm{Poisson}(\\\\text{input})\\n\\n\\\\text{loss}(\\\\text{input}, \\\\text{target}) = \\\\text{input} - \\\\text{target} * \\\\log(\\\\text{input})\\n                            + \\\\log(\\\\text{target!})target∼Poisson(input)loss(input,target)=input−target∗log(input)+log(target!)  The last term can be omitted or approximated with Stirling formula. The\\napproximation is used for target values more than 1. For targets less or\\nequal to 1 zeros are added to the loss.  Parameters  log_input ( bool  ,  optional ) – if True the loss is computed asexp  \\u2061  (  input  )  −  target  ∗  input  \\\\exp(\\\\text{input}) - \\\\text{target}*\\\\text{input}exp(input)−target∗input, if False the loss isinput  −  target  ∗  log  \\u2061  (  input  +  eps  )  \\\\text{input} - \\\\text{target}*\\\\log(\\\\text{input}+\\\\text{eps})input−target∗log(input+eps).  full ( bool  ,  optional ) – whether to compute full loss, i. e. to add the\\nStirling approximation term  target  ∗  log  \\u2061  (  target  )  −  target  +  0.5  ∗  log  \\u2061  (  2  π  target  )  .  \\\\text{target}*\\\\log(\\\\text{target}) - \\\\text{target} + 0.5 * \\\\log(2\\\\pi\\\\text{target}).target∗log(target)−target+0.5∗log(2πtarget).  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  eps ( float  ,  optional ) – Small value to avoid evaluation oflog  \\u2061  (  0  )  \\\\log(0)log(0)when log_input=False . Default: 1e-8  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Examples:  >>>loss=nn.PoissonNLLLoss()>>>log_input=torch.randn(5,2,requires_grad=True)>>>target=torch.randn(5,2)>>>output=loss(log_input,target)>>>output.backward()  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Output: scalar by default. If reduction is \\'none\\' , then(  ∗  )  (*)(∗),\\nthe same shape as the input.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html#poissonnllloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html#poissonnllloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#PoissonNLLLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L278', 'https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a12'), 'title': 'nn.GaussianNLLLoss', 'page_text': 'GaussianNLLLoss [LINK_1]  class torch.nn.GaussianNLLLoss( * , full=False , eps=1e-06 , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Gaussian negative log likelihood loss.  The targets are treated as samples from Gaussian distributions with\\nexpectations and variances predicted by the neural network. For a target tensor modelled as having Gaussian distribution with a tensor\\nof expectations input and a tensor of positive variances var the loss is:  loss  =  1  2  (  log  \\u2061  (  max  (  var  ,  eps  )  )  +  (  input  −  target  )  2  max  (  var  ,  eps  )  )  +  const.  \\\\text{loss} = \\\\frac{1}{2}\\\\left(\\\\log\\\\left(\\\\text{max}\\\\left(\\\\text{var},\\n\\\\ \\\\text{eps}\\\\right)\\\\right) + \\\\frac{\\\\left(\\\\text{input} - \\\\text{target}\\\\right)^2}\\n{\\\\text{max}\\\\left(\\\\text{var}, \\\\ \\\\text{eps}\\\\right)}\\\\right) + \\\\text{const.}loss=21\\u200b(log(max(var,eps))+max(var,eps)(input−target)2\\u200b)+const.  where eps is used for stability. By default, the constant term of\\nthe loss function is omitted unless full is True . If var is not the same\\nsize as input (due to a homoscedastic assumption), it must either have a final dimension\\nof 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.  Parameters  full ( bool  ,  optional ) – include the constant term in the loss\\ncalculation. Default: False .  eps ( float  ,  optional ) – value used to clamp var (see note below), for\\nstability. Default: 1e-6.  reduction ( str  ,  optional ) – specifies the reduction to apply to the\\noutput: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction\\nwill be applied, \\'mean\\' : the output is the average of all batch\\nmember losses, \\'sum\\' : the output is the sum of all batch member\\nlosses. Default: \\'mean\\' .  Shape:  Input:(  N  ,  ∗  )  (N, *)(N,∗)or(  ∗  )  (*)(∗)where∗  *∗means any number of additional\\ndimensions  Target:(  N  ,  ∗  )  (N, *)(N,∗)or(  ∗  )  (*)(∗), same shape as the input, or same shape as the input\\nbut with one dimension equal to 1 (to allow for broadcasting)  Var:(  N  ,  ∗  )  (N, *)(N,∗)or(  ∗  )  (*)(∗), same shape as the input, or same shape as the input but\\nwith one dimension equal to 1, or same shape as the input but with one fewer\\ndimension (to allow for broadcasting), or a scalar value  Output: scalar if reduction is \\'mean\\' (default) or \\'sum\\' . If reduction is \\'none\\' , then(  N  ,  ∗  )  (N, *)(N,∗), same\\nshape as the input  Examples::  >>>loss=nn.GaussianNLLLoss()>>>input=torch.randn(5,2,requires_grad=True)>>>target=torch.randn(5,2)>>>var=torch.ones(5,2,requires_grad=True)# heteroscedastic>>>output=loss(input,target,var)>>>output.backward()  >>>loss=nn.GaussianNLLLoss()>>>input=torch.randn(5,2,requires_grad=True)>>>target=torch.randn(5,2)>>>var=torch.ones(5,1,requires_grad=True)# homoscedastic>>>output=loss(input,target,var)>>>output.backward()  Note  The clamping of var is ignored with respect to autograd, and so the\\ngradients are unaffected by it.  Reference:  Nix, D. A. and Weigend, A. S., “Estimating the mean and variance of the\\ntarget probability distribution”, Proceedings of 1994 IEEE International\\nConference on Neural Networks (ICNN’94), Orlando, FL, USA, 1994, pp. 55-60\\nvol.1, doi: 10.1109/ICNN.1994.374138.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html#gaussiannllloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html#gaussiannllloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#GaussianNLLLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L364', 'https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a13'), 'title': 'nn.KLDivLoss', 'page_text': 'KLDivLoss [LINK_1]  class torch.nn.KLDivLoss( size_average=None , reduce=None , reduction=\\'mean\\' , log_target=False ) [source]  [source]  [LINK_2]  The Kullback-Leibler divergence loss.  For tensors of the same shapey  pred  ,  y  true  y_{\\\\text{pred}},\\\\ y_{\\\\text{true}}ypred\\u200b,ytrue\\u200b,\\nwherey  pred  y_{\\\\text{pred}}ypred\\u200bis the input andy  true  y_{\\\\text{true}}ytrue\\u200bis the target , we define the pointwise KL-divergence as  L  (  y  pred  ,  y  true  )  =  y  true  ⋅  log  \\u2061  y  true  y  pred  =  y  true  ⋅  (  log  \\u2061  y  true  −  log  \\u2061  y  pred  )  L(y_{\\\\text{pred}},\\\\ y_{\\\\text{true}})\\n    = y_{\\\\text{true}} \\\\cdot \\\\log \\\\frac{y_{\\\\text{true}}}{y_{\\\\text{pred}}}\\n    = y_{\\\\text{true}} \\\\cdot (\\\\log y_{\\\\text{true}} - \\\\log y_{\\\\text{pred}})L(ypred\\u200b,ytrue\\u200b)=ytrue\\u200b⋅logypred\\u200bytrue\\u200b\\u200b=ytrue\\u200b⋅(logytrue\\u200b−logypred\\u200b)  To avoid underflow issues when computing this quantity, this loss expects the argument input in the log-space. The argument target may also be provided in the\\nlog-space if log_target  = True .  To summarise, this function is roughly equivalent to computing  ifnotlog_target:# defaultloss_pointwise=target*(target.log()-input)else:loss_pointwise=target.exp()*(target-input)  and then reducing this result depending on the argument reduction as  ifreduction==\"mean\":# defaultloss=loss_pointwise.mean()elifreduction==\"batchmean\":# mathematically correctloss=loss_pointwise.sum()/input.size(0)elifreduction==\"sum\":loss=loss_pointwise.sum()else:# reduction == \"none\"loss=loss_pointwise  Note  As all the other losses in PyTorch, this function expects the first argument, input , to be the output of the model (e.g. the neural network)\\nand the second, target , to be the observations in the dataset.\\nThis differs from the standard mathematical notationK  L  (  P  ∣  ∣  Q  )  KL(P\\\\ ||\\\\ Q)KL(P∣∣Q)whereP  PPdenotes the distribution of the observations andQ  QQdenotes the model.  Warning  reduction  = “mean” doesn’t return the true KL divergence value, please use reduction  = “batchmean” which aligns with the mathematical definition.  Parameters  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output. Default: “mean”  log_target ( bool  ,  optional ) – Specifies whether target is the log space. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Output: scalar by default. If reduction is ‘none’ , then(  ∗  )  (*)(∗),\\nsame shape as the input.  Examples::  >>>kl_loss=nn.KLDivLoss(reduction=\"batchmean\")>>># input should be a distribution in the log space>>>input=F.log_softmax(torch.randn(3,5,requires_grad=True),dim=1)>>># Sample a batch of distributions. Usually this would come from the dataset>>>target=F.softmax(torch.rand(3,5),dim=1)>>>output=kl_loss(input,target)  >>>kl_loss=nn.KLDivLoss(reduction=\"batchmean\",log_target=True)>>>log_target=F.log_softmax(torch.rand(3,5),dim=1)>>>output=kl_loss(input,log_target)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#kldivloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#kldivloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#KLDivLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L449', 'https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a14'), 'title': 'nn.BCELoss', 'page_text': 'BCELoss [LINK_1]  class torch.nn.BCELoss( weight=None , size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that measures the Binary Cross Entropy between the target and\\nthe input probabilities:  The unreduced (i.e. with reduction set to \\'none\\' ) loss can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  …  ,  l  N  }  ⊤  ,  l  n  =  −  w  n  [  y  n  ⋅  log  \\u2061  x  n  +  (  1  −  y  n  )  ⋅  log  \\u2061  (  1  −  x  n  )  ]  ,  \\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - w_n \\\\left[ y_n \\\\cdot \\\\log x_n + (1 - y_n) \\\\cdot \\\\log (1 - x_n) \\\\right],ℓ(x,y)=L={l1\\u200b,…,lN\\u200b}⊤,ln\\u200b=−wn\\u200b[yn\\u200b⋅logxn\\u200b+(1−yn\\u200b)⋅log(1−xn\\u200b)],  whereN  NNis the batch size. If reduction is not \\'none\\' (default \\'mean\\' ), then  ℓ  (  x  ,  y  )  =  {  mean  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘mean’;  sum  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) = \\\\begin{cases}\\n    \\\\operatorname{mean}(L), & \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  & \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={mean(L),sum(L),\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  This is used for measuring the error of a reconstruction in for example\\nan auto-encoder. Note that the targetsy  yyshould be numbers\\nbetween 0 and 1.  Notice that ifx  n  x_nxn\\u200bis either 0 or 1, one of the log terms would be\\nmathematically undefined in the above loss equation. PyTorch chooses to setlog  \\u2061  (  0  )  =  −  ∞  \\\\log (0) = -\\\\inftylog(0)=−∞, sincelim  \\u2061  x  →  0  log  \\u2061  (  x  )  =  −  ∞  \\\\lim_{x\\\\to 0} \\\\log (x) = -\\\\inftylimx→0\\u200blog(x)=−∞.\\nHowever, an infinite term in the loss equation is not desirable for several reasons.  For one, if eithery  n  =  0  y_n = 0yn\\u200b=0or(  1  −  y  n  )  =  0  (1 - y_n) = 0(1−yn\\u200b)=0, then we would be\\nmultiplying 0 with infinity. Secondly, if we have an infinite loss value, then\\nwe would also have an infinite term in our gradient, sincelim  \\u2061  x  →  0  d  d  x  log  \\u2061  (  x  )  =  ∞  \\\\lim_{x\\\\to 0} \\\\frac{d}{dx} \\\\log (x) = \\\\inftylimx→0\\u200bdxd\\u200blog(x)=∞.\\nThis would make BCELoss’s backward method nonlinear with respect tox  n  x_nxn\\u200b,\\nand using it for things like linear regression would not be straight-forward.  Our solution is that BCELoss clamps its log function outputs to be greater than\\nor equal to -100. This way, we can always have a finite loss value and a linear\\nbackward method.  Parameters  weight ( Tensor  ,  optional ) – a manual rescaling weight given to the loss\\nof each batch element. If given, has to be a Tensor of size nbatch .  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Output: scalar. If reduction is \\'none\\' , then(  ∗  )  (*)(∗), same\\nshape as input.  Examples:  >>>m=nn.Sigmoid()>>>loss=nn.BCELoss()>>>input=torch.randn(3,2,requires_grad=True)>>>target=torch.rand(3,2,requires_grad=False)>>>output=loss(m(input),target)>>>output.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#bceloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#bceloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#BCELoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L613', 'https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a15'), 'title': 'nn.BCEWithLogitsLoss', 'page_text': 'BCEWithLogitsLoss [LINK_1]  class torch.nn.BCEWithLogitsLoss( weight=None , size_average=None , reduce=None , reduction=\\'mean\\' , pos_weight=None ) [source]  [source]  [LINK_2]  This loss combines a Sigmoid layer and the BCELoss in one single\\nclass. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer,\\nwe take advantage of the log-sum-exp trick for numerical stability.  The unreduced (i.e. with reduction set to \\'none\\' ) loss can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  …  ,  l  N  }  ⊤  ,  l  n  =  −  w  n  [  y  n  ⋅  log  \\u2061  σ  (  x  n  )  +  (  1  −  y  n  )  ⋅  log  \\u2061  (  1  −  σ  (  x  n  )  )  ]  ,  \\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - w_n \\\\left[ y_n \\\\cdot \\\\log \\\\sigma(x_n)\\n+ (1 - y_n) \\\\cdot \\\\log (1 - \\\\sigma(x_n)) \\\\right],ℓ(x,y)=L={l1\\u200b,…,lN\\u200b}⊤,ln\\u200b=−wn\\u200b[yn\\u200b⋅logσ(xn\\u200b)+(1−yn\\u200b)⋅log(1−σ(xn\\u200b))],  whereN  NNis the batch size. If reduction is not \\'none\\' (default \\'mean\\' ), then  ℓ  (  x  ,  y  )  =  {  mean  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘mean’;  sum  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) = \\\\begin{cases}\\n    \\\\operatorname{mean}(L), & \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  & \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={mean(L),sum(L),\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  This is used for measuring the error of a reconstruction in for example\\nan auto-encoder. Note that the targets t[i] should be numbers\\nbetween 0 and 1.  It’s possible to trade off recall and precision by adding weights to positive examples.\\nIn the case of multi-label classification the loss can be described as:  ℓ  c  (  x  ,  y  )  =  L  c  =  {  l  1  ,  c  ,  …  ,  l  N  ,  c  }  ⊤  ,  l  n  ,  c  =  −  w  n  ,  c  [  p  c  y  n  ,  c  ⋅  log  \\u2061  σ  (  x  n  ,  c  )  +  (  1  −  y  n  ,  c  )  ⋅  log  \\u2061  (  1  −  σ  (  x  n  ,  c  )  )  ]  ,  \\\\ell_c(x, y) = L_c = \\\\{l_{1,c},\\\\dots,l_{N,c}\\\\}^\\\\top, \\\\quad\\nl_{n,c} = - w_{n,c} \\\\left[ p_c y_{n,c} \\\\cdot \\\\log \\\\sigma(x_{n,c})\\n+ (1 - y_{n,c}) \\\\cdot \\\\log (1 - \\\\sigma(x_{n,c})) \\\\right],ℓc\\u200b(x,y)=Lc\\u200b={l1,c\\u200b,…,lN,c\\u200b}⊤,ln,c\\u200b=−wn,c\\u200b[pc\\u200byn,c\\u200b⋅logσ(xn,c\\u200b)+(1−yn,c\\u200b)⋅log(1−σ(xn,c\\u200b))],  wherec  ccis the class number (c  >  1  c > 1c>1for multi-label binary classification,c  =  1  c = 1c=1for single-label binary classification),n  nnis the number of the sample in the batch andp  c  p_cpc\\u200bis the weight of the positive answer for the classc  cc.  p  c  >  1  p_c > 1pc\\u200b>1increases the recall,p  c  <  1  p_c < 1pc\\u200b<1increases the precision.  For example, if a dataset contains 100 positive and 300 negative examples of a single class,\\nthen pos_weight for the class should be equal to300  100  =  3  \\\\frac{300}{100}=3100300\\u200b=3.\\nThe loss would act as if the dataset contains3  ×  100  =  300  3\\\\times 100=3003×100=300positive examples.  Examples:  >>>target=torch.ones([10,64],dtype=torch.float32)# 64 classes, batch size = 10>>>output=torch.full([10,64],1.5)# A prediction (logit)>>>pos_weight=torch.ones([64])# All weights are equal to 1>>>criterion=torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)>>>criterion(output,target)# -log(sigmoid(1.5))tensor(0.20...)  In the above example, the pos_weight tensor’s elements correspond to the 64 distinct classes\\nin a multi-label binary classification scenario. Each element in pos_weight is designed to adjust the\\nloss function based on the imbalance between negative and positive samples for the respective class.\\nThis approach is useful in datasets with varying levels of class imbalance, ensuring that the loss\\ncalculation accurately accounts for the distribution in each class.  Parameters  weight ( Tensor  ,  optional ) – a manual rescaling weight given to the loss\\nof each batch element. If given, has to be a Tensor of size nbatch .  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  pos_weight ( Tensor  ,  optional ) – a weight of positive examples to be broadcasted with target.\\nMust be a tensor with equal size along the class dimension to the number of classes.\\nPay close attention to PyTorch’s broadcasting semantics in order to achieve the desired\\noperations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\\nsize [B, C, H, W] will apply different pos_weights to each element of the batch or\\n[C, H, W] the same pos_weights across the batch. To apply the same positive weight\\nalong all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\\nDefault: None  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Output: scalar. If reduction is \\'none\\' , then(  ∗  )  (*)(∗), same\\nshape as input.  Examples:  >>>loss=nn.BCEWithLogitsLoss()>>>input=torch.randn(3,requires_grad=True)>>>target=torch.empty(3).random_(2)>>>output=loss(input,target)>>>output.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#bcewithlogitsloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#bcewithlogitsloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#BCEWithLogitsLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L704', 'https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a16'), 'title': 'nn.MarginRankingLoss', 'page_text': 'MarginRankingLoss [LINK_1]  class torch.nn.MarginRankingLoss( margin=0.0 , size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that measures the loss given\\ninputsx  1  x1x1,x  2  x2x2, two 1D mini-batch or 0D Tensors ,\\nand a label 1D mini-batch or 0D Tensor y  yy(containing 1 or -1).  Ify  =  1  y = 1y=1then it assumed the first input should be ranked higher\\n(have a larger value) than the second input, and vice-versa fory  =  −  1  y = -1y=−1.  The loss function for each pair of samples in the mini-batch is:  loss  (  x  1  ,  x  2  ,  y  )  =  max  \\u2061  (  0  ,  −  y  ∗  (  x  1  −  x  2  )  +  margin  )  \\\\text{loss}(x1, x2, y) = \\\\max(0, -y * (x1 - x2) + \\\\text{margin})loss(x1,x2,y)=max(0,−y∗(x1−x2)+margin)  Parameters  margin ( float  ,  optional ) – Has a default value of0  00.  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input1:(  N  )  (N)(N)or(  )  ()()where N is the batch size.  Input2:(  N  )  (N)(N)or(  )  ()(), same shape as the Input1.  Target:(  N  )  (N)(N)or(  )  ()(), same shape as the inputs.  Output: scalar. If reduction is \\'none\\' and Input size is not(  )  ()(), then(  N  )  (N)(N).  Examples:  >>>loss=nn.MarginRankingLoss()>>>input1=torch.randn(3,requires_grad=True)>>>input2=torch.randn(3,requires_grad=True)>>>target=torch.randn(3).sign()>>>output=loss(input1,input2,target)>>>output.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html#marginrankingloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html#marginrankingloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MarginRankingLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1430', 'https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a17'), 'title': 'nn.HingeEmbeddingLoss', 'page_text': 'HingeEmbeddingLoss [LINK_1]  class torch.nn.HingeEmbeddingLoss( margin=1.0 , size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Measures the loss given an input tensorx  xxand a labels tensory  yy(containing 1 or -1).\\nThis is usually used for measuring whether two inputs are similar or\\ndissimilar, e.g. using the L1 pairwise distance asx  xx, and is typically\\nused for learning nonlinear embeddings or semi-supervised learning.  The loss function forn  nn-th sample in the mini-batch is  l  n  =  {  x  n  ,  if  y  n  =  1  ,  max  \\u2061  {  0  ,  m  a  r  g  i  n  −  x  n  }  ,  if  y  n  =  −  1  ,  l_n = \\\\begin{cases}\\n    x_n, & \\\\text{if}\\\\; y_n = 1,\\\\\\\\\\n    \\\\max \\\\{0, margin - x_n\\\\}, & \\\\text{if}\\\\; y_n = -1,\\n\\\\end{cases}ln\\u200b={xn\\u200b,max{0,margin−xn\\u200b},\\u200bifyn\\u200b=1,ifyn\\u200b=−1,\\u200b  and the total loss functions is  ℓ  (  x  ,  y  )  =  {  mean  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘mean’;  sum  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) = \\\\begin{cases}\\n    \\\\operatorname{mean}(L), & \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  & \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={mean(L),sum(L),\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  whereL  =  {  l  1  ,  …  ,  l  N  }  ⊤  L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\topL={l1\\u200b,…,lN\\u200b}⊤.  Parameters  margin ( float  ,  optional ) – Has a default value of 1 .  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  ∗  )  (*)(∗)where∗  *∗means, any number of dimensions. The sum operation\\noperates over all the elements.  Target:(  ∗  )  (*)(∗), same shape as the input  Output: scalar. If reduction is \\'none\\' , then same shape as the input\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html#hingeembeddingloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html#hingeembeddingloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#HingeEmbeddingLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L830', 'https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelMarginLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a18'), 'title': 'nn.MultiLabelMarginLoss', 'page_text': 'MultiLabelMarginLoss [LINK_1]  class torch.nn.MultiLabelMarginLoss( size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that optimizes a multi-class multi-classification\\nhinge loss (margin-based loss) between inputx  xx(a 2D mini-batch Tensor )\\nand outputy  yy(which is a 2D Tensor of target class indices).\\nFor each sample in the mini-batch:  loss  (  x  ,  y  )  =  ∑  i  j  max  \\u2061  (  0  ,  1  −  (  x  [  y  [  j  ]  ]  −  x  [  i  ]  )  )  x.size  (  0  )  \\\\text{loss}(x, y) = \\\\sum_{ij}\\\\frac{\\\\max(0, 1 - (x[y[j]] - x[i]))}{\\\\text{x.size}(0)}loss(x,y)=ij∑\\u200bx.size(0)max(0,1−(x[y[j]]−x[i]))\\u200b  wherex  ∈  {  0  ,  ⋯  ,  x.size  (  0  )  −  1  }  x \\\\in \\\\left\\\\{0, \\\\; \\\\cdots , \\\\; \\\\text{x.size}(0) - 1\\\\right\\\\}x∈{0,⋯,x.size(0)−1},y  ∈  {  0  ,  ⋯  ,  y.size  (  0  )  −  1  }  y \\\\in \\\\left\\\\{0, \\\\; \\\\cdots , \\\\; \\\\text{y.size}(0) - 1\\\\right\\\\}y∈{0,⋯,y.size(0)−1},0  ≤  y  [  j  ]  ≤  x.size  (  0  )  −  1  0 \\\\leq y[j] \\\\leq \\\\text{x.size}(0)-10≤y[j]≤x.size(0)−1, andi  ≠  y  [  j  ]  i \\\\neq y[j]i\\ue020=y[j]for alli  iiandj  jj.  y  yyandx  xxmust have the same size.  The criterion only considers a contiguous block of non-negative targets that\\nstarts at the front.  This allows for different samples to have variable amounts of target classes.  Parameters  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  C  )  (C)(C)or(  N  ,  C  )  (N, C)(N,C)where N is the batch size and C is the number of classes.  Target:(  C  )  (C)(C)or(  N  ,  C  )  (N, C)(N,C), label targets padded by -1 ensuring same shape as the input.  Output: scalar. If reduction is \\'none\\' , then(  N  )  (N)(N).  Examples:  >>>loss=nn.MultiLabelMarginLoss()>>>x=torch.FloatTensor([[0.1,0.2,0.4,0.8]])>>># for target y, only consider labels 3 and 0, not after label -1>>>y=torch.LongTensor([[3,0,-1,1]])>>># 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))>>>loss(x,y)tensor(0.85...)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelMarginLoss.html#multilabelmarginloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelMarginLoss.html#multilabelmarginloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MultiLabelMarginLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L898', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a19'), 'title': 'nn.HuberLoss', 'page_text': 'HuberLoss [LINK_1]  class torch.nn.HuberLoss( reduction=\\'mean\\' , delta=1.0 ) [source]  [source]  [LINK_2]  Creates a criterion that uses a squared term if the absolute\\nelement-wise error falls below delta and a delta-scaled L1 term otherwise.\\nThis loss combines advantages of both L1Loss and MSELoss ; the\\ndelta-scaled L1 region makes the loss less sensitive to outliers than MSELoss ,\\nwhile the L2 region provides smoothness over L1Loss near 0. See Huber loss for more information.  For a batch of sizeN  NN, the unreduced loss can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  .  .  .  ,  l  N  }  T  \\\\ell(x, y) = L = \\\\{l_1, ..., l_N\\\\}^Tℓ(x,y)=L={l1\\u200b,...,lN\\u200b}T  with  l  n  =  {  0.5  (  x  n  −  y  n  )  2  ,  if  ∣  x  n  −  y  n  ∣  <  d  e  l  t  a  d  e  l  t  a  ∗  (  ∣  x  n  −  y  n  ∣  −  0.5  ∗  d  e  l  t  a  )  ,  otherwise  l_n = \\\\begin{cases}\\n0.5 (x_n - y_n)^2, & \\\\text{if } |x_n - y_n| < delta \\\\\\\\\\ndelta * (|x_n - y_n| - 0.5 * delta), & \\\\text{otherwise }\\n\\\\end{cases}ln\\u200b={0.5(xn\\u200b−yn\\u200b)2,delta∗(∣xn\\u200b−yn\\u200b∣−0.5∗delta),\\u200bif∣xn\\u200b−yn\\u200b∣<deltaotherwise\\u200b  If reduction is not none , then:  ℓ  (  x  ,  y  )  =  {  mean  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘mean’;  sum  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) =\\n\\\\begin{cases}\\n    \\\\operatorname{mean}(L), &  \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  &  \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={mean(L),sum(L),\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  Note  When delta is set to 1, this loss is equivalent to SmoothL1Loss .\\nIn general, this loss differs from SmoothL1Loss by a factor of delta (AKA beta\\nin Smooth L1).\\nSee SmoothL1Loss for additional discussion on the differences in behavior\\nbetween the two losses.  Parameters  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Default: \\'mean\\'  delta ( float  ,  optional ) – Specifies the threshold at which to change between delta-scaled L1 and L2 loss.\\nThe value must be positive.  Default: 1.0  Shape:  Input:(  ∗  )  (*)(∗)where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Output: scalar. If reduction is \\'none\\' , then(  ∗  )  (*)(∗), same shape as the input.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#huberloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#huberloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#HuberLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1045', 'https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss', 'https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss', 'https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss', 'https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss', 'https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss', 'https://en.wikipedia.org/wiki/Huber_loss', 'https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss', 'https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss', 'https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelMarginLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a1a'), 'title': 'nn.SmoothL1Loss', 'page_text': 'SmoothL1Loss [LINK_1]  class torch.nn.SmoothL1Loss( size_average=None , reduce=None , reduction=\\'mean\\' , beta=1.0 ) [source]  [source]  [LINK_2]  Creates a criterion that uses a squared term if the absolute\\nelement-wise error falls below beta and an L1 term otherwise.\\nIt is less sensitive to outliers than torch.nn.MSELoss and in some cases\\nprevents exploding gradients (e.g. see the paper Fast R-CNN by Ross Girshick).  For a batch of sizeN  NN, the unreduced loss can be described as:  ℓ  (  x  ,  y  )  =  L  =  {  l  1  ,  .  .  .  ,  l  N  }  T  \\\\ell(x, y) = L = \\\\{l_1, ..., l_N\\\\}^Tℓ(x,y)=L={l1\\u200b,...,lN\\u200b}T  with  l  n  =  {  0.5  (  x  n  −  y  n  )  2  /  b  e  t  a  ,  if  ∣  x  n  −  y  n  ∣  <  b  e  t  a  ∣  x  n  −  y  n  ∣  −  0.5  ∗  b  e  t  a  ,  otherwise  l_n = \\\\begin{cases}\\n0.5 (x_n - y_n)^2 / beta, & \\\\text{if } |x_n - y_n| < beta \\\\\\\\\\n|x_n - y_n| - 0.5 * beta, & \\\\text{otherwise }\\n\\\\end{cases}ln\\u200b={0.5(xn\\u200b−yn\\u200b)2/beta,∣xn\\u200b−yn\\u200b∣−0.5∗beta,\\u200bif∣xn\\u200b−yn\\u200b∣<betaotherwise\\u200b  If reduction is not none , then:  ℓ  (  x  ,  y  )  =  {  mean  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘mean’;  sum  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) =\\n\\\\begin{cases}\\n    \\\\operatorname{mean}(L), &  \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  &  \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={mean(L),sum(L),\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  Note  Smooth L1 loss can be seen as exactly L1Loss , but with the∣  x  −  y  ∣  <  b  e  t  a  |x - y| < beta∣x−y∣<betaportion replaced with a quadratic function such that its slope is 1 at∣  x  −  y  ∣  =  b  e  t  a  |x - y| = beta∣x−y∣=beta.\\nThe quadratic segment smooths the L1 loss near∣  x  −  y  ∣  =  0  |x - y| = 0∣x−y∣=0.  Note  Smooth L1 loss is closely related to HuberLoss , being\\nequivalent toh  u  b  e  r  (  x  ,  y  )  /  b  e  t  a  huber(x, y) / betahuber(x,y)/beta(note that Smooth L1’s beta hyper-parameter is\\nalso known as delta for Huber). This leads to the following differences:  As beta -> 0, Smooth L1 loss converges to L1Loss , while HuberLoss converges to a constant 0 loss. When beta is 0, Smooth L1 loss is equivalent to L1 loss.  As beta ->+  ∞  +\\\\infty+∞, Smooth L1 loss converges to a constant 0 loss, while HuberLoss converges to MSELoss .  For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope of 1.\\nFor HuberLoss , the slope of the L1 segment is beta.  Parameters  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  beta ( float  ,  optional ) – Specifies the threshold at which to change between L1 and L2 loss.\\nThe value must be non-negative. Default: 1.0  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Output: scalar. If reduction is \\'none\\' , then(  ∗  )  (*)(∗), same shape as the input.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#smoothl1loss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#smoothl1loss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#SmoothL1Loss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L962', 'https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss', 'https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss', 'https://arxiv.org/abs/1504.08083', 'https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss', 'https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss', 'https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss', 'https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss', 'https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss', 'https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss', 'https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.SoftMarginLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a1b'), 'title': 'nn.SoftMarginLoss', 'page_text': 'SoftMarginLoss [LINK_1]  class torch.nn.SoftMarginLoss( size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that optimizes a two-class classification\\nlogistic loss between input tensorx  xxand target tensory  yy(containing 1 or -1).  loss  (  x  ,  y  )  =  ∑  i  log  \\u2061  (  1  +  exp  \\u2061  (  −  y  [  i  ]  ∗  x  [  i  ]  )  )  x.nelement  (  )  \\\\text{loss}(x, y) = \\\\sum_i \\\\frac{\\\\log(1 + \\\\exp(-y[i]*x[i]))}{\\\\text{x.nelement}()}loss(x,y)=i∑\\u200bx.nelement()log(1+exp(−y[i]∗x[i]))\\u200b  Parameters  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Target:(  ∗  )  (*)(∗), same shape as the input.  Output: scalar. If reduction is \\'none\\' , then(  ∗  )  (*)(∗), same\\nshape as input.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.SoftMarginLoss.html#softmarginloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.SoftMarginLoss.html#softmarginloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#SoftMarginLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1105', 'https://pytorch.org/docs/stable/generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a1c'), 'title': 'nn.MultiLabelSoftMarginLoss', 'page_text': 'MultiLabelSoftMarginLoss [LINK_1]  class torch.nn.MultiLabelSoftMarginLoss( weight=None , size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that optimizes a multi-label one-versus-all\\nloss based on max-entropy, between inputx  xxand targety  yyof size(  N  ,  C  )  (N, C)(N,C).\\nFor each sample in the minibatch:  l  o  s  s  (  x  ,  y  )  =  −  1  C  ∗  ∑  i  y  [  i  ]  ∗  log  \\u2061  (  (  1  +  exp  \\u2061  (  −  x  [  i  ]  )  )  −  1  )  +  (  1  −  y  [  i  ]  )  ∗  log  \\u2061  (  exp  \\u2061  (  −  x  [  i  ]  )  (  1  +  exp  \\u2061  (  −  x  [  i  ]  )  )  )  loss(x, y) = - \\\\frac{1}{C} * \\\\sum_i y[i] * \\\\log((1 + \\\\exp(-x[i]))^{-1})\\n                 + (1-y[i]) * \\\\log\\\\left(\\\\frac{\\\\exp(-x[i])}{(1 + \\\\exp(-x[i]))}\\\\right)loss(x,y)=−C1\\u200b∗i∑\\u200by[i]∗log((1+exp(−x[i]))−1)+(1−y[i])∗log((1+exp(−x[i]))exp(−x[i])\\u200b)  wherei  ∈  {  0  ,  ⋯  ,  x.nElement  (  )  −  1  }  i \\\\in \\\\left\\\\{0, \\\\; \\\\cdots , \\\\; \\\\text{x.nElement}() - 1\\\\right\\\\}i∈{0,⋯,x.nElement()−1},y  [  i  ]  ∈  {  0  ,  1  }  y[i] \\\\in \\\\left\\\\{0, \\\\; 1\\\\right\\\\}y[i]∈{0,1}.  Parameters  weight ( Tensor  ,  optional ) – a manual rescaling weight given to each\\nclass. If given, it has to be a Tensor of size C . Otherwise, it is\\ntreated as if having all ones.  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  N  ,  C  )  (N, C)(N,C)where N is the batch size and C is the number of classes.  Target:(  N  ,  C  )  (N, C)(N,C), label targets must have the same shape as the input.  Output: scalar. If reduction is \\'none\\' , then(  N  )  (N)(N).\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html#multilabelsoftmarginloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html#multilabelsoftmarginloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MultiLabelSoftMarginLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1305', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.SoftMarginLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a1d'), 'title': 'nn.CosineEmbeddingLoss', 'page_text': 'CosineEmbeddingLoss [LINK_1]  class torch.nn.CosineEmbeddingLoss( margin=0.0 , size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that measures the loss given input tensorsx  1  x_1x1\\u200b,x  2  x_2x2\\u200band a Tensor labely  yywith values 1 or -1.\\nUse (y  =  1  y=1y=1) to maximize the cosine similarity of two inputs, and (y  =  −  1  y=-1y=−1) otherwise.\\nThis is typically used for learning nonlinear\\nembeddings or semi-supervised learning.  The loss function for each sample is:  loss  (  x  ,  y  )  =  {  1  −  cos  \\u2061  (  x  1  ,  x  2  )  ,  if  y  =  1  max  \\u2061  (  0  ,  cos  \\u2061  (  x  1  ,  x  2  )  −  margin  )  ,  if  y  =  −  1  \\\\text{loss}(x, y) =\\n\\\\begin{cases}\\n1 - \\\\cos(x_1, x_2), & \\\\text{if } y = 1 \\\\\\\\\\n\\\\max(0, \\\\cos(x_1, x_2) - \\\\text{margin}), & \\\\text{if } y = -1\\n\\\\end{cases}loss(x,y)={1−cos(x1\\u200b,x2\\u200b),max(0,cos(x1\\u200b,x2\\u200b)−margin),\\u200bify=1ify=−1\\u200b  Parameters  margin ( float  ,  optional ) – Should be a number from−  1  -1−1to1  11,0  00to0.5  0.50.5is suggested. If margin is missing, the\\ndefault value is0  00.  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input1:(  N  ,  D  )  (N, D)(N,D)or(  D  )  (D)(D), where N is the batch size and D is the embedding dimension.  Input2:(  N  ,  D  )  (N, D)(N,D)or(  D  )  (D)(D), same shape as Input1.  Target:(  N  )  (N)(N)or(  )  ()().  Output: If reduction is \\'none\\' , then(  N  )  (N)(N), otherwise scalar.  Examples:  >>>loss=nn.CosineEmbeddingLoss()>>>input1=torch.randn(3,5,requires_grad=True)>>>input2=torch.randn(3,5,requires_grad=True)>>>target=torch.ones(3)>>>output=loss(input1,input2,target)>>>output.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#cosineembeddingloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#cosineembeddingloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CosineEmbeddingLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1360', 'https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiMarginLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a1e'), 'title': 'nn.MultiMarginLoss', 'page_text': 'MultiMarginLoss [LINK_1]  class torch.nn.MultiMarginLoss( p=1 , margin=1.0 , weight=None , size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that optimizes a multi-class classification hinge\\nloss (margin-based loss) between inputx  xx(a 2D mini-batch Tensor ) and\\noutputy  yy(which is a 1D tensor of target class indices,0  ≤  y  ≤  x.size  (  1  )  −  1  0 \\\\leq y \\\\leq \\\\text{x.size}(1)-10≤y≤x.size(1)−1):  For each mini-batch sample, the loss in terms of the 1D inputx  xxand scalar\\noutputy  yyis:  loss  (  x  ,  y  )  =  ∑  i  max  \\u2061  (  0  ,  margin  −  x  [  y  ]  +  x  [  i  ]  )  p  x.size  (  0  )  \\\\text{loss}(x, y) = \\\\frac{\\\\sum_i \\\\max(0, \\\\text{margin} - x[y] + x[i])^p}{\\\\text{x.size}(0)}loss(x,y)=x.size(0)∑i\\u200bmax(0,margin−x[y]+x[i])p\\u200b  wherei  ∈  {  0  ,  ⋯  ,  x.size  (  0  )  −  1  }  i \\\\in \\\\left\\\\{0, \\\\; \\\\cdots , \\\\; \\\\text{x.size}(0) - 1\\\\right\\\\}i∈{0,⋯,x.size(0)−1}andi  ≠  y  i \\\\neq yi\\ue020=y.  Optionally, you can give non-equal weighting on the classes by passing\\na 1D weight tensor into the constructor.  The loss function then becomes:  loss  (  x  ,  y  )  =  ∑  i  w  [  y  ]  ∗  max  \\u2061  (  0  ,  margin  −  x  [  y  ]  +  x  [  i  ]  )  p  x.size  (  0  )  \\\\text{loss}(x, y) = \\\\frac{\\\\sum_i w[y] * \\\\max(0, \\\\text{margin} - x[y] + x[i])^p}{\\\\text{x.size}(0)}loss(x,y)=x.size(0)∑i\\u200bw[y]∗max(0,margin−x[y]+x[i])p\\u200b  Parameters  p ( int  ,  optional ) – Has a default value of1  11.1  11and2  22are the only supported values.  margin ( float  ,  optional ) – Has a default value of1  11.  weight ( Tensor  ,  optional ) – a manual rescaling weight given to each\\nclass. If given, it has to be a Tensor of size C . Otherwise, it is\\ntreated as if having all ones.  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  N  ,  C  )  (N, C)(N,C)or(  C  )  (C)(C), whereN  NNis the batch size andC  CCis the number of classes.  Target:(  N  )  (N)(N)or(  )  ()(), where each value is0  ≤  targets  [  i  ]  ≤  C  −  1  0 \\\\leq \\\\text{targets}[i] \\\\leq C-10≤targets[i]≤C−1.  Output: scalar. If reduction is \\'none\\' , then same shape as the target.  Examples:  >>>loss=nn.MultiMarginLoss()>>>x=torch.tensor([[0.1,0.2,0.4,0.8]])>>>y=torch.tensor([3])>>># 0.25 * ((1-(0.8-0.1)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))>>>loss(x,y)tensor(0.32...)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiMarginLoss.html#multimarginloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.MultiMarginLoss.html#multimarginloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MultiMarginLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1495', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a1f'), 'title': 'nn.TripletMarginLoss', 'page_text': 'TripletMarginLoss [LINK_1]  class torch.nn.TripletMarginLoss( margin=1.0 , p=2.0 , eps=1e-06 , swap=False , size_average=None , reduce=None , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that measures the triplet loss given an input\\ntensorsx  1  x1x1,x  2  x2x2,x  3  x3x3and a margin with a value greater than0  00.\\nThis is used for measuring a relative similarity between samples. A triplet\\nis composed by a , p and n (i.e., anchor , positive examples and negative\\nexamples respectively). The shapes of all input tensors should be(  N  ,  D  )  (N, D)(N,D).  The distance swap is described in detail in the paper Learning shallow\\nconvolutional feature descriptors with triplet losses by\\nV. Balntas, E. Riba et al.  The loss function for each sample in the mini-batch is:  L  (  a  ,  p  ,  n  )  =  max  \\u2061  {  d  (  a  i  ,  p  i  )  −  d  (  a  i  ,  n  i  )  +  m  a  r  g  i  n  ,  0  }  L(a, p, n) = \\\\max \\\\{d(a_i, p_i) - d(a_i, n_i) + {\\\\rm margin}, 0\\\\}L(a,p,n)=max{d(ai\\u200b,pi\\u200b)−d(ai\\u200b,ni\\u200b)+margin,0}  where  d  (  x  i  ,  y  i  )  =  ∥  x  i  −  y  i  ∥  p  d(x_i, y_i) = \\\\left\\\\lVert {\\\\bf x}_i - {\\\\bf y}_i \\\\right\\\\rVert_pd(xi\\u200b,yi\\u200b)=∥xi\\u200b−yi\\u200b∥p\\u200b  The norm is calculated using the specified p value and a small constantε  \\\\varepsilonεis\\nadded for numerical stability.  See also TripletMarginWithDistanceLoss , which computes the\\ntriplet margin loss for input tensors using a custom distance function.  Parameters  margin ( float  ,  optional ) – Default:1  11.  p ( int  ,  optional ) – The norm degree for pairwise distance. Default:2  22.  eps ( float  ,  optional ) – Small constant for numerical stability. Default:1  e  −  6  1e-61e−6.  swap ( bool  ,  optional ) – The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by\\nV. Balntas, E. Riba et al. Default: False .  size_average ( bool  ,  optional ) – Deprecated (see reduction ). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field size_average is set to False , the losses are instead summed for each minibatch. Ignored\\nwhen reduce is False . Default: True  reduce ( bool  ,  optional ) – Deprecated (see reduction ). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non size_average . When reduce is False , returns a loss per\\nbatch element instead and ignores size_average . Default: True  reduction ( str  ,  optional ) – Specifies the reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override reduction . Default: \\'mean\\'  Shape:  Input:(  N  ,  D  )  (N, D)(N,D)or(  D  )  (D)(D)whereD  DDis the vector dimension.  Output: A Tensor of shape(  N  )  (N)(N)if reduction is \\'none\\' and\\ninput shape is(  N  ,  D  )  (N, D)(N,D); a scalar otherwise.  Examples:  >>>triplet_loss=nn.TripletMarginLoss(margin=1.0,p=2,eps=1e-7)>>>anchor=torch.randn(100,128,requires_grad=True)>>>positive=torch.randn(100,128,requires_grad=True)>>>negative=torch.randn(100,128,requires_grad=True)>>>output=triplet_loss(anchor,positive,negative)>>>output.backward()\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html#tripletmarginloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html#tripletmarginloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#TripletMarginLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1589', 'https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss', 'https://bmva-archive.org.uk/bmvc/2016/papers/paper119/index.html', 'https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html', 'https://pytorch.org/docs/stable/generated/torch.nn.MultiMarginLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a20'), 'title': 'nn.TripletMarginWithDistanceLoss', 'page_text': 'TripletMarginWithDistanceLoss [LINK_1]  class torch.nn.TripletMarginWithDistanceLoss( * , distance_function=None , margin=1.0 , swap=False , reduction=\\'mean\\' ) [source]  [source]  [LINK_2]  Creates a criterion that measures the triplet loss given input\\ntensorsa  aa,p  pp, andn  nn(representing anchor,\\npositive, and negative examples, respectively), and a nonnegative,\\nreal-valued function (“distance function”) used to compute the relationship\\nbetween the anchor and positive example (“positive distance”) and the\\nanchor and negative example (“negative distance”).  The unreduced loss (i.e., with reduction set to \\'none\\' )\\ncan be described as:  ℓ  (  a  ,  p  ,  n  )  =  L  =  {  l  1  ,  …  ,  l  N  }  ⊤  ,  l  i  =  max  \\u2061  {  d  (  a  i  ,  p  i  )  −  d  (  a  i  ,  n  i  )  +  m  a  r  g  i  n  ,  0  }  \\\\ell(a, p, n) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_i = \\\\max \\\\{d(a_i, p_i) - d(a_i, n_i) + {\\\\rm margin}, 0\\\\}ℓ(a,p,n)=L={l1\\u200b,…,lN\\u200b}⊤,li\\u200b=max{d(ai\\u200b,pi\\u200b)−d(ai\\u200b,ni\\u200b)+margin,0}  whereN  NNis the batch size;d  ddis a nonnegative, real-valued function\\nquantifying the closeness of two tensors, referred to as the distance_function ;\\nandm  a  r  g  i  n  marginmarginis a nonnegative margin representing the minimum difference\\nbetween the positive and negative distances that is required for the loss to\\nbe 0.  The input tensors haveN  NNelements each and can be of any shape\\nthat the distance function can handle.  If reduction is not \\'none\\' (default \\'mean\\' ), then:  ℓ  (  x  ,  y  )  =  {  mean  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘mean’;  sum  \\u2061  (  L  )  ,  if\\xa0reduction  =  ‘sum’.  \\\\ell(x, y) =\\n\\\\begin{cases}\\n    \\\\operatorname{mean}(L), &  \\\\text{if reduction} = \\\\text{`mean\\';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  &  \\\\text{if reduction} = \\\\text{`sum\\'.}\\n\\\\end{cases}ℓ(x,y)={mean(L),sum(L),\\u200bif\\xa0reduction=‘mean’;if\\xa0reduction=‘sum’.\\u200b  See also TripletMarginLoss , which computes the triplet\\nloss for input tensors using thel  p  l_plp\\u200bdistance as the distance function.  Parameters  distance_function ( Callable  ,  optional ) – A nonnegative, real-valued function that\\nquantifies the closeness of two tensors. If not specified, nn.PairwiseDistance will be used.  Default: None  margin ( float  ,  optional ) – A nonnegative margin representing the minimum difference\\nbetween the positive and negative distances required for the loss to be 0. Larger\\nmargins penalize cases where the negative examples are not distant enough from the\\nanchors, relative to the positives. Default:1  11.  swap ( bool  ,  optional ) – Whether to use the distance swap described in the paper Learning shallow convolutional feature descriptors with triplet losses by\\nV. Balntas, E. Riba et al. If True, and if the positive example is closer to the\\nnegative example than the anchor is, swaps the positive example and the anchor in\\nthe loss computation. Default: False .  reduction ( str  ,  optional ) – Specifies the (optional) reduction to apply to the output: \\'none\\' | \\'mean\\' | \\'sum\\' . \\'none\\' : no reduction will be applied, \\'mean\\' : the sum of the output will be divided by the number of\\nelements in the output, \\'sum\\' : the output will be summed. Default: \\'mean\\'  Shape:  Input:(  N  ,  ∗  )  (N, *)(N,∗)where∗  *∗represents any number of additional dimensions\\nas supported by the distance function.  Output: A Tensor of shape(  N  )  (N)(N)if reduction is \\'none\\' , or a scalar\\notherwise.  Examples:  >>># Initialize embeddings>>>embedding=nn.Embedding(1000,128)>>>anchor_ids=torch.randint(0,1000,(1,))>>>positive_ids=torch.randint(0,1000,(1,))>>>negative_ids=torch.randint(0,1000,(1,))>>>anchor=embedding(anchor_ids)>>>positive=embedding(positive_ids)>>>negative=embedding(negative_ids)>>>>>># Built-in Distance Function>>>triplet_loss=\\\\>>>nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance())>>>output=triplet_loss(anchor,positive,negative)>>>output.backward()>>>>>># Custom Distance Function>>>defl_infinity(x1,x2):>>>returntorch.max(torch.abs(x1-x2),dim=1).values>>>>>>triplet_loss=(>>>nn.TripletMarginWithDistanceLoss(distance_function=l_infinity,margin=1.5))>>>output=triplet_loss(anchor,positive,negative)>>>output.backward()>>>>>># Custom Distance Function (Lambda)>>>triplet_loss=(>>>nn.TripletMarginWithDistanceLoss(>>>distance_function=lambdax,y:1.0-F.cosine_similarity(x,y)))>>>output=triplet_loss(anchor,positive,negative)>>>output.backward()  Reference:  V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: https://bmva-archive.org.uk/bmvc/2016/papers/paper119/index.html\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#tripletmarginwithdistanceloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#tripletmarginwithdistanceloss', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#TripletMarginWithDistanceLoss', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/loss.py#L1697', 'https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss', 'https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://bmva-archive.org.uk/bmvc/2016/papers/paper119/index.html', 'https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html', 'https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a21'), 'title': 'nn.PixelShuffle', 'page_text': 'PixelShuffle [LINK_1]  class torch.nn.PixelShuffle( upscale_factor ) [source]  [source]  [LINK_2]  Rearrange elements in a tensor according to an upscaling factor.  Rearranges elements in a tensor of shape(  ∗  ,  C  ×  r  2  ,  H  ,  W  )  (*, C \\\\times r^2, H, W)(∗,C×r2,H,W)to a tensor of shape(  ∗  ,  C  ,  H  ×  r  ,  W  ×  r  )  (*, C, H \\\\times r, W \\\\times r)(∗,C,H×r,W×r), where r is an upscale factor.  This is useful for implementing efficient sub-pixel convolution\\nwith a stride of1  /  r  1/r1/r.  See the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et al. (2016) for more details.  Parameters  upscale_factor ( int ) – factor to increase spatial resolution by  Shape:  Input:(  ∗  ,  C  i  n  ,  H  i  n  ,  W  i  n  )  (*, C_{in}, H_{in}, W_{in})(∗,Cin\\u200b,Hin\\u200b,Win\\u200b), where * is zero or more batch dimensions  Output:(  ∗  ,  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (*, C_{out}, H_{out}, W_{out})(∗,Cout\\u200b,Hout\\u200b,Wout\\u200b), where  C  o  u  t  =  C  i  n  ÷  upscale_factor  2  C_{out} = C_{in} \\\\div \\\\text{upscale\\\\_factor}^2Cout\\u200b=Cin\\u200b÷upscale_factor2  H  o  u  t  =  H  i  n  ×  upscale_factor  H_{out} = H_{in} \\\\times \\\\text{upscale\\\\_factor}Hout\\u200b=Hin\\u200b×upscale_factor  W  o  u  t  =  W  i  n  ×  upscale_factor  W_{out} = W_{in} \\\\times \\\\text{upscale\\\\_factor}Wout\\u200b=Win\\u200b×upscale_factor  Examples:  >>>pixel_shuffle=nn.PixelShuffle(3)>>>input=torch.randn(1,9,4,4)>>>output=pixel_shuffle(input)>>>print(output.size())torch.Size([1, 1, 12, 12])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#pixelshuffle\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#pixelshuffle', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pixelshuffle.html#PixelShuffle', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pixelshuffle.py#L10', 'https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle', 'https://arxiv.org/abs/1609.05158', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html', 'https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a22'), 'title': 'nn.PixelUnshuffle', 'page_text': 'PixelUnshuffle [LINK_1]  class torch.nn.PixelUnshuffle( downscale_factor ) [source]  [source]  [LINK_2]  Reverse the PixelShuffle operation.  Reverses the PixelShuffle operation by rearranging elements\\nin a tensor of shape(  ∗  ,  C  ,  H  ×  r  ,  W  ×  r  )  (*, C, H \\\\times r, W \\\\times r)(∗,C,H×r,W×r)to a tensor of shape(  ∗  ,  C  ×  r  2  ,  H  ,  W  )  (*, C \\\\times r^2, H, W)(∗,C×r2,H,W), where r is a downscale factor.  See the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et al. (2016) for more details.  Parameters  downscale_factor ( int ) – factor to decrease spatial resolution by  Shape:  Input:(  ∗  ,  C  i  n  ,  H  i  n  ,  W  i  n  )  (*, C_{in}, H_{in}, W_{in})(∗,Cin\\u200b,Hin\\u200b,Win\\u200b), where * is zero or more batch dimensions  Output:(  ∗  ,  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (*, C_{out}, H_{out}, W_{out})(∗,Cout\\u200b,Hout\\u200b,Wout\\u200b), where  C  o  u  t  =  C  i  n  ×  downscale_factor  2  C_{out} = C_{in} \\\\times \\\\text{downscale\\\\_factor}^2Cout\\u200b=Cin\\u200b×downscale_factor2  H  o  u  t  =  H  i  n  ÷  downscale_factor  H_{out} = H_{in} \\\\div \\\\text{downscale\\\\_factor}Hout\\u200b=Hin\\u200b÷downscale_factor  W  o  u  t  =  W  i  n  ÷  downscale_factor  W_{out} = W_{in} \\\\div \\\\text{downscale\\\\_factor}Wout\\u200b=Win\\u200b÷downscale_factor  Examples:  >>>pixel_unshuffle=nn.PixelUnshuffle(3)>>>input=torch.randn(1,1,12,12)>>>output=pixel_unshuffle(input)>>>print(output.size())torch.Size([1, 9, 4, 4])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html#pixelunshuffle\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html#pixelunshuffle', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/pixelshuffle.html#PixelUnshuffle', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/pixelshuffle.py#L65', 'https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle', 'https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle', 'https://arxiv.org/abs/1609.05158', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html', 'https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a23'), 'title': 'nn.Upsample', 'page_text': 'Upsample [LINK_1]  class torch.nn.Upsample( size=None , scale_factor=None , mode=\\'nearest\\' , align_corners=None , recompute_scale_factor=None ) [source]  [source]  [LINK_2]  Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.  The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width .\\nHence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.  The algorithms available for upsampling are nearest neighbor and linear,\\nbilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor,\\nrespectively.  One can either give a scale_factor or the target output size to\\ncalculate the output size. (You cannot give both, as it is ambiguous)  Parameters  size ( int  or  Tuple  [  int  ] or  Tuple  [  int  ,  int  ] or  Tuple  [  int  ,  int  ,  int  ]  ,  optional ) – output spatial sizes  scale_factor ( float  or  Tuple  [  float  ] or  Tuple  [  float  ,  float  ] or  Tuple  [  float  ,  float  ,  float  ]  ,  optional ) – multiplier for spatial size. Has to match input size if it is a tuple.  mode ( str  ,  optional ) – the upsampling algorithm: one of \\'nearest\\' , \\'linear\\' , \\'bilinear\\' , \\'bicubic\\' and \\'trilinear\\' .\\nDefault: \\'nearest\\'  align_corners ( bool  ,  optional ) – if True , the corner pixels of the input\\nand output tensors are aligned, and thus preserving the values at\\nthose pixels. This only has effect when mode is \\'linear\\' , \\'bilinear\\' , \\'bicubic\\' , or \\'trilinear\\' .\\nDefault: False  recompute_scale_factor ( bool  ,  optional ) – recompute the scale_factor for use in the\\ninterpolation calculation. If recompute_scale_factor is True , then scale_factor must be passed in and scale_factor is used to compute the\\noutput size . The computed output size will be used to infer new scales for\\nthe interpolation. Note that when scale_factor is floating-point, it may differ\\nfrom the recomputed scale_factor due to rounding and precision issues.\\nIf recompute_scale_factor is False , then size or scale_factor will\\nbe used directly for interpolation.  Shape:  Input:(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b),(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b),(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  ⌊  D  i  n  ×  scale_factor  ⌋  D_{out} = \\\\left\\\\lfloor D_{in} \\\\times \\\\text{scale\\\\_factor} \\\\right\\\\rfloorDout\\u200b=⌊Din\\u200b×scale_factor⌋  H  o  u  t  =  ⌊  H  i  n  ×  scale_factor  ⌋  H_{out} = \\\\left\\\\lfloor H_{in} \\\\times \\\\text{scale\\\\_factor} \\\\right\\\\rfloorHout\\u200b=⌊Hin\\u200b×scale_factor⌋  W  o  u  t  =  ⌊  W  i  n  ×  scale_factor  ⌋  W_{out} = \\\\left\\\\lfloor W_{in} \\\\times \\\\text{scale\\\\_factor} \\\\right\\\\rfloorWout\\u200b=⌊Win\\u200b×scale_factor⌋  Warning  With align_corners=True , the linearly interpolating modes\\n( linear , bilinear , bicubic , and trilinear ) don’t proportionally\\nalign the output and input pixels, and thus the output values can depend\\non the input size. This was the default behavior for these modes up to\\nversion 0.3.1. Since then, the default behavior is align_corners=False . See below for concrete examples on how this\\naffects the outputs.  Note  If you want downsampling/general resizing, you should use interpolate() .  Examples:  >>>input=torch.arange(1,5,dtype=torch.float32).view(1,1,2,2)>>>inputtensor([[[[1., 2.],[3., 4.]]]])>>>m=nn.Upsample(scale_factor=2,mode=\\'nearest\\')>>>m(input)tensor([[[[1., 1., 2., 2.],[1., 1., 2., 2.],[3., 3., 4., 4.],[3., 3., 4., 4.]]]])>>>m=nn.Upsample(scale_factor=2,mode=\\'bilinear\\')# align_corners=False>>>m(input)tensor([[[[1.0000, 1.2500, 1.7500, 2.0000],[1.5000, 1.7500, 2.2500, 2.5000],[2.5000, 2.7500, 3.2500, 3.5000],[3.0000, 3.2500, 3.7500, 4.0000]]]])>>>m=nn.Upsample(scale_factor=2,mode=\\'bilinear\\',align_corners=True)>>>m(input)tensor([[[[1.0000, 1.3333, 1.6667, 2.0000],[1.6667, 2.0000, 2.3333, 2.6667],[2.3333, 2.6667, 3.0000, 3.3333],[3.0000, 3.3333, 3.6667, 4.0000]]]])>>># Try scaling the same data in a larger tensor>>>input_3x3=torch.zeros(3,3).view(1,1,3,3)>>>input_3x3[:,:,:2,:2].copy_(input)tensor([[[[1., 2.],[3., 4.]]]])>>>input_3x3tensor([[[[1., 2., 0.],[3., 4., 0.],[0., 0., 0.]]]])>>>m=nn.Upsample(scale_factor=2,mode=\\'bilinear\\')# align_corners=False>>># Notice that values in top left corner are the same with the small input (except at boundary)>>>m(input_3x3)tensor([[[[1.0000, 1.2500, 1.7500, 1.5000, 0.5000, 0.0000],[1.5000, 1.7500, 2.2500, 1.8750, 0.6250, 0.0000],[2.5000, 2.7500, 3.2500, 2.6250, 0.8750, 0.0000],[2.2500, 2.4375, 2.8125, 2.2500, 0.7500, 0.0000],[0.7500, 0.8125, 0.9375, 0.7500, 0.2500, 0.0000],[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])>>>m=nn.Upsample(scale_factor=2,mode=\\'bilinear\\',align_corners=True)>>># Notice that values in top left corner are now changed>>>m(input_3x3)tensor([[[[1.0000, 1.4000, 1.8000, 1.6000, 0.8000, 0.0000],[1.8000, 2.2000, 2.6000, 2.2400, 1.1200, 0.0000],[2.6000, 3.0000, 3.4000, 2.8800, 1.4400, 0.0000],[2.4000, 2.7200, 3.0400, 2.5600, 1.2800, 0.0000],[1.2000, 1.3600, 1.5200, 1.2800, 0.6400, 0.0000],[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html#upsample\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html#torch.nn.Upsample', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html#upsample', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#Upsample', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/upsampling.py#L14', 'https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html#torch.nn.Upsample', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a24'), 'title': 'nn.UpsamplingNearest2d', 'page_text': 'UpsamplingNearest2d [LINK_1]  class torch.nn.UpsamplingNearest2d( size=None , scale_factor=None ) [source]  [source]  [LINK_2]  Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.  To specify the scale, it takes either the size or the scale_factor as it’s constructor argument.  When size is given, it is the output size of the image (h, w) .  Parameters  size ( int  or  Tuple  [  int  ,  int  ]  ,  optional ) – output spatial sizes  scale_factor ( float  or  Tuple  [  float  ,  float  ]  ,  optional ) – multiplier for\\nspatial size.  Warning  This class is deprecated in favor of interpolate() .  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)where  H  o  u  t  =  ⌊  H  i  n  ×  scale_factor  ⌋  H_{out} = \\\\left\\\\lfloor H_{in} \\\\times \\\\text{scale\\\\_factor} \\\\right\\\\rfloorHout\\u200b=⌊Hin\\u200b×scale_factor⌋  W  o  u  t  =  ⌊  W  i  n  ×  scale_factor  ⌋  W_{out} = \\\\left\\\\lfloor W_{in} \\\\times \\\\text{scale\\\\_factor} \\\\right\\\\rfloorWout\\u200b=⌊Win\\u200b×scale_factor⌋  Examples:  >>>input=torch.arange(1,5,dtype=torch.float32).view(1,1,2,2)>>>inputtensor([[[[1., 2.],[3., 4.]]]])>>>m=nn.UpsamplingNearest2d(scale_factor=2)>>>m(input)tensor([[[[1., 1., 2., 2.],[1., 1., 2., 2.],[3., 3., 4., 4.],[3., 3., 4., 4.]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html#upsamplingnearest2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html#upsamplingnearest2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/upsampling.py#L196', 'https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a25'), 'title': 'nn.UpsamplingBilinear2d', 'page_text': 'UpsamplingBilinear2d [LINK_1]  class torch.nn.UpsamplingBilinear2d( size=None , scale_factor=None ) [source]  [source]  [LINK_2]  Applies a 2D bilinear upsampling to an input signal composed of several input channels.  To specify the scale, it takes either the size or the scale_factor as it’s constructor argument.  When size is given, it is the output size of the image (h, w) .  Parameters  size ( int  or  Tuple  [  int  ,  int  ]  ,  optional ) – output spatial sizes  scale_factor ( float  or  Tuple  [  float  ,  float  ]  ,  optional ) – multiplier for\\nspatial size.  Warning  This class is deprecated in favor of interpolate() . It is\\nequivalent to nn.functional.interpolate(...,mode=\\'bilinear\\',align_corners=True) .  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)where  H  o  u  t  =  ⌊  H  i  n  ×  scale_factor  ⌋  H_{out} = \\\\left\\\\lfloor H_{in} \\\\times \\\\text{scale\\\\_factor} \\\\right\\\\rfloorHout\\u200b=⌊Hin\\u200b×scale_factor⌋  W  o  u  t  =  ⌊  W  i  n  ×  scale_factor  ⌋  W_{out} = \\\\left\\\\lfloor W_{in} \\\\times \\\\text{scale\\\\_factor} \\\\right\\\\rfloorWout\\u200b=⌊Win\\u200b×scale_factor⌋  Examples:  >>>input=torch.arange(1,5,dtype=torch.float32).view(1,1,2,2)>>>inputtensor([[[[1., 2.],[3., 4.]]]])>>>m=nn.UpsamplingBilinear2d(scale_factor=2)>>>m(input)tensor([[[[1.0000, 1.3333, 1.6667, 2.0000],[1.6667, 2.0000, 2.3333, 2.6667],[2.3333, 2.6667, 3.0000, 3.3333],[3.0000, 3.3333, 3.6667, 4.0000]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html#upsamplingbilinear2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html#upsamplingbilinear2d', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/upsampling.py#L245', 'https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.ChannelShuffle.html', 'https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a26'), 'title': 'nn.ChannelShuffle', 'page_text': 'ChannelShuffle [LINK_1]  class torch.nn.ChannelShuffle( groups ) [source]  [source]  [LINK_2]  Divides and rearranges the channels in a tensor.  This operation divides the channels in a tensor of shape(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)into g groups as(  N  ,  C  g  ,  g  ,  ∗  )  (N, \\\\frac{C}{g}, g, *)(N,gC\\u200b,g,∗)and shuffles them,\\nwhile retaining the original tensor shape in the final output.  Parameters  groups ( int ) – number of groups to divide channels in.  Examples:  >>>channel_shuffle=nn.ChannelShuffle(2)>>>input=torch.arange(1,17,dtype=torch.float32).view(1,4,2,2)>>>inputtensor([[[[ 1.,  2.],[ 3.,  4.]],[[ 5.,  6.],[ 7.,  8.]],[[ 9., 10.],[11., 12.]],[[13., 14.],[15., 16.]]]])>>>output=channel_shuffle(input)>>>outputtensor([[[[ 1.,  2.],[ 3.,  4.]],[[ 9., 10.],[11., 12.]],[[ 5.,  6.],[ 7.,  8.]],[[13., 14.],[15., 16.]]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ChannelShuffle.html#channelshuffle\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.ChannelShuffle.html#channelshuffle', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/channelshuffle.html#ChannelShuffle', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/channelshuffle.py#L10', 'https://pytorch.org/docs/stable/generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html', 'https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a27'), 'title': 'nn.DataParallel', 'page_text': 'DataParallel [LINK_1]  class torch.nn.DataParallel( module , device_ids=None , output_device=None , dim=0 ) [source]  [source]  [LINK_2]  Implements data parallelism at the module level.  This container parallelizes the application of the given module by\\nsplitting the input across the specified devices by chunking in the batch\\ndimension (other objects will be copied once per device). In the forward\\npass, the module is replicated on each device, and each replica handles a\\nportion of the input. During the backwards pass, gradients from each replica\\nare summed into the original module.  The batch size should be larger than the number of GPUs used.  Warning  It is recommended to use DistributedDataParallel ,\\ninstead of this class, to do multi-GPU training, even if there is only a single\\nnode. See: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel and Distributed Data Parallel .  Arbitrary positional and keyword inputs are allowed to be passed into\\nDataParallel but some types are specially handled. tensors will be scattered on dim specified (default 0). tuple, list and dict types will\\nbe shallow copied. The other types will be shared among different threads\\nand can be corrupted if written to in the model’s forward pass.  The parallelized module must have its parameters and buffers on device_ids[0] before running this DataParallel module.  Warning  In each forward, module is replicated on each device, so any\\nupdates to the running module in forward will be lost. For example,\\nif module has a counter attribute that is incremented in each forward , it will always stay at the initial value because the update\\nis done on the replicas which are destroyed after forward . However, DataParallel guarantees that the replica on device[0] will have its parameters and buffers sharing storage with\\nthe base parallelized module . So in-place updates to the\\nparameters or buffers on device[0] will be recorded. E.g., BatchNorm2d and spectral_norm() rely on this behavior to update the buffers.  Warning  Forward and backward hooks defined on module and its submodules\\nwill be invoked len(device_ids) times, each with inputs located on\\na particular device. Particularly, the hooks are only guaranteed to be\\nexecuted in correct order with respect to operations on corresponding\\ndevices. For example, it is not guaranteed that hooks set via register_forward_pre_hook() be executed before all  len(device_ids)  forward() calls, but\\nthat each such hook be executed before the corresponding forward() call of that device.  Warning  When module returns a scalar (i.e., 0-dimensional tensor) in forward() , this wrapper will return a vector of length equal to\\nnumber of devices used in data parallelism, containing the result from\\neach device.  Note  There is a subtlety in using the packsequence->recurrentnetwork->unpacksequence pattern in a Module wrapped in DataParallel .\\nSee My recurrent network doesn’t work with data parallelism section in FAQ for\\ndetails.  Parameters  module ( Module ) – module to be parallelized  device_ids ( list  of  int  or  torch.device ) – CUDA devices (default: all devices)  output_device ( int  or  torch.device ) – device location of output (default: device_ids[0])  Variables  module ( Module ) – the module to be parallelized  Example:  >>>net=torch.nn.DataParallel(model,device_ids=[0,1,2])>>>output=net(input_var)# input_var can be on any device, including CPU\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#dataparallel\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#dataparallel', 'https://pytorch.org/docs/stable/_modules/torch/nn/parallel/data_parallel.html#DataParallel', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parallel/data_parallel.py#L52', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel', 'https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel', 'https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead', 'https://pytorch.org/docs/stable/notes/ddp.html#ddp', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel', 'https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html#torch.nn.utils.spectral_norm', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel', 'https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html', 'https://pytorch.org/docs/stable/generated/torch.nn.ChannelShuffle.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a28'), 'title': 'nn.parallel.DistributedDataParallel', 'page_text': 'DistributedDataParallel [LINK_1]  class torch.nn.parallel.DistributedDataParallel( module , device_ids=None , output_device=None , dim=0 , broadcast_buffers=True , process_group=None , bucket_cap_mb=None , find_unused_parameters=False , check_reduction=False , gradient_as_bucket_view=False , static_graph=False , delay_all_reduce_named_params=None , param_to_hook_all_reduce=None , mixed_precision=None , device_mesh=None ) [source]  [source]  [LINK_2]  Implement distributed data parallelism based on torch.distributed at module level.  This container provides data parallelism by synchronizing gradients\\nacross each model replica. The devices to synchronize across are\\nspecified by the input process_group , which is the entire world\\nby default. Note that DistributedDataParallel does not chunk or\\notherwise shard the input across participating GPUs; the user is\\nresponsible for defining how to do so, for example through the use\\nof a DistributedSampler .  See also: Basics and Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel .\\nThe same constraints on input as in torch.nn.DataParallel apply.  Creation of this class requires that torch.distributed to be already\\ninitialized, by calling torch.distributed.init_process_group() .  DistributedDataParallel is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data\\nparallel training.  To use DistributedDataParallel on a host with N GPUs, you should spawn\\nup N processes, ensuring that each process exclusively works on a single\\nGPU from 0 to N-1. This can be done by either setting CUDA_VISIBLE_DEVICES for every process or by calling:  >>>torch.cuda.set_device(i)  where i is from 0 to N-1. In each process, you should refer the following\\nto construct this module:  >>>torch.distributed.init_process_group(>>>backend=\\'nccl\\',world_size=N,init_method=\\'...\\'>>>)>>>model=DistributedDataParallel(model,device_ids=[i],output_device=i)  In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn .  Note  Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.  Note  DistributedDataParallel can be used in conjunction with torch.distributed.optim.ZeroRedundancyOptimizer to reduce\\nper-rank optimizer states memory footprint. Please refer to ZeroRedundancyOptimizer recipe for more details.  Note  nccl backend is currently the fastest and highly recommended\\nbackend when using GPUs. This applies to both single-node and\\nmulti-node distributed training.  Note  This module also supports mixed-precision distributed training.\\nThis means that your model can have different types of parameters such\\nas mixed types of fp16 and fp32 , the gradient reduction on these\\nmixed types of parameters will just work fine.  Note  If you use torch.save on one process to checkpoint the module,\\nand torch.load on some other processes to recover it, make sure that map_location is configured properly for every process. Without map_location , torch.load would recover the module to devices\\nwhere the module was saved from.  Note  When a model is trained on M nodes with batch=N , the\\ngradient will be M times smaller when compared to the same model\\ntrained on a single node with batch=M*N if the loss is summed (NOT\\naveraged as usual) across instances in a batch (because the gradients\\nbetween different nodes are averaged). You should take this into\\nconsideration when you want to obtain a mathematically equivalent\\ntraining process compared to the local training counterpart. But in most\\ncases, you can just treat a DistributedDataParallel wrapped model, a\\nDataParallel wrapped model and an ordinary model on a single GPU as the\\nsame (E.g. using the same learning rate for equivalent batch size).  Note  Parameters are never broadcast between processes. The module performs\\nan all-reduce step on gradients and assumes that they will be modified\\nby the optimizer in all processes in the same way. Buffers\\n(e.g. BatchNorm stats) are broadcast from the module in process of rank\\n0, to all other replicas in the system in every iteration.  Note  If you are using DistributedDataParallel in conjunction with the Distributed RPC Framework , you should always use torch.distributed.autograd.backward() to compute gradients and torch.distributed.optim.DistributedOptimizer for optimizing\\nparameters.  Example:  >>>importtorch.distributed.autogradasdist_autograd>>>fromtorch.nn.parallelimportDistributedDataParallelasDDP>>>importtorch>>>fromtorchimportoptim>>>fromtorch.distributed.optimimportDistributedOptimizer>>>importtorch.distributed.rpcasrpc>>>fromtorch.distributed.rpcimportRRef>>>>>>t1=torch.rand((3,3),requires_grad=True)>>>t2=torch.rand((3,3),requires_grad=True)>>>rref=rpc.remote(\"worker1\",torch.add,args=(t1,t2))>>>ddp_model=DDP(my_model)>>>>>># Setup optimizer>>>optimizer_params=[rref]>>>forparaminddp_model.parameters():>>>optimizer_params.append(RRef(param))>>>>>>dist_optim=DistributedOptimizer(>>>optim.SGD,>>>optimizer_params,>>>lr=0.05,>>>)>>>>>>withdist_autograd.context()ascontext_id:>>>pred=ddp_model(rref.to_here())>>>loss=loss_func(pred,target)>>>dist_autograd.backward(context_id,[loss])>>>dist_optim.step(context_id)  Note  DistributedDataParallel currently offers limited support for gradient\\ncheckpointing with torch.utils.checkpoint() .\\nIf the checkpoint is done with use_reentrant=False (recommended), DDP\\nwill work as expected without any limitations.\\nIf, however, the checkpoint is done with use_reentrant=True (the default),\\nDDP will work as expected when there are no unused parameters in the model\\nand each layer is checkpointed at most once (make sure you are not passing find_unused_parameters=True to DDP). We currently do not support the\\ncase where a layer is checkpointed multiple times, or when there unused\\nparameters in the checkpointed model.  Note  To let a non-DDP model load a state dict from a DDP model, consume_prefix_in_state_dict_if_present() needs to be applied to strip the prefix “module.” in the DDP state dict before loading.  Warning  Constructor, forward method, and differentiation of the output (or a\\nfunction of the output of this module) are distributed synchronization\\npoints. Take that into account in case different processes might be\\nexecuting different code.  Warning  This module assumes all parameters are registered in the model by the\\ntime it is created. No parameters should be added nor removed later.\\nSame applies to buffers.  Warning  This module assumes all parameters are registered in the model of each\\ndistributed processes are in the same order. The module itself will\\nconduct gradient allreduce following the reverse order of the\\nregistered parameters of the model. In other words, it is users’\\nresponsibility to ensure that each distributed process has the exact\\nsame model and thus the exact same parameter registration order.  Warning  This module allows parameters with non-rowmajor-contiguous strides.\\nFor example, your model may contain some parameters whose torch.memory_format is torch.contiguous_format and others whose format is torch.channels_last .  However,\\ncorresponding parameters in different processes must have the\\nsame strides.  Warning  This module doesn’t work with torch.autograd.grad() (i.e. it will\\nonly work if gradients are to be accumulated in .grad attributes of\\nparameters).  Warning  If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses\\nmultiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn . Unfortunately\\nGloo (that uses Infiniband) and NCCL2 are not fork safe, and you will\\nlikely experience deadlocks if you don’t change this setting.  Warning  You should never try to change your model’s parameters after wrapping\\nup your model with DistributedDataParallel . Because, when\\nwrapping up your model with DistributedDataParallel , the constructor\\nof DistributedDataParallel will register the additional gradient\\nreduction functions on all the parameters of the model itself at the\\ntime of construction. If you change the model’s parameters afterwards,\\ngradient reduction functions no longer match the correct set of\\nparameters.  Warning  Using DistributedDataParallel in conjunction with the Distributed RPC Framework is experimental and subject to change.  Parameters  module ( Module ) – module to be parallelized  device_ids ( list  of  int  or  torch.device ) – CUDA devices.\\n1) For single-device modules, device_ids can\\ncontain exactly one device id, which represents the only\\nCUDA device where the input module corresponding to this process resides.\\nAlternatively, device_ids can also be None .\\n2) For multi-device modules and CPU modules, device_ids must be None .  When device_ids is None for both cases,\\nboth the input data for the forward pass and the actual module\\nmust be placed on the correct device.\\n(default: None )  output_device ( int  or  torch.device ) – Device location of output for\\nsingle-device CUDA modules. For multi-device modules and\\nCPU modules, it must be None , and the module itself\\ndictates the output location. (default: device_ids[0] for single-device modules)  broadcast_buffers ( bool ) – Flag that enables syncing (broadcasting)\\nbuffers of the module at beginning of the forward function. (default: True )  process_group – The process group to be used for distributed data\\nall-reduction. If None , the default process group, which\\nis created by torch.distributed.init_process_group() ,\\nwill be used. (default: None )  bucket_cap_mb – DistributedDataParallel will bucket parameters into\\nmultiple buckets so that gradient reduction of each\\nbucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in\\nMebiBytes (MiB). If None , a default size of 25 MiB\\nwill be used. (default: None )  find_unused_parameters ( bool ) – Traverse the autograd graph from all\\ntensors contained in the return value of the\\nwrapped module’s forward function. Parameters\\nthat don’t receive gradients as part of this\\ngraph are preemptively marked as being ready to\\nbe reduced. In addition, parameters that may have\\nbeen used in the wrapped module’s forward function but were not part of loss computation and\\nthus would also not receive gradients are\\npreemptively marked as ready to be reduced.\\n(default: False )  check_reduction – This argument is deprecated.  gradient_as_bucket_view ( bool ) – When set to True , gradients will be views\\npointing to different offsets of allreduce communication\\nbuckets. This can reduce peak memory usage, where the\\nsaved memory size will be equal to the total gradients\\nsize. Moreover, it avoids the overhead of copying between\\ngradients and allreduce communication buckets. When\\ngradients are views, detach_() cannot be called on the\\ngradients. If hitting such errors, please fix it by\\nreferring to the zero_grad() function in torch/optim/optimizer.py as a solution.\\nNote that gradients will be views after first iteration, so\\nthe peak memory saving should be checked after first iteration.  static_graph ( bool ) – When set to True , DDP knows the trained graph is\\nstatic. Static graph means 1) The set of used and unused\\nparameters will not change during the whole training loop; in\\nthis case, it does not matter whether users set find_unused_parameters=True or not. 2) How the graph is trained\\nwill not change during the whole training loop (meaning there is\\nno control flow depending on iterations).\\nWhen static_graph is set to be True , DDP will support cases that\\ncan not be supported in the past:\\n1) Reentrant backwards.\\n2) Activation checkpointing multiple times.\\n3) Activation checkpointing when model has unused parameters.\\n4) There are model parameters that are outside of forward function.\\n5) Potentially improve performance when there are unused parameters,\\nas DDP will not search graph in each iteration to detect unused\\nparameters when static_graph is set to be True .\\nTo check whether you can set static_graph to be True , one way is to\\ncheck ddp logging data at the end of your previous model training,\\nif ddp_logging_data.get(\"can_set_static_graph\")==True , mostly you\\ncan set static_graph=True as well.  Example::  >>>model_DDP=torch.nn.parallel.DistributedDataParallel(model)>>># Training loop>>>...>>>ddp_logging_data=model_DDP._get_ddp_logging_data()>>>static_graph=ddp_logging_data.get(\"can_set_static_graph\")  delay_all_reduce_named_params ( list  of  tuple  of  str and torch.nn.Parameter ) – a list\\nof named parameters whose all reduce will be delayed when the gradient of\\nthe parameter specified in param_to_hook_all_reduce is ready. Other\\narguments of DDP do not apply to named params specified in this argument\\nas these named params will be ignored by DDP reducer.  param_to_hook_all_reduce ( torch.nn.Parameter ) – a parameter to hook delayed all reduce\\nof parameters specified in delay_all_reduce_named_params .  Variables  module ( Module ) – the module to be parallelized.  Example:  >>>torch.distributed.init_process_group(backend=\\'nccl\\',world_size=4,init_method=\\'...\\')>>>net=torch.nn.parallel.DistributedDataParallel(model)  join( divide_by_initial_world_size=True , enable=True , throw_on_early_termination=False ) [source]  [source]  [LINK_3]  Context manager for training with uneven inputs across processes in DDP.  This context manager will keep track of already-joined DDP processes,\\nand “shadow” the forward and backward passes by inserting collective\\ncommunication operations to match with the ones created by non-joined\\nDDP processes. This will ensure each collective call has a corresponding\\ncall by already-joined DDP processes, preventing hangs or errors that\\nwould otherwise happen when training with uneven inputs across\\nprocesses. Alternatively, if the flag throw_on_early_termination is\\nspecified to be True , all trainers will throw an error once one rank\\nruns out of inputs, allowing these errors to be caught and handled\\naccording to application logic.  Once all DDP processes have joined, the context manager will broadcast\\nthe model corresponding to the last joined process to all processes to\\nensure the model is the same across all processes\\n(which is guaranteed by DDP).  To use this to enable training with uneven inputs across processes,\\nsimply wrap this context manager around your training loop. No further\\nmodifications to the model or data loading is required.  Warning  If the model or training loop this context manager is wrapped around\\nhas additional distributed collective operations, such as SyncBatchNorm in the model’s forward pass, then the flag throw_on_early_termination must be enabled. This is because this\\ncontext manager is not aware of non-DDP collective communication.\\nThis flag will cause all ranks to throw when any one rank\\nexhausts inputs, allowing these errors to be caught and recovered\\nfrom across all ranks.  Parameters  divide_by_initial_world_size ( bool ) – If True , will divide\\ngradients by the initial world_size DDP training was launched\\nwith. If False , will compute the effective world size\\n(number of ranks that have not depleted their inputs yet) and\\ndivide gradients by that during allreduce. Set divide_by_initial_world_size=True to ensure every input\\nsample including the uneven inputs have equal weight in terms of\\nhow much they contribute to the global gradient. This is\\nachieved by always dividing the gradient by the initial world_size even when we encounter uneven inputs. If you set\\nthis to False , we divide the gradient by the remaining\\nnumber of nodes. This ensures parity with training on a smaller world_size although it also means the uneven inputs would\\ncontribute more towards the global gradient. Typically, you\\nwould want to set this to True for cases where the last few\\ninputs of your training job are uneven. In extreme cases, where\\nthere is a large discrepancy in the number of inputs, setting\\nthis to False might provide better results.  enable ( bool ) – Whether to enable uneven input detection or not. Pass\\nin enable=False to disable in cases where you know that\\ninputs are even across participating processes. Default is True .  throw_on_early_termination ( bool ) – Whether to throw an error\\nor continue training when at least one rank has exhausted\\ninputs. If True , will throw upon the first rank reaching end\\nof data. If False , will continue training with a smaller\\neffective world size until all ranks are joined. Note that if\\nthis flag is specified, then the flag divide_by_initial_world_size would be ignored. Default\\nis False .  Example:  >>>importtorch>>>importtorch.distributedasdist>>>importos>>>importtorch.multiprocessingasmp>>>importtorch.nnasnn>>># On each spawned worker>>>defworker(rank):>>>dist.init_process_group(\"nccl\",rank=rank,world_size=2)>>>torch.cuda.set_device(rank)>>>model=nn.Linear(1,1,bias=False).to(rank)>>>model=torch.nn.parallel.DistributedDataParallel(>>>model,device_ids=[rank],output_device=rank>>>)>>># Rank 1 gets one more input than rank 0.>>>inputs=[torch.tensor([1]).float()for_inrange(10+rank)]>>>withmodel.join():>>>for_inrange(5):>>>forinpininputs:>>>loss=model(inp).sum()>>>loss.backward()>>># Without the join() API, the below synchronization will hang>>># blocking for rank 1\\'s allreduce to complete.>>>torch.cuda.synchronize(device=rank)  join_hook( **kwargs ) [source]  [source]  [LINK_4]  DDP join hook enables training on uneven inputs by mirroring communications in forward and backward passes.  Parameters  kwargs ( dict ) – a dict containing any keyword arguments\\nto modify the behavior of the join hook at run time; all Joinable instances sharing the same join context\\nmanager are forwarded the same value for kwargs .  The hook supports the following keyword arguments:  divide_by_initial_world_size (bool, optional):  If True , then gradients are divided by the initial world\\nsize that DDP was launched with.\\nIf False , then gradients are divided by the effective world\\nsize (i.e. the number of non-joined processes), meaning that\\nthe uneven inputs contribute more toward the global gradient.\\nTypically, this should be set to True if the degree of\\nunevenness is small but can be set to False in extreme\\ncases for possibly better results.\\nDefault is True .  no_sync() [source]  [source]  [LINK_5]  Context manager to disable gradient synchronizations across DDP processes.  Within this context, gradients will be accumulated on module\\nvariables, which will later be synchronized in the first\\nforward-backward pass exiting the context.  Example:  >>>ddp=torch.nn.parallel.DistributedDataParallel(model,pg)>>>withddp.no_sync():>>>forinputininputs:>>>ddp(input).backward()# no synchronization, accumulate grads>>>ddp(another_input).backward()# synchronize grads  Warning  The forward pass should be included inside the context manager, or\\nelse gradients will still be synchronized.  register_comm_hook( state , hook ) [source]  [source]  [LINK_6]  Register communication hook for user-defined DDP aggregation of gradients across multiple workers.  This hook would be very useful for researchers to try out new ideas. For\\nexample, this hook can be used to implement several algorithms like GossipGrad\\nand gradient compression which involve different communication strategies for\\nparameter syncs while running Distributed DataParallel training.  Parameters  state ( object ) – Passed to the hook to maintain any state information during the training process.\\nExamples include error feedback in gradient compression,\\npeers to communicate with next in GossipGrad, etc.  It is locally stored by each worker\\nand shared by all the gradient tensors on the worker.  hook ( Callable ) – Callable with the following signature: hook(state:object,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor] :  This function is called once the bucket is ready. The\\nhook can perform whatever processing is needed and return\\na Future indicating completion of any async work (ex: allreduce).\\nIf the hook doesn’t perform any communication, it still\\nmust return a completed Future. The Future should hold the\\nnew value of grad bucket’s tensors. Once a bucket is ready,\\nc10d reducer would call this hook and use the tensors returned\\nby the Future and copy grads to individual parameters.\\nNote that the future’s return type must be a single tensor.  We also provide an API called get_future to retrieve a\\nFuture associated with the completion of c10d.ProcessGroup.Work . get_future is currently supported for NCCL and also supported for most\\noperations on GLOO and MPI, except for peer to peer operations (send/recv).  Warning  Grad bucket’s tensors will not be predivided by world_size. User is responsible\\nto divide by the world_size in case of operations like allreduce.  Warning  DDP communication hook can only be registered once and should be registered\\nbefore calling backward.  Warning  The Future object that hook returns should contain a single tensor\\nthat has the same shape with the tensors inside grad bucket.  Warning  get_future API supports NCCL, and partially GLOO and MPI backends (no support\\nfor peer-to-peer operations like send/recv) and will return a torch.futures.Future .  Example::  Below is an example of a noop hook that returns the same tensor.  >>>defnoop(state:object,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]:>>>fut=torch.futures.Future()>>>fut.set_result(bucket.buffer())>>>returnfut>>>ddp.register_comm_hook(state=None,hook=noop)  Example::  Below is an example of a Parallel SGD algorithm where gradients are encoded before\\nallreduce, and then decoded after allreduce.  >>>defencode_and_decode(state:object,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]:>>>encoded_tensor=encode(bucket.buffer())# encode gradients>>>fut=torch.distributed.all_reduce(encoded_tensor).get_future()>>># Define the then callback to decode.>>>defdecode(fut):>>>decoded_tensor=decode(fut.value()[0])# decode gradients>>>returndecoded_tensor>>>returnfut.then(decode)>>>ddp.register_comm_hook(state=None,hook=encode_and_decode)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#distributeddataparallel\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join_hook\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.register_comm_hook', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#distributeddataparallel', 'https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parallel/distributed.py#L326', 'https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel', 'https://pytorch.org/docs/stable/distributed.html#distributed-basics', 'https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel', 'https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel', 'https://pytorch.org/tutorials/beginner/dist_overview.html', 'https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer', 'https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html', 'https://pytorch.org/docs/stable/rpc.html#distributed-rpc-framework', 'https://pytorch.org/docs/stable/rpc.html#torch.distributed.autograd.backward', 'https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.DistributedOptimizer', 'https://pytorch.org/docs/stable/utils.html#module-torch.utils.checkpoint', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format', 'https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad', 'https://pytorch.org/docs/stable/rpc.html#distributed-rpc-framework', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.join', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parallel/distributed.py#L1742', 'https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.join_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parallel/distributed.py#L1848', 'https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join_hook', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.no_sync', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parallel/distributed.py#L1408', 'https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync', 'https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.register_comm_hook', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/parallel/distributed.py#L1930', 'https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.register_comm_hook', 'https://docs.python.org/3/library/functions.html#object', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a29'), 'title': 'clip_grad_norm_', 'page_text': 'torch.nn.utils.clip_grad_norm_ [LINK_1]  torch.nn.utils.clip_grad_norm_( parameters , max_norm , norm_type=2.0 , error_if_nonfinite=False , foreach=None ) [source]  [source]  [LINK_2]  Clip the gradient norm of an iterable of parameters.  The norm is computed over the norms of the individual gradients of all parameters,\\nas if the norms of the individual gradients were concatenated into a single vector.\\nGradients are modified in-place.  This function is equivalent to torch.nn.utils.get_total_norm() followed by torch.nn.utils.clip_grads_with_norm_() with the total_norm returned by get_total_norm .  Parameters  parameters ( Iterable  [  Tensor  ] or  Tensor ) – an iterable of Tensors or a\\nsingle Tensor that will have gradients normalized  max_norm ( float ) – max norm of the gradients  norm_type ( float ) – type of the used p-norm. Can be \\'inf\\' for\\ninfinity norm.  error_if_nonfinite ( bool ) – if True, an error is thrown if the total\\nnorm of the gradients from parameters is nan , inf , or -inf . Default: False (will switch to True in the future)  foreach ( bool ) – use the faster foreach-based implementation.\\nIf None , use the foreach implementation for CUDA and CPU native tensors and silently\\nfall back to the slow implementation for other device types.\\nDefault: None  Returns  Total norm of the parameter gradients (viewed as a single vector).  Return type  Tensor\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/clip_grad.py#L175', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.get_total_norm.html#torch.nn.utils.get_total_norm', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grads_with_norm_.html#torch.nn.utils.clip_grads_with_norm_', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a2a'), 'title': 'clip_grad_norm', 'page_text': 'torch.nn.utils.clip_grad_norm [LINK_1]  torch.nn.utils.clip_grad_norm( parameters , max_norm , norm_type=2.0 , error_if_nonfinite=False , foreach=None ) [source]  [source]  [LINK_2]  Clip the gradient norm of an iterable of parameters.  Warning  This method is now deprecated in favor of torch.nn.utils.clip_grad_norm_() .  Return type  Tensor\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm.html#torch-nn-utils-clip-grad-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm.html#torch.nn.utils.clip_grad_norm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm.html#torch-nn-utils-clip-grad-norm', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/clip_grad.py#L220', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm.html#torch.nn.utils.clip_grad_norm', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a2b'), 'title': 'clip_grad_value_', 'page_text': 'torch.nn.utils.clip_grad_value_ [LINK_1]  torch.nn.utils.clip_grad_value_( parameters , clip_value , foreach=None ) [source]  [source]  [LINK_2]  Clip the gradients of an iterable of parameters at specified value.  Gradients are modified in-place.  Parameters  parameters ( Iterable  [  Tensor  ] or  Tensor ) – an iterable of Tensors or a\\nsingle Tensor that will have gradients normalized  clip_value ( float ) – maximum allowed value of the gradients.\\nThe gradients are clipped in the range[  -clip_value  ,  clip_value  ]  \\\\left[\\\\text{-clip\\\\_value}, \\\\text{clip\\\\_value}\\\\right][-clip_value,clip_value]  foreach ( bool ) – use the faster foreach-based implementation\\nIf None , use the foreach implementation for CUDA and CPU native tensors and\\nsilently fall back to the slow implementation for other device types.\\nDefault: None\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch-nn-utils-clip-grad-value\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch-nn-utils-clip-grad-value', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_value_', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/clip_grad.py#L241', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.get_total_norm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a2c'), 'title': 'get_total_norm', 'page_text': 'torch.nn.utils.get_total_norm [LINK_1]  torch.nn.utils.get_total_norm( tensors , norm_type=2.0 , error_if_nonfinite=False , foreach=None ) [source]  [LINK_2]  Compute the norm of an iterable of tensors.  The norm is computed over the norms of the individual tensors, as if the norms of\\nthe individual tensors were concatenated into a single vector.  Parameters  tensors ( Iterable  [  Tensor  ] or  Tensor ) – an iterable of Tensors or a\\nsingle Tensor that will be normalized  norm_type ( float ) – type of the used p-norm. Can be \\'inf\\' for\\ninfinity norm.  error_if_nonfinite ( bool ) – if True, an error is thrown if the total\\nnorm of tensors is nan , inf , or -inf .\\nDefault: False  foreach ( bool ) – use the faster foreach-based implementation.\\nIf None , use the foreach implementation for CUDA and CPU native tensors and silently\\nfall back to the slow implementation for other device types.\\nDefault: None  Returns  Total norm of the tensors (viewed as a single vector).  Return type  Tensor\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.get_total_norm.html#torch-nn-utils-get-total-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.get_total_norm.html#torch.nn.utils.get_total_norm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.get_total_norm.html#torch-nn-utils-get-total-norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/clip_grad.py#L40', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.get_total_norm.html#torch.nn.utils.get_total_norm', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grads_with_norm_.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a2d'), 'title': 'clip_grads_with_norm_', 'page_text': 'torch.nn.utils.clip_grads_with_norm_ [LINK_1]  torch.nn.utils.clip_grads_with_norm_( parameters , max_norm , total_norm , foreach=None ) [source]  [LINK_2]  Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm.  The gradients will be scaled by the following calculation  g  r  a  d  =  g  r  a  d  ∗  m  a  x  _  n  o  r  m  t  o  t  a  l  _  n  o  r  m  +  1  e  −  6  grad = grad * \\\\frac{max\\\\_norm}{total\\\\_norm + 1e-6}grad=grad∗total_norm+1e−6max_norm\\u200b  Gradients are modified in-place.  This function is equivalent to torch.nn.utils.clip_grad_norm_() with a pre-calculated\\ntotal norm.  Parameters  parameters ( Iterable  [  Tensor  ] or  Tensor ) – an iterable of Tensors or a\\nsingle Tensor that will have gradients normalized  max_norm ( float ) – max norm of the gradients  total_norm ( Tensor ) – total norm of the gradients to use for clipping  foreach ( bool ) – use the faster foreach-based implementation.\\nIf None , use the foreach implementation for CUDA and CPU native tensors and silently\\nfall back to the slow implementation for other device types.\\nDefault: None  Returns  None  Return type  None\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grads_with_norm_.html#torch-nn-utils-clip-grads-with-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grads_with_norm_.html#torch.nn.utils.clip_grads_with_norm_', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grads_with_norm_.html#torch-nn-utils-clip-grads-with-norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/clip_grad.py#L111', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grads_with_norm_.html#torch.nn.utils.clip_grads_with_norm_', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parameters_to_vector.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.get_total_norm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a2e'), 'title': 'parameters_to_vector', 'page_text': 'torch.nn.utils.parameters_to_vector [LINK_1]  torch.nn.utils.parameters_to_vector( parameters ) [source]  [source]  [LINK_2]  Flatten an iterable of parameters into a single vector.  Parameters  parameters ( Iterable  [  Tensor  ] ) – an iterable of Tensors that are the\\nparameters of a model.  Returns  The parameters represented by a single vector  Return type  Tensor\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parameters_to_vector.html#torch-nn-utils-parameters-to-vector\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parameters_to_vector.html#torch-nn-utils-parameters-to-vector', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/convert_parameters.py#L6', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.vector_to_parameters.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grads_with_norm_.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a2f'), 'title': 'vector_to_parameters', 'page_text': 'torch.nn.utils.vector_to_parameters [LINK_1]  torch.nn.utils.vector_to_parameters( vec , parameters ) [source]  [source]  [LINK_2]  Copy slices of a vector into an iterable of parameters.  Parameters  vec ( Tensor ) – a single vector representing the parameters of a model.  parameters ( Iterable  [  Tensor  ] ) – an iterable of Tensors that are the\\nparameters of a model.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.vector_to_parameters.html#torch-nn-utils-vector-to-parameters\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.vector_to_parameters.html#torch-nn-utils-vector-to-parameters', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/convert_parameters.py#L28', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parameters_to_vector.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a30'), 'title': 'fuse_conv_bn_eval', 'page_text': 'torch.nn.utils.fuse_conv_bn_eval [LINK_1]  torch.nn.utils.fuse_conv_bn_eval( conv , bn , transpose=False ) [source]  [source]  [LINK_2]  Fuse a convolutional module and a BatchNorm module into a single, new convolutional module.  Parameters  conv ( torch.nn.modules.conv._ConvNd ) – A convolutional module.  bn ( torch.nn.modules.batchnorm._BatchNorm ) – A BatchNorm module.  transpose ( bool  ,  optional ) – If True, transpose the convolutional weight. Defaults to False.  Returns  The fused convolutional module.  Return type  torch.nn.modules.conv._ConvNd  Note  Both conv and bn must be in eval mode, and bn must have its running buffers computed.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html#torch-nn-utils-fuse-conv-bn-eval\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html#torch.nn.utils.fuse_conv_bn_eval', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html#torch-nn-utils-fuse-conv-bn-eval', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/fusion.html#fuse_conv_bn_eval', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/fusion.py#L20', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html#torch.nn.utils.fuse_conv_bn_eval', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_weights.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.vector_to_parameters.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a31'), 'title': 'fuse_conv_bn_weights', 'page_text': 'torch.nn.utils.fuse_conv_bn_weights [LINK_1]  torch.nn.utils.fuse_conv_bn_weights( conv_w , conv_b , bn_rm , bn_rv , bn_eps , bn_w , bn_b , transpose=False ) [source]  [source]  [LINK_2]  Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters.  Parameters  conv_w ( torch.Tensor ) – Convolutional weight.  conv_b ( Optional  [  torch.Tensor  ] ) – Convolutional bias.  bn_rm ( torch.Tensor ) – BatchNorm running mean.  bn_rv ( torch.Tensor ) – BatchNorm running variance.  bn_eps ( float ) – BatchNorm epsilon.  bn_w ( Optional  [  torch.Tensor  ] ) – BatchNorm weight.  bn_b ( Optional  [  torch.Tensor  ] ) – BatchNorm bias.  transpose ( bool  ,  optional ) – If True, transpose the conv weight. Defaults to False.  Returns  Fused convolutional weight and bias.  Return type  Tuple[torch.nn.Parameter, torch.nn.Parameter]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_weights.html#torch-nn-utils-fuse-conv-bn-weights\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_weights.html#torch.nn.utils.fuse_conv_bn_weights', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_weights.html#torch-nn-utils-fuse-conv-bn-weights', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/fusion.html#fuse_conv_bn_weights', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/fusion.py#L56', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_weights.html#torch.nn.utils.fuse_conv_bn_weights', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_eval.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a32'), 'title': 'fuse_linear_bn_eval', 'page_text': 'torch.nn.utils.fuse_linear_bn_eval [LINK_1]  torch.nn.utils.fuse_linear_bn_eval( linear , bn ) [source]  [source]  [LINK_2]  Fuse a linear module and a BatchNorm module into a single, new linear module.  Parameters  linear ( torch.nn.Linear ) – A Linear module.  bn ( torch.nn.modules.batchnorm._BatchNorm ) – A BatchNorm module.  Returns  The fused linear module.  Return type  torch.nn.Linear  Note  Both linear and bn must be in eval mode, and bn must have its running buffers computed.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_eval.html#torch-nn-utils-fuse-linear-bn-eval\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_eval.html#torch.nn.utils.fuse_linear_bn_eval', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_eval.html#torch-nn-utils-fuse-linear-bn-eval', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/fusion.html#fuse_linear_bn_eval', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/fusion.py#L109', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_eval.html#torch.nn.utils.fuse_linear_bn_eval', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_weights.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_weights.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a33'), 'title': 'fuse_linear_bn_weights', 'page_text': 'torch.nn.utils.fuse_linear_bn_weights [LINK_1]  torch.nn.utils.fuse_linear_bn_weights( linear_w , linear_b , bn_rm , bn_rv , bn_eps , bn_w , bn_b ) [source]  [source]  [LINK_2]  Fuse linear module parameters and BatchNorm module parameters into new linear module parameters.  Parameters  linear_w ( torch.Tensor ) – Linear weight.  linear_b ( Optional  [  torch.Tensor  ] ) – Linear bias.  bn_rm ( torch.Tensor ) – BatchNorm running mean.  bn_rv ( torch.Tensor ) – BatchNorm running variance.  bn_eps ( float ) – BatchNorm epsilon.  bn_w ( torch.Tensor ) – BatchNorm weight.  bn_b ( torch.Tensor ) – BatchNorm bias.  Returns  Fused linear weight and bias.  Return type  Tuple[torch.nn.Parameter, torch.nn.Parameter]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_weights.html#torch-nn-utils-fuse-linear-bn-weights\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_weights.html#torch.nn.utils.fuse_linear_bn_weights', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_weights.html#torch-nn-utils-fuse-linear-bn-weights', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/fusion.html#fuse_linear_bn_weights', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/fusion.py#L156', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_weights.html#torch.nn.utils.fuse_linear_bn_weights', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv2d_weight_memory_format.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_eval.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a34'), 'title': 'convert_conv2d_weight_memory_format', 'page_text': 'torch.nn.utils.convert_conv2d_weight_memory_format [LINK_1]  torch.nn.utils.convert_conv2d_weight_memory_format( module , memory_format ) [source]  [source]  [LINK_2]  Convert memory_format of nn.Conv2d.weight to memory_format .  The conversion recursively applies to nested nn.Module , including module .\\nNote that it only changes the memory_format, but not the semantics of each dimensions.\\nThis function is used to facilitate the computation to adopt NHWC kernels, which\\nprovides considerable speed up for fp16 data on CUDA devices with compute capability >= 7.0  Note  Calling model.to(memory_format=torch.channels_last) is more aggressive\\nthan the utility function convert_conv2d_weight_memory_format . Any\\nlayer with 4d weight will be affected by model.to , which does not\\nnecessarily benefit from conversion to specified memory_format .\\nOne place we are confident in is that NHWC(channels_last) conversion for\\nconvolution in cuDNN, as it is beneficial to run convolution in NHWC,\\neven in cases where we have to apply permutation to input tensors.  Hence our strategy here is to convert only the weight of convolution to\\nchannels_last. This ensures that;\\n1. Fast convolution kernels will be used, the benefit of which could\\noutweigh overhead of permutation (if input is not in the same format).\\n2. No unnecessary permutations are applied on layers that do not benefit\\nfrom memory_format conversion.  The optimal case is that, layers between convolution layers are channels\\nlast compatible. Input tensor would be permuted to channels last when it\\nencounters the first convolution layer and stay in that memory format.\\nHence following convolutions will not need to permute its input tensor.  In case where a channels last incompatible layer is between convolution\\nlayers, we need to permute the input tensor back to contiguous format\\nfor that layer. The input tensor will go through the remaining layers in\\ncontiguous format and be permuted to channels last when it encounters\\nanother convolution layer. There’s no point in propagating that\\npermutation to an earlier layer, as most layers are quite agnostic to memory_format .  This claim might change when PyTorch supports fusion of permutation, as\\nthere might have been a better spot to fuse the permutation other than\\nimmediately before a convolution.  Parameters  module ( nn.Module ) – nn.Conv2d & nn.ConvTranspose2d or container nn.Module  memory_format – user specified memory_format ,\\ne.g. torch.channels_last or torch.contiguous_format  Returns  The original module with updated nn.Conv2d  Example  >>>input=torch.randint(1,10,(2,8,4,4),dtype=torch.float16,device=\"cuda\")>>>model=nn.Sequential(>>>nn.Conv2d(8,4,3)).cuda().half()>>># This is identical to:>>># nn.utils.convert_conv2d_weight_memory_format(model, torch.channels_last)>>>model=nn.utils.convert_conv2d_weight_memory_format(model,torch.channels_last)>>>out=model(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch-nn-utils-convert-conv2d-weight-memory-format\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch.nn.utils.convert_conv2d_weight_memory_format', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch-nn-utils-convert-conv2d-weight-memory-format', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/memory_format.html#convert_conv2d_weight_memory_format', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/memory_format.py#L5', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch.nn.utils.convert_conv2d_weight_memory_format', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv3d_weight_memory_format.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_linear_bn_weights.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a35'), 'title': 'convert_conv3d_weight_memory_format', 'page_text': 'torch.nn.utils.convert_conv3d_weight_memory_format [LINK_1]  torch.nn.utils.convert_conv3d_weight_memory_format( module , memory_format ) [source]  [source]  [LINK_2]  Convert memory_format of nn.Conv3d.weight to memory_format The conversion recursively applies to nested nn.Module , including module .\\nNote that it only changes the memory_format, but not the semantics of each dimensions.\\nThis function is used to facilitate the computation to adopt NHWC kernels, which\\nprovides considerable speed up for fp16 data on CUDA devices with compute capability >= 7.0  Note  Calling model.to(memory_format=torch.channels_last_3d) is more aggressive\\nthan the utility function convert_conv3d_weight_memory_format . Any\\nlayer with 4d weight will be affected by model.to , which does not\\nnecessarily benefit from conversion to specified memory_format .\\nOne place we are confident in is that NDHWC(channels_last_3d) conversion for\\nconvolution in cuDNN, as it is beneficial to run convolution in NDHWC,\\neven in cases where we have to apply permutation to input tensors.  Hence our strategy here is to convert only the weight of convolution to\\nchannels_last_3d. This ensures that;\\n1. Fast convolution kernels will be used, the benefit of which could\\noutweigh overhead of permutation (if input is not in the same format).\\n2. No unnecessary permutations are applied on layers that do not benefit\\nfrom memory_format conversion.  The optimal case is that, layers between convolution layers are channels\\nlast compatible. Input tensor would be permuted to channels last when it\\nencounters the first convolution layer and stay in that memory format.\\nHence following convolutions will not need to permute its input tensor.  In case where a channels last incompatible layer is between convolution\\nlayers, we need to permute the input tensor back to contiguous format\\nfor that layer. The input tensor will go through the remaining layers in\\ncontiguous format and be permuted to channels last when it encounters\\nanother convolution layer. There’s no point in propagating that\\npermutation to an earlier layer, as most layers are quite agnostic to memory_format .  This claim might change when PyTorch supports fusion of permutation, as\\nthere might have been a better spot to fuse the permutation other than\\nimmediately before a convolution.  Parameters  module ( nn.Module ) – nn.Conv3d & nn.ConvTranspose3d or container nn.Module  memory_format – user specified memory_format ,\\ne.g. torch.channels_last or torch.contiguous_format  Returns  The original module with updated nn.Conv3d  Example  >>>input=torch.randint(1,10,(2,8,4,4,4),dtype=torch.float16,device=\"cuda\")>>>model=nn.Sequential(>>>nn.Conv3d(8,4,3)).cuda().half()>>># This is identical to:>>># nn.utils.convert_conv3d_weight_memory_format(model, torch.channels_last_3d)>>>model=nn.utils.convert_conv3d_weight_memory_format(model,torch.channels_last_3d)>>>out=model(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv3d_weight_memory_format.html#torch-nn-utils-convert-conv3d-weight-memory-format\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv3d_weight_memory_format.html#torch.nn.utils.convert_conv3d_weight_memory_format', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv3d_weight_memory_format.html#torch-nn-utils-convert-conv3d-weight-memory-format', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/memory_format.html#convert_conv3d_weight_memory_format', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/memory_format.py#L80', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv3d_weight_memory_format.html#torch.nn.utils.convert_conv3d_weight_memory_format', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv2d_weight_memory_format.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a36'), 'title': 'weight_norm', 'page_text': 'torch.nn.utils.weight_norm [LINK_1]  torch.nn.utils.weight_norm( module , name=\\'weight\\' , dim=0 ) [source]  [source]  [LINK_2]  Apply weight normalization to a parameter in the given module.  w  =  g  v  ∥  v  ∥  \\\\mathbf{w} = g \\\\dfrac{\\\\mathbf{v}}{\\\\|\\\\mathbf{v}\\\\|}w=g∥v∥v\\u200b  Weight normalization is a reparameterization that decouples the magnitude\\nof a weight tensor from its direction. This replaces the parameter specified\\nby name (e.g. \\'weight\\' ) with two parameters: one specifying the magnitude\\n(e.g. \\'weight_g\\' ) and one specifying the direction (e.g. \\'weight_v\\' ).\\nWeight normalization is implemented via a hook that recomputes the weight\\ntensor from the magnitude and direction before every forward() call.  By default, with dim=0 , the norm is computed independently per output\\nchannel/plane. To compute a norm over the entire weight tensor, use dim=None .  See https://arxiv.org/abs/1602.07868  Warning  This function is deprecated.  Use torch.nn.utils.parametrizations.weight_norm() which uses the modern parametrization API.  The new weight_norm is compatible\\nwith state_dict generated from old weight_norm .  Migration guide:  The magnitude ( weight_g ) and direction ( weight_v ) are now expressed\\nas parametrizations.weight.original0 and parametrizations.weight.original1 respectively.  If this is bothering you, please comment on https://github.com/pytorch/pytorch/issues/102999  To remove the weight normalization reparametrization, use torch.nn.utils.parametrize.remove_parametrizations() .  The weight is no longer recomputed once at module forward; instead, it will\\nbe recomputed on every access.  To restore the old behavior, use torch.nn.utils.parametrize.cached() before invoking the module\\nin question.  Parameters  module ( Module ) – containing module  name ( str  ,  optional ) – name of weight parameter  dim ( int  ,  optional ) – dimension over which to compute the norm  Returns  The original module with the weight norm hook  Return type  T_module  Example:  >>>m=weight_norm(nn.Linear(20,40),name=\\'weight\\')>>>mLinear(in_features=20, out_features=40, bias=True)>>>m.weight_g.size()torch.Size([40, 1])>>>m.weight_v.size()torch.Size([40, 20])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html#torch-nn-utils-weight-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html#torch.nn.utils.weight_norm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html#torch-nn-utils-weight-norm', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html#weight_norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/weight_norm.py#L83', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html#torch.nn.utils.weight_norm', 'https://arxiv.org/abs/1602.07868', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm', 'https://github.com/pytorch/pytorch/issues/102999', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_weight_norm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.convert_conv3d_weight_memory_format.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a37'), 'title': 'remove_weight_norm', 'page_text': 'torch.nn.utils.remove_weight_norm [LINK_1]  torch.nn.utils.remove_weight_norm( module , name=\\'weight\\' ) [source]  [source]  [LINK_2]  Remove the weight normalization reparameterization from a module.  Parameters  module ( Module ) – containing module  name ( str  ,  optional ) – name of weight parameter  Return type  T_module  Example  >>>m=weight_norm(nn.Linear(20,40))>>>remove_weight_norm(m)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_weight_norm.html#torch-nn-utils-remove-weight-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_weight_norm.html#torch-nn-utils-remove-weight-norm', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html#remove_weight_norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/weight_norm.py#L147', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a38'), 'title': 'spectral_norm', 'page_text': 'torch.nn.utils.spectral_norm [LINK_1]  torch.nn.utils.spectral_norm( module , name=\\'weight\\' , n_power_iterations=1 , eps=1e-12 , dim=None ) [source]  [source]  [LINK_2]  Apply spectral normalization to a parameter in the given module.  W  S  N  =  W  σ  (  W  )  ,  σ  (  W  )  =  max  \\u2061  h  :  h  ≠  0  ∥  W  h  ∥  2  ∥  h  ∥  2  \\\\mathbf{W}_{SN} = \\\\dfrac{\\\\mathbf{W}}{\\\\sigma(\\\\mathbf{W})},\\n\\\\sigma(\\\\mathbf{W}) = \\\\max_{\\\\mathbf{h}: \\\\mathbf{h} \\\\ne 0} \\\\dfrac{\\\\|\\\\mathbf{W} \\\\mathbf{h}\\\\|_2}{\\\\|\\\\mathbf{h}\\\\|_2}WSN\\u200b=σ(W)W\\u200b,σ(W)=h:h\\ue020=0max\\u200b∥h∥2\\u200b∥Wh∥2\\u200b\\u200b  Spectral normalization stabilizes the training of discriminators (critics)\\nin Generative Adversarial Networks (GANs) by rescaling the weight tensor\\nwith spectral normσ  \\\\sigmaσof the weight matrix calculated using\\npower iteration method. If the dimension of the weight tensor is greater\\nthan 2, it is reshaped to 2D in power iteration method to get spectral\\nnorm. This is implemented via a hook that calculates spectral norm and\\nrescales weight before every forward() call.  See Spectral Normalization for Generative Adversarial Networks .  Parameters  module ( nn.Module ) – containing module  name ( str  ,  optional ) – name of weight parameter  n_power_iterations ( int  ,  optional ) – number of power iterations to\\ncalculate spectral norm  eps ( float  ,  optional ) – epsilon for numerical stability in\\ncalculating norms  dim ( int  ,  optional ) – dimension corresponding to number of outputs,\\nthe default is 0 , except for modules that are instances of\\nConvTranspose{1,2,3}d, when it is 1  Returns  The original module with the spectral norm hook  Return type  T_module  Note  This function has been reimplemented as torch.nn.utils.parametrizations.spectral_norm() using the new\\nparametrization functionality in torch.nn.utils.parametrize.register_parametrization() . Please use\\nthe newer version. This function will be deprecated in a future version\\nof PyTorch.  Example:  >>>m=spectral_norm(nn.Linear(20,40))>>>mLinear(in_features=20, out_features=40, bias=True)>>>m.weight_u.size()torch.Size([40])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html#torch-nn-utils-spectral-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html#torch.nn.utils.spectral_norm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html#torch-nn-utils-spectral-norm', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/spectral_norm.html#spectral_norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/spectral_norm.py#L265', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html#torch.nn.utils.spectral_norm', 'https://arxiv.org/abs/1802.05957', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_spectral_norm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_weight_norm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a39'), 'title': 'remove_spectral_norm', 'page_text': 'torch.nn.utils.remove_spectral_norm [LINK_1]  torch.nn.utils.remove_spectral_norm( module , name=\\'weight\\' ) [source]  [source]  [LINK_2]  Remove the spectral normalization reparameterization from a module.  Parameters  module ( Module ) – containing module  name ( str  ,  optional ) – name of weight parameter  Return type  T_module  Example  >>>m=spectral_norm(nn.Linear(40,10))>>>remove_spectral_norm(m)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_spectral_norm.html#torch-nn-utils-remove-spectral-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_spectral_norm.html#torch-nn-utils-remove-spectral-norm', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/spectral_norm.py#L337', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.skip_init.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a3a'), 'title': 'skip_init', 'page_text': 'torch.nn.utils.skip_init [LINK_1]  torch.nn.utils.skip_init( module_cls , *args , **kwargs ) [source]  [source]  [LINK_2]  Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers.  This can be useful if initialization is slow or if custom initialization will\\nbe performed, making the default initialization unnecessary. There are some caveats to this, due to\\nthe way this function is implemented:  1. The module must accept a device arg in its constructor that is passed to any parameters\\nor buffers created during construction.  2. The module must not perform any computation on parameters in its constructor except\\ninitialization (i.e. functions from torch.nn.init ).  If these conditions are satisfied, the module can be instantiated with parameter / buffer values\\nuninitialized, as if having been created using torch.empty() .  Parameters  module_cls – Class object; should be a subclass of torch.nn.Module  args – args to pass to the module’s constructor  kwargs – kwargs to pass to the module’s constructor  Returns  Instantiated module with uninitialized parameters / buffers  Example:  >>>importtorch>>>m=torch.nn.utils.skip_init(torch.nn.Linear,5,1)>>>m.weightParameter containing:tensor([[0.0000e+00, 1.5846e+29, 7.8307e+00, 2.5250e-29, 1.1210e-44]],requires_grad=True)>>>m2=torch.nn.utils.skip_init(torch.nn.Linear,in_features=6,out_features=1)>>>m2.weightParameter containing:tensor([[-1.4677e+24,  4.5915e-41,  1.4013e-45,  0.0000e+00, -1.4677e+24,4.5915e-41]], requires_grad=True)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.skip_init.html#torch-nn-utils-skip-init\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.skip_init.html#torch-nn-utils-skip-init', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/init.html#skip_init', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/init.py#L7', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init', 'https://pytorch.org/docs/stable/nn.html#module-torch.nn.init', 'https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_spectral_norm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a3b'), 'title': 'prune.BasePruningMethod', 'page_text': 'BasePruningMethod [LINK_1]  class torch.nn.utils.prune.BasePruningMethod [source]  [source]  [LINK_2]  Abstract base class for creation of new pruning techniques.  Provides a skeleton for customization requiring the overriding of methods\\nsuch as compute_mask() and apply() .  classmethod apply( module , name , *args , importance_scores=None , **kwargs ) [source]  [source]  [LINK_3]  Add pruning on the fly and reparametrization of a tensor.  Adds the forward pre-hook that enables pruning on the fly and\\nthe reparametrization of a tensor in terms of the original tensor\\nand the pruning mask.  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  args – arguments passed on to a subclass of BasePruningMethod  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as module parameter) used to compute mask for pruning.\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the parameter being pruned.\\nIf unspecified or None, the parameter will be used in its place.  kwargs – keyword arguments passed on to a subclass of a BasePruningMethod  apply_mask( module ) [source]  [source]  [LINK_4]  Simply handles the multiplication between the parameter being pruned and the generated mask.  Fetches the mask and the original tensor from the module\\nand returns the pruned version of the tensor.  Parameters  module ( nn.Module ) – module containing the tensor to prune  Returns  pruned version of the input tensor  Return type  pruned_tensor ( torch.Tensor )  abstract compute_mask( t , default_mask ) [source]  [source]  [LINK_5]  Compute and returns a mask for the input tensor t .  Starting from a base default_mask (which should be a mask of ones\\nif the tensor has not been pruned yet), generate a random mask to\\napply on top of the default_mask according to the specific pruning\\nmethod recipe.  Parameters  t ( torch.Tensor ) – tensor representing the importance scores of the  prune. ( parameter to ) –  default_mask ( torch.Tensor ) – Base mask from previous pruning  iterations –  is ( that need to be respected after the new mask ) –  t. ( applied. Same dims as ) –  Returns  mask to apply to t , of same dims as t  Return type  mask ( torch.Tensor )  prune( t , default_mask=None , importance_scores=None ) [source]  [source]  [LINK_6]  Compute and returns a pruned version of input tensor t .  According to the pruning rule specified in compute_mask() .  Parameters  t ( torch.Tensor ) – tensor to prune (of same dimensions as default_mask ).  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as t ) used to compute mask for pruning t .\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the t that is being pruned.\\nIf unspecified or None, the tensor t will be used in its place.  default_mask ( torch.Tensor  ,  optional ) – mask from previous pruning\\niteration, if any. To be considered when determining what\\nportion of the tensor that pruning should act on. If None,\\ndefault to a mask of ones.  Returns  pruned version of tensor t .  remove( module ) [source]  [source]  [LINK_7]  Remove the pruning reparameterization from a module.  The pruned parameter named name remains permanently pruned,\\nand the parameter named name+\\'_orig\\' is removed from the parameter list.\\nSimilarly, the buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#basepruningmethod\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply_mask\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.compute_mask\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.prune\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#basepruningmethod', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#BasePruningMethod', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L11', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.compute_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#BasePruningMethod.apply', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L75', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#BasePruningMethod.apply_mask', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L53', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#BasePruningMethod.compute_mask', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L33', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.compute_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#BasePruningMethod.prune', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.prune', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.compute_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#BasePruningMethod.remove', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.skip_init.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a3c'), 'title': 'prune.PruningContainer', 'page_text': 'PruningContainer [LINK_1]  class torch.nn.utils.prune.PruningContainer( *args ) [source]  [source]  [LINK_2]  Container holding a sequence of pruning methods for iterative pruning.  Keeps track of the order in which pruning methods are applied and handles\\ncombining successive pruning calls.  Accepts as argument an instance of a BasePruningMethod or an iterable of\\nthem.  add_pruning_method( method ) [source]  [source]  [LINK_3]  Add a child pruning method to the container.  Parameters  method ( subclass  of  BasePruningMethod ) – child pruning method\\nto be added to the container.  classmethod apply( module , name , *args , importance_scores=None , **kwargs ) [source]  [LINK_4]  Add pruning on the fly and reparametrization of a tensor.  Adds the forward pre-hook that enables pruning on the fly and\\nthe reparametrization of a tensor in terms of the original tensor\\nand the pruning mask.  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  args – arguments passed on to a subclass of BasePruningMethod  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as module parameter) used to compute mask for pruning.\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the parameter being pruned.\\nIf unspecified or None, the parameter will be used in its place.  kwargs – keyword arguments passed on to a subclass of a BasePruningMethod  apply_mask( module ) [source]  [LINK_5]  Simply handles the multiplication between the parameter being pruned and the generated mask.  Fetches the mask and the original tensor from the module\\nand returns the pruned version of the tensor.  Parameters  module ( nn.Module ) – module containing the tensor to prune  Returns  pruned version of the input tensor  Return type  pruned_tensor ( torch.Tensor )  compute_mask( t , default_mask ) [source]  [source]  [LINK_6]  Apply the latest method by computing the new partial masks and returning its combination with the default_mask .  The new partial mask should be computed on the entries or channels\\nthat were not zeroed out by the default_mask .\\nWhich portions of the tensor t the new mask will be calculated from\\ndepends on the PRUNING_TYPE (handled by the type handler):  for ‘unstructured’, the mask will be computed from the raveled\\nlist of nonmasked entries;  for ‘structured’, the mask will be computed from the nonmasked\\nchannels in the tensor;  for ‘global’, the mask will be computed across all entries.  Parameters  t ( torch.Tensor ) – tensor representing the parameter to prune\\n(of same dimensions as default_mask ).  default_mask ( torch.Tensor ) – mask from previous pruning iteration.  Returns  new mask that combines the effects\\nof the default_mask and the new mask from the current\\npruning method (of same dimensions as default_mask and t ).  Return type  mask ( torch.Tensor )  prune( t , default_mask=None , importance_scores=None ) [source]  [LINK_7]  Compute and returns a pruned version of input tensor t .  According to the pruning rule specified in compute_mask() .  Parameters  t ( torch.Tensor ) – tensor to prune (of same dimensions as default_mask ).  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as t ) used to compute mask for pruning t .\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the t that is being pruned.\\nIf unspecified or None, the tensor t will be used in its place.  default_mask ( torch.Tensor  ,  optional ) – mask from previous pruning\\niteration, if any. To be considered when determining what\\nportion of the tensor that pruning should act on. If None,\\ndefault to a mask of ones.  Returns  pruned version of tensor t .  remove( module ) [source]  [LINK_8]  Remove the pruning reparameterization from a module.  The pruned parameter named name remains permanently pruned,\\nand the parameter named name+\\'_orig\\' is removed from the parameter list.\\nSimilarly, the buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#pruningcontainer\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.add_pruning_method\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.apply\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.apply_mask\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.compute_mask\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.prune\\n [LINK_8]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#pruningcontainer', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#PruningContainer', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L262', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#PruningContainer.add_pruning_method', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L284', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.add_pruning_method', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L75', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.apply', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L53', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.apply_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#PruningContainer.compute_mask', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L312', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.compute_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.prune', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.compute_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a3d'), 'title': 'prune.Identity', 'page_text': 'Identity [LINK_1]  class torch.nn.utils.prune.Identity [source]  [source]  [LINK_2]  Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.  classmethod apply( module , name ) [source]  [source]  [LINK_3]  Add pruning on the fly and reparametrization of a tensor.  Adds the forward pre-hook that enables pruning on the fly and\\nthe reparametrization of a tensor in terms of the original tensor\\nand the pruning mask.  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  apply_mask( module ) [source]  [LINK_4]  Simply handles the multiplication between the parameter being pruned and the generated mask.  Fetches the mask and the original tensor from the module\\nand returns the pruned version of the tensor.  Parameters  module ( nn.Module ) – module containing the tensor to prune  Returns  pruned version of the input tensor  Return type  pruned_tensor ( torch.Tensor )  prune( t , default_mask=None , importance_scores=None ) [source]  [LINK_5]  Compute and returns a pruned version of input tensor t .  According to the pruning rule specified in compute_mask() .  Parameters  t ( torch.Tensor ) – tensor to prune (of same dimensions as default_mask ).  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as t ) used to compute mask for pruning t .\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the t that is being pruned.\\nIf unspecified or None, the tensor t will be used in its place.  default_mask ( torch.Tensor  ,  optional ) – mask from previous pruning\\niteration, if any. To be considered when determining what\\nportion of the tensor that pruning should act on. If None,\\ndefault to a mask of ones.  Returns  pruned version of tensor t .  remove( module ) [source]  [LINK_6]  Remove the pruning reparameterization from a module.  The pruned parameter named name remains permanently pruned,\\nand the parameter named name+\\'_orig\\' is removed from the parameter list.\\nSimilarly, the buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#identity\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.apply\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.apply_mask\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.prune\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#identity', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#Identity', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L408', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#Identity.apply', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L417', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.apply', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L53', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.apply_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.prune', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a3e'), 'title': 'prune.RandomUnstructured', 'page_text': 'RandomUnstructured [LINK_1]  class torch.nn.utils.prune.RandomUnstructured( amount ) [source]  [source]  [LINK_2]  Prune (currently unpruned) units in a tensor at random.  Parameters  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  classmethod apply( module , name , amount ) [source]  [source]  [LINK_3]  Add pruning on the fly and reparametrization of a tensor.  Adds the forward pre-hook that enables pruning on the fly and\\nthe reparametrization of a tensor in terms of the original tensor\\nand the pruning mask.  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  apply_mask( module ) [source]  [LINK_4]  Simply handles the multiplication between the parameter being pruned and the generated mask.  Fetches the mask and the original tensor from the module\\nand returns the pruned version of the tensor.  Parameters  module ( nn.Module ) – module containing the tensor to prune  Returns  pruned version of the input tensor  Return type  pruned_tensor ( torch.Tensor )  prune( t , default_mask=None , importance_scores=None ) [source]  [LINK_5]  Compute and returns a pruned version of input tensor t .  According to the pruning rule specified in compute_mask() .  Parameters  t ( torch.Tensor ) – tensor to prune (of same dimensions as default_mask ).  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as t ) used to compute mask for pruning t .\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the t that is being pruned.\\nIf unspecified or None, the tensor t will be used in its place.  default_mask ( torch.Tensor  ,  optional ) – mask from previous pruning\\niteration, if any. To be considered when determining what\\nportion of the tensor that pruning should act on. If None,\\ndefault to a mask of ones.  Returns  pruned version of tensor t .  remove( module ) [source]  [LINK_6]  Remove the pruning reparameterization from a module.  The pruned parameter named name remains permanently pruned,\\nand the parameter named name+\\'_orig\\' is removed from the parameter list.\\nSimilarly, the buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#randomunstructured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.apply\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.apply_mask\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.prune\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#randomunstructured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#RandomUnstructured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L433', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#RandomUnstructured.apply', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L472', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.apply', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L53', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.apply_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.prune', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.Identity.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a3f'), 'title': 'prune.L1Unstructured', 'page_text': 'L1Unstructured [LINK_1]  class torch.nn.utils.prune.L1Unstructured( amount ) [source]  [source]  [LINK_2]  Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.  Parameters  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  classmethod apply( module , name , amount , importance_scores=None ) [source]  [source]  [LINK_3]  Add pruning on the fly and reparametrization of a tensor.  Adds the forward pre-hook that enables pruning on the fly and\\nthe reparametrization of a tensor in terms of the original tensor\\nand the pruning mask.  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  importance_scores ( torch.Tensor ) – tensor of importance scores (of same\\nshape as module parameter) used to compute mask for pruning.\\nThe values in this tensor indicate the importance of the corresponding\\nelements in the parameter being pruned.\\nIf unspecified or None, the module parameter will be used in its place.  apply_mask( module ) [source]  [LINK_4]  Simply handles the multiplication between the parameter being pruned and the generated mask.  Fetches the mask and the original tensor from the module\\nand returns the pruned version of the tensor.  Parameters  module ( nn.Module ) – module containing the tensor to prune  Returns  pruned version of the input tensor  Return type  pruned_tensor ( torch.Tensor )  prune( t , default_mask=None , importance_scores=None ) [source]  [LINK_5]  Compute and returns a pruned version of input tensor t .  According to the pruning rule specified in compute_mask() .  Parameters  t ( torch.Tensor ) – tensor to prune (of same dimensions as default_mask ).  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as t ) used to compute mask for pruning t .\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the t that is being pruned.\\nIf unspecified or None, the tensor t will be used in its place.  default_mask ( torch.Tensor  ,  optional ) – mask from previous pruning\\niteration, if any. To be considered when determining what\\nportion of the tensor that pruning should act on. If None,\\ndefault to a mask of ones.  Returns  pruned version of tensor t .  remove( module ) [source]  [LINK_6]  Remove the pruning reparameterization from a module.  The pruned parameter named name remains permanently pruned,\\nand the parameter named name+\\'_orig\\' is removed from the parameter list.\\nSimilarly, the buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#l1unstructured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.apply\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.apply_mask\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.prune\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#l1unstructured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#L1Unstructured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L492', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#L1Unstructured.apply', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L531', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.apply', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L53', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.apply_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.prune', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomUnstructured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a40'), 'title': 'prune.RandomStructured', 'page_text': 'RandomStructured [LINK_1]  class torch.nn.utils.prune.RandomStructured( amount , dim=-1 ) [source]  [source]  [LINK_2]  Prune entire (currently unpruned) channels in a tensor at random.  Parameters  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  dim ( int  ,  optional ) – index of the dim along which we define\\nchannels to prune. Default: -1.  classmethod apply( module , name , amount , dim=-1 ) [source]  [source]  [LINK_3]  Add pruning on the fly and reparametrization of a tensor.  Adds the forward pre-hook that enables pruning on the fly and\\nthe reparametrization of a tensor in terms of the original tensor\\nand the pruning mask.  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  dim ( int  ,  optional ) – index of the dim along which we define\\nchannels to prune. Default: -1.  apply_mask( module ) [source]  [LINK_4]  Simply handles the multiplication between the parameter being pruned and the generated mask.  Fetches the mask and the original tensor from the module\\nand returns the pruned version of the tensor.  Parameters  module ( nn.Module ) – module containing the tensor to prune  Returns  pruned version of the input tensor  Return type  pruned_tensor ( torch.Tensor )  compute_mask( t , default_mask ) [source]  [source]  [LINK_5]  Compute and returns a mask for the input tensor t .  Starting from a base default_mask (which should be a mask of ones\\nif the tensor has not been pruned yet), generate a random mask to\\napply on top of the default_mask by randomly zeroing out channels\\nalong the specified dim of the tensor.  Parameters  t ( torch.Tensor ) – tensor representing the parameter to prune  default_mask ( torch.Tensor ) – Base mask from previous pruning\\niterations, that need to be respected after the new mask is\\napplied. Same dims as t .  Returns  mask to apply to t , of same dims as t  Return type  mask ( torch.Tensor )  Raises  IndexError – if self.dim>=len(t.shape)  prune( t , default_mask=None , importance_scores=None ) [source]  [LINK_6]  Compute and returns a pruned version of input tensor t .  According to the pruning rule specified in compute_mask() .  Parameters  t ( torch.Tensor ) – tensor to prune (of same dimensions as default_mask ).  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as t ) used to compute mask for pruning t .\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the t that is being pruned.\\nIf unspecified or None, the tensor t will be used in its place.  default_mask ( torch.Tensor  ,  optional ) – mask from previous pruning\\niteration, if any. To be considered when determining what\\nportion of the tensor that pruning should act on. If None,\\ndefault to a mask of ones.  Returns  pruned version of tensor t .  remove( module ) [source]  [LINK_7]  Remove the pruning reparameterization from a module.  The pruned parameter named name remains permanently pruned,\\nand the parameter named name+\\'_orig\\' is removed from the parameter list.\\nSimilarly, the buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#randomstructured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.apply\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.apply_mask\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.compute_mask\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.prune\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#randomstructured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#RandomStructured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L558', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#RandomStructured.apply', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L641', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.apply', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L53', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.apply_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#RandomStructured.compute_mask', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L578', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.compute_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/exceptions.html#IndexError', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.prune', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.compute_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a41'), 'title': 'prune.LnStructured', 'page_text': 'LnStructured [LINK_1]  class torch.nn.utils.prune.LnStructured( amount , n , dim=-1 ) [source]  [source]  [LINK_2]  Prune entire (currently unpruned) channels in a tensor based on their L n -norm.  Parameters  amount ( int  or  float ) – quantity of channels to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  n ( int  ,  float  ,  inf  ,  -inf  ,  \\'fro\\'  ,  \\'nuc\\' ) – See documentation of valid\\nentries for argument p in torch.norm() .  dim ( int  ,  optional ) – index of the dim along which we define\\nchannels to prune. Default: -1.  classmethod apply( module , name , amount , n , dim , importance_scores=None ) [source]  [source]  [LINK_3]  Add pruning on the fly and reparametrization of a tensor.  Adds the forward pre-hook that enables pruning on the fly and\\nthe reparametrization of a tensor in terms of the original tensor\\nand the pruning mask.  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  n ( int  ,  float  ,  inf  ,  -inf  ,  \\'fro\\'  ,  \\'nuc\\' ) – See documentation of valid\\nentries for argument p in torch.norm() .  dim ( int ) – index of the dim along which we define channels to\\nprune.  importance_scores ( torch.Tensor ) – tensor of importance scores (of same\\nshape as module parameter) used to compute mask for pruning.\\nThe values in this tensor indicate the importance of the corresponding\\nelements in the parameter being pruned.\\nIf unspecified or None, the module parameter will be used in its place.  apply_mask( module ) [source]  [LINK_4]  Simply handles the multiplication between the parameter being pruned and the generated mask.  Fetches the mask and the original tensor from the module\\nand returns the pruned version of the tensor.  Parameters  module ( nn.Module ) – module containing the tensor to prune  Returns  pruned version of the input tensor  Return type  pruned_tensor ( torch.Tensor )  compute_mask( t , default_mask ) [source]  [source]  [LINK_5]  Compute and returns a mask for the input tensor t .  Starting from a base default_mask (which should be a mask of ones\\nif the tensor has not been pruned yet), generate a mask to apply on\\ntop of the default_mask by zeroing out the channels along the\\nspecified dim with the lowest L n -norm.  Parameters  t ( torch.Tensor ) – tensor representing the parameter to prune  default_mask ( torch.Tensor ) – Base mask from previous pruning\\niterations, that need to be respected after the new mask is\\napplied.  Same dims as t .  Returns  mask to apply to t , of same dims as t  Return type  mask ( torch.Tensor )  Raises  IndexError – if self.dim>=len(t.shape)  prune( t , default_mask=None , importance_scores=None ) [source]  [LINK_6]  Compute and returns a pruned version of input tensor t .  According to the pruning rule specified in compute_mask() .  Parameters  t ( torch.Tensor ) – tensor to prune (of same dimensions as default_mask ).  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as t ) used to compute mask for pruning t .\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the t that is being pruned.\\nIf unspecified or None, the tensor t will be used in its place.  default_mask ( torch.Tensor  ,  optional ) – mask from previous pruning\\niteration, if any. To be considered when determining what\\nportion of the tensor that pruning should act on. If None,\\ndefault to a mask of ones.  Returns  pruned version of tensor t .  remove( module ) [source]  [LINK_7]  Remove the pruning reparameterization from a module.  The pruned parameter named name remains permanently pruned,\\nand the parameter named name+\\'_orig\\' is removed from the parameter list.\\nSimilarly, the buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#lnstructured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.apply\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.apply_mask\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.compute_mask\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.prune\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#lnstructured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#LnStructured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L663', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#LnStructured.apply', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L756', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.apply', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L53', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.apply_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#LnStructured.compute_mask', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L686', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.compute_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/exceptions.html#IndexError', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.prune', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.compute_mask', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a42'), 'title': 'prune.CustomFromMask', 'page_text': 'CustomFromMask [LINK_1]  class torch.nn.utils.prune.CustomFromMask( mask ) [source]  [source]  [LINK_2]  classmethod apply( module , name , mask ) [source]  [source]  [LINK_3]  Add pruning on the fly and reparametrization of a tensor.  Adds the forward pre-hook that enables pruning on the fly and\\nthe reparametrization of a tensor in terms of the original tensor\\nand the pruning mask.  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  apply_mask( module ) [source]  [LINK_4]  Simply handles the multiplication between the parameter being pruned and the generated mask.  Fetches the mask and the original tensor from the module\\nand returns the pruned version of the tensor.  Parameters  module ( nn.Module ) – module containing the tensor to prune  Returns  pruned version of the input tensor  Return type  pruned_tensor ( torch.Tensor )  prune( t , default_mask=None , importance_scores=None ) [source]  [LINK_5]  Compute and returns a pruned version of input tensor t .  According to the pruning rule specified in compute_mask() .  Parameters  t ( torch.Tensor ) – tensor to prune (of same dimensions as default_mask ).  importance_scores ( torch.Tensor ) – tensor of importance scores (of\\nsame shape as t ) used to compute mask for pruning t .\\nThe values in this tensor indicate the importance of the\\ncorresponding elements in the t that is being pruned.\\nIf unspecified or None, the tensor t will be used in its place.  default_mask ( torch.Tensor  ,  optional ) – mask from previous pruning\\niteration, if any. To be considered when determining what\\nportion of the tensor that pruning should act on. If None,\\ndefault to a mask of ones.  Returns  pruned version of tensor t .  remove( module ) [source]  [LINK_6]  Remove the pruning reparameterization from a module.  The pruned parameter named name remains permanently pruned,\\nand the parameter named name+\\'_orig\\' is removed from the parameter list.\\nSimilarly, the buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#customfrommask\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.apply\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.apply_mask\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.prune\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#customfrommask', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#CustomFromMask', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L792', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#CustomFromMask.apply', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L803', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.apply', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L53', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.apply_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L204', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.prune', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L234', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.identity.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a43'), 'title': 'prune.identity', 'page_text': 'torch.nn.utils.prune.identity [LINK_1]  torch.nn.utils.prune.identity( module , name ) [source]  [source]  [LINK_2]  Apply pruning reparametrization without pruning any units.  Applies pruning reparametrization to the tensor corresponding to the\\nparameter called name in module without actually pruning any\\nunits. Modifies module in place (and also return the modified module)\\nby:  adding a named buffer called name+\\'_mask\\' corresponding to the\\nbinary mask applied to the parameter name by the pruning method.  replacing the parameter name by its pruned version, while the\\noriginal (unpruned) parameter is stored in a new parameter named name+\\'_orig\\' .  Note  The mask is a tensor of ones.  Parameters  module ( nn.Module ) – module containing the tensor to prune.  name ( str ) – parameter name within module on which pruning\\nwill act.  Returns  modified (i.e. pruned) version of the input module  Return type  module ( nn.Module )  Examples  >>>m=prune.identity(nn.Linear(2,3),\\'bias\\')>>>print(m.bias_mask)tensor([1., 1., 1.])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.identity.html#torch-nn-utils-prune-identity\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.identity.html#torch-nn-utils-prune-identity', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#identity', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L819', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.CustomFromMask.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a44'), 'title': 'prune.random_unstructured', 'page_text': 'torch.nn.utils.prune.random_unstructured [LINK_1]  torch.nn.utils.prune.random_unstructured( module , name , amount ) [source]  [source]  [LINK_2]  Prune tensor by removing random (currently unpruned) units.  Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units\\nselected at random.\\nModifies module in place (and also return the modified module) by:  adding a named buffer called name+\\'_mask\\' corresponding to the\\nbinary mask applied to the parameter name by the pruning method.  replacing the parameter name by its pruned version, while the\\noriginal (unpruned) parameter is stored in a new parameter named name+\\'_orig\\' .  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  Returns  modified (i.e. pruned) version of the input module  Return type  module ( nn.Module )  Examples  >>>m=prune.random_unstructured(nn.Linear(2,3),\\'weight\\',amount=1)>>>torch.sum(m.weight_mask==0)tensor(1)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html#torch-nn-utils-prune-random-unstructured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html#torch-nn-utils-prune-random-unstructured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#random_unstructured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L854', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.identity.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a45'), 'title': 'prune.l1_unstructured', 'page_text': 'torch.nn.utils.prune.l1_unstructured [LINK_1]  torch.nn.utils.prune.l1_unstructured( module , name , amount , importance_scores=None ) [source]  [source]  [LINK_2]  Prune tensor by removing units with the lowest L1-norm.  Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the\\nlowest L1-norm.\\nModifies module in place (and also return the modified module)\\nby:  adding a named buffer called name+\\'_mask\\' corresponding to the\\nbinary mask applied to the parameter name by the pruning method.  replacing the parameter name by its pruned version, while the\\noriginal (unpruned) parameter is stored in a new parameter named name+\\'_orig\\' .  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  importance_scores ( torch.Tensor ) – tensor of importance scores (of same\\nshape as module parameter) used to compute mask for pruning.\\nThe values in this tensor indicate the importance of the corresponding\\nelements in the parameter being pruned.\\nIf unspecified or None, the module parameter will be used in its place.  Returns  modified (i.e. pruned) version of the input module  Return type  module ( nn.Module )  Examples  >>>m=prune.l1_unstructured(nn.Linear(2,3),\\'weight\\',amount=0.2)>>>m.state_dict().keys()odict_keys([\\'bias\\', \\'weight_orig\\', \\'weight_mask\\'])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch-nn-utils-prune-l1-unstructured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch-nn-utils-prune-l1-unstructured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#l1_unstructured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L891', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_structured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a46'), 'title': 'prune.random_structured', 'page_text': 'torch.nn.utils.prune.random_structured [LINK_1]  torch.nn.utils.prune.random_structured( module , name , amount , dim ) [source]  [source]  [LINK_2]  Prune tensor by removing random channels along the specified dimension.  Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels\\nalong the specified dim selected at random.\\nModifies module in place (and also return the modified module)\\nby:  adding a named buffer called name+\\'_mask\\' corresponding to the\\nbinary mask applied to the parameter name by the pruning method.  replacing the parameter name by its pruned version, while the\\noriginal (unpruned) parameter is stored in a new parameter named name+\\'_orig\\' .  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  dim ( int ) – index of the dim along which we define channels to prune.  Returns  modified (i.e. pruned) version of the input module  Return type  module ( nn.Module )  Examples  >>>m=prune.random_structured(...nn.Linear(5,3),\\'weight\\',amount=3,dim=1...)>>>columns_pruned=int(sum(torch.sum(m.weight,dim=0)==0))>>>print(columns_pruned)3\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_structured.html#torch-nn-utils-prune-random-structured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_structured.html#torch-nn-utils-prune-random-structured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#random_structured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L935', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a47'), 'title': 'prune.ln_structured', 'page_text': 'torch.nn.utils.prune.ln_structured [LINK_1]  torch.nn.utils.prune.ln_structured( module , name , amount , n , dim , importance_scores=None ) [source]  [source]  [LINK_2]  Prune tensor by removing channels with the lowest L n -norm along the specified dimension.  Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels\\nalong the specified dim with the lowest L n -norm.\\nModifies module in place (and also return the modified module)\\nby:  adding a named buffer called name+\\'_mask\\' corresponding to the\\nbinary mask applied to the parameter name by the pruning method.  replacing the parameter name by its pruned version, while the\\noriginal (unpruned) parameter is stored in a new parameter named name+\\'_orig\\' .  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  amount ( int  or  float ) – quantity of parameters to prune.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  n ( int  ,  float  ,  inf  ,  -inf  ,  \\'fro\\'  ,  \\'nuc\\' ) – See documentation of valid\\nentries for argument p in torch.norm() .  dim ( int ) – index of the dim along which we define channels to prune.  importance_scores ( torch.Tensor ) – tensor of importance scores (of same\\nshape as module parameter) used to compute mask for pruning.\\nThe values in this tensor indicate the importance of the corresponding\\nelements in the parameter being pruned.\\nIf unspecified or None, the module parameter will be used in its place.  Returns  modified (i.e. pruned) version of the input module  Return type  module ( nn.Module )  Examples  >>>fromtorch.nn.utilsimportprune>>>m=prune.ln_structured(...nn.Conv2d(5,3,2),\\'weight\\',amount=0.3,dim=1,n=float(\\'-inf\\')...)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html#torch-nn-utils-prune-ln-structured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html#torch-nn-utils-prune-ln-structured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#ln_structured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L976', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_structured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a48'), 'title': 'prune.global_unstructured', 'page_text': 'torch.nn.utils.prune.global_unstructured [LINK_1]  torch.nn.utils.prune.global_unstructured( parameters , pruning_method , importance_scores=None , **kwargs ) [source]  [source]  [LINK_2]  Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method .  Modifies modules in place by:  adding a named buffer called name+\\'_mask\\' corresponding to the\\nbinary mask applied to the parameter name by the pruning method.  replacing the parameter name by its pruned version, while the\\noriginal (unpruned) parameter is stored in a new parameter named name+\\'_orig\\' .  Parameters  parameters ( Iterable  of  (  module  ,  name  )  tuples ) – parameters of\\nthe model to prune in a global fashion, i.e. by aggregating all\\nweights prior to deciding which ones to prune. module must be of\\ntype nn.Module , and name must be a string.  pruning_method ( function ) – a valid pruning function from this module,\\nor a custom one implemented by the user that satisfies the\\nimplementation guidelines and has PRUNING_TYPE=\\'unstructured\\' .  importance_scores ( dict ) – a dictionary mapping (module, name) tuples to\\nthe corresponding parameter’s importance scores tensor. The tensor\\nshould be the same shape as the parameter, and is used for computing\\nmask for pruning.\\nIf unspecified or None, the parameter will be used in place of its\\nimportance scores.  kwargs – other keyword arguments such as:\\namount (int or float): quantity of parameters to prune across the\\nspecified parameters.\\nIf float , should be between 0.0 and 1.0 and represent the\\nfraction of parameters to prune. If int , it represents the\\nabsolute number of parameters to prune.  Raises  TypeError – if PRUNING_TYPE!=\\'unstructured\\'  Note  Since global structured pruning doesn’t make much sense unless the\\nnorm is normalized by the size of the parameter, we now limit the\\nscope of global pruning to unstructured methods.  Examples  >>>fromtorch.nn.utilsimportprune>>>fromcollectionsimportOrderedDict>>>net=nn.Sequential(OrderedDict([...(\\'first\\',nn.Linear(10,4)),...(\\'second\\',nn.Linear(4,1)),...]))>>>parameters_to_prune=(...(net.first,\\'weight\\'),...(net.second,\\'weight\\'),...)>>>prune.global_unstructured(...parameters_to_prune,...pruning_method=prune.L1Unstructured,...amount=10,...)>>>print(sum(torch.nn.utils.parameters_to_vector(net.buffers())==0))tensor(10)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html#torch-nn-utils-prune-global-unstructured\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html#torch-nn-utils-prune-global-unstructured', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#global_unstructured', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L1023', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://docs.python.org/3/library/exceptions.html#TypeError', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.custom_from_mask.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a49'), 'title': 'prune.custom_from_mask', 'page_text': 'torch.nn.utils.prune.custom_from_mask [LINK_1]  torch.nn.utils.prune.custom_from_mask( module , name , mask ) [source]  [source]  [LINK_2]  Prune tensor corresponding to parameter called name in module by applying the pre-computed mask in mask .  Modifies module in place (and also return the modified module) by:  adding a named buffer called name+\\'_mask\\' corresponding to the\\nbinary mask applied to the parameter name by the pruning method.  replacing the parameter name by its pruned version, while the\\noriginal (unpruned) parameter is stored in a new parameter named name+\\'_orig\\' .  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  mask ( Tensor ) – binary mask to be applied to the parameter.  Returns  modified (i.e. pruned) version of the input module  Return type  module ( nn.Module )  Examples  >>>fromtorch.nn.utilsimportprune>>>m=prune.custom_from_mask(...nn.Linear(5,3),name=\\'bias\\',mask=torch.tensor([0,1,0])...)>>>print(m.bias_mask)tensor([0., 1., 0.])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.custom_from_mask.html#torch-nn-utils-prune-custom-from-mask\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.custom_from_mask.html#torch-nn-utils-prune-custom-from-mask', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#custom_from_mask', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L1142', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a4a'), 'title': 'prune.remove', 'page_text': 'torch.nn.utils.prune.remove [LINK_1]  torch.nn.utils.prune.remove( module , name ) [source]  [source]  [LINK_2]  Remove the pruning reparameterization from a module and the pruning method from the forward hook.  The pruned parameter named name remains permanently pruned, and the parameter\\nnamed name+\\'_orig\\' is removed from the parameter list. Similarly,\\nthe buffer named name+\\'_mask\\' is removed from the buffers.  Note  Pruning itself is NOT undone or reversed!  Parameters  module ( nn.Module ) – module containing the tensor to prune  name ( str ) – parameter name within module on which pruning\\nwill act.  Examples  >>>m=random_unstructured(nn.Linear(5,7),name=\\'weight\\',amount=0.2)>>>m=remove(m,name=\\'weight\\')\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html#torch-nn-utils-prune-remove\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html#torch-nn-utils-prune-remove', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#remove', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L1175', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.is_pruned.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.custom_from_mask.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a4b'), 'title': 'prune.is_pruned', 'page_text': 'torch.nn.utils.prune.is_pruned [LINK_1]  torch.nn.utils.prune.is_pruned( module ) [source]  [source]  [LINK_2]  Check if a module is pruned by looking for pruning pre-hooks.  Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod .  Parameters  module ( nn.Module ) – object that is either pruned or unpruned  Returns  binary answer to whether module is pruned.  Examples  >>>fromtorch.nn.utilsimportprune>>>m=nn.Linear(5,7)>>>print(prune.is_pruned(m))False>>>prune.random_unstructured(m,name=\\'weight\\',amount=0.2)>>>print(prune.is_pruned(m))True\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.is_pruned.html#torch-nn-utils-prune-is-pruned\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.is_pruned.html#torch-nn-utils-prune-is-pruned', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/prune.html#is_pruned', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/prune.py#L1205', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.orthogonal.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a4c'), 'title': 'parametrizations.orthogonal', 'page_text': 'torch.nn.utils.parametrizations.orthogonal [LINK_1]  torch.nn.utils.parametrizations.orthogonal( module , name=\\'weight\\' , orthogonal_map=None , * , use_trivialization=True ) [source]  [source]  [LINK_2]  Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices.  LettingK  \\\\mathbb{K}KbeR  \\\\mathbb{R}RorC  \\\\mathbb{C}C, the parametrized\\nmatrixQ  ∈  K  m  ×  n  Q \\\\in \\\\mathbb{K}^{m \\\\times n}Q∈Km×nis orthogonal as  Q  H  Q  =  I  n  if  m  ≥  n  Q  Q  H  =  I  m  if  m  <  n  \\\\begin{align*}\\n    Q^{\\\\text{H}}Q &= \\\\mathrm{I}_n \\\\mathrlap{\\\\qquad \\\\text{if }m \\\\geq n}\\\\\\\\\\n    QQ^{\\\\text{H}} &= \\\\mathrm{I}_m \\\\mathrlap{\\\\qquad \\\\text{if }m < n}\\n\\\\end{align*}QHQQQH\\u200b=In\\u200bifm≥n=Im\\u200bifm<n\\u200b  whereQ  H  Q^{\\\\text{H}}QHis the conjugate transpose whenQ  QQis complex\\nand the transpose whenQ  QQis real-valued, andI  n  \\\\mathrm{I}_nIn\\u200bis the n -dimensional identity matrix.\\nIn plain words,Q  QQwill have orthonormal columns wheneverm  ≥  n  m \\\\geq nm≥nand orthonormal rows otherwise.  If the tensor has more than two dimensions, we consider it as a batch of matrices of shape (…, m, n) .  The matrixQ  QQmay be parametrized via three different orthogonal_map in terms of the original tensor:  \"matrix_exp\" / \"cayley\" :\\nthe matrix_exp() Q  =  exp  \\u2061  (  A  )  Q = \\\\exp(A)Q=exp(A)and the Cayley map Q  =  (  I  n  +  A  /  2  )  (  I  n  −  A  /  2  )  −  1  Q = (\\\\mathrm{I}_n + A/2)(\\\\mathrm{I}_n - A/2)^{-1}Q=(In\\u200b+A/2)(In\\u200b−A/2)−1are applied to a skew-symmetricA  AAto give an orthogonal matrix.  \"householder\" : computes a product of Householder reflectors\\n( householder_product() ).  \"matrix_exp\" / \"cayley\" often make the parametrized weight converge faster than \"householder\" , but they are slower to compute for very thin or very wide matrices.  If use_trivialization=True (default), the parametrization implements the “Dynamic Trivialization Framework”,\\nwhere an extra matrixB  ∈  K  n  ×  n  B \\\\in \\\\mathbb{K}^{n \\\\times n}B∈Kn×nis stored under module.parametrizations.weight[0].base . This helps the\\nconvergence of the parametrized layer at the expense of some extra memory use.\\nSee Trivializations for Gradient-Based Optimization on Manifolds .  Initial value ofQ  QQ:\\nIf the original tensor is not parametrized and use_trivialization=True (default), the initial value\\nofQ  QQis that of the original tensor if it is orthogonal (or unitary in the complex case)\\nand it is orthogonalized via the QR decomposition otherwise (see torch.linalg.qr() ).\\nSame happens when it is not parametrized and orthogonal_map=\"householder\" even when use_trivialization=False .\\nOtherwise, the initial value is the result of the composition of all the registered\\nparametrizations applied to the original tensor.  Note  This function is implemented using the parametrization functionality\\nin register_parametrization() .  Parameters  module ( nn.Module ) – module on which to register the parametrization.  name ( str  ,  optional ) – name of the tensor to make orthogonal. Default: \"weight\" .  orthogonal_map ( str  ,  optional ) – One of the following: \"matrix_exp\" , \"cayley\" , \"householder\" .\\nDefault: \"matrix_exp\" if the matrix is square or complex, \"householder\" otherwise.  use_trivialization ( bool  ,  optional ) – whether to use the dynamic trivialization framework.\\nDefault: True .  Returns  The original module with an orthogonal parametrization registered to the specified\\nweight  Return type  Module  Example:  >>>orth_linear=orthogonal(nn.Linear(20,40))>>>orth_linearParametrizedLinear(in_features=20, out_features=40, bias=True(parametrizations): ModuleDict((weight): ParametrizationList((0): _Orthogonal())))>>>Q=orth_linear.weight>>>torch.dist(Q.T@Q,torch.eye(20))tensor(4.9332e-07)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.orthogonal.html#torch-nn-utils-parametrizations-orthogonal\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.orthogonal.html#torch-nn-utils-parametrizations-orthogonal', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrizations.html#orthogonal', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrizations.py#L191', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal', 'https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp', 'https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map', 'https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product', 'https://arxiv.org/abs/1909.09501', 'https://pytorch.org/docs/stable/generated/torch.linalg.qr.html#torch.linalg.qr', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.weight_norm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.is_pruned.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a4d'), 'title': 'parametrizations.weight_norm', 'page_text': 'torch.nn.utils.parametrizations.weight_norm [LINK_1]  torch.nn.utils.parametrizations.weight_norm( module , name=\\'weight\\' , dim=0 ) [source]  [source]  [LINK_2]  Apply weight normalization to a parameter in the given module.  w  =  g  v  ∥  v  ∥  \\\\mathbf{w} = g \\\\dfrac{\\\\mathbf{v}}{\\\\|\\\\mathbf{v}\\\\|}w=g∥v∥v\\u200b  Weight normalization is a reparameterization that decouples the magnitude\\nof a weight tensor from its direction. This replaces the parameter specified\\nby name with two parameters: one specifying the magnitude\\nand one specifying the direction.  By default, with dim=0 , the norm is computed independently per output\\nchannel/plane. To compute a norm over the entire weight tensor, use dim=None .  See https://arxiv.org/abs/1602.07868  Parameters  module ( Module ) – containing module  name ( str  ,  optional ) – name of weight parameter  dim ( int  ,  optional ) – dimension over which to compute the norm  Returns  The original module with the weight norm hook  Example:  >>>m=weight_norm(nn.Linear(20,40),name=\\'weight\\')>>>mParametrizedLinear(in_features=20, out_features=40, bias=True(parametrizations): ModuleDict((weight): ParametrizationList((0): _WeightNorm())))>>>m.parametrizations.weight.original0.size()torch.Size([40, 1])>>>m.parametrizations.weight.original1.size()torch.Size([40, 20])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.weight_norm.html#torch-nn-utils-parametrizations-weight-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.weight_norm.html#torch-nn-utils-parametrizations-weight-norm', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrizations.html#weight_norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrizations.py#L334', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm', 'https://arxiv.org/abs/1602.07868', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.orthogonal.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a4e'), 'title': 'parametrizations.spectral_norm', 'page_text': 'torch.nn.utils.parametrizations.spectral_norm [LINK_1]  torch.nn.utils.parametrizations.spectral_norm( module , name=\\'weight\\' , n_power_iterations=1 , eps=1e-12 , dim=None ) [source]  [source]  [LINK_2]  Apply spectral normalization to a parameter in the given module.  W  S  N  =  W  σ  (  W  )  ,  σ  (  W  )  =  max  \\u2061  h  :  h  ≠  0  ∥  W  h  ∥  2  ∥  h  ∥  2  \\\\mathbf{W}_{SN} = \\\\dfrac{\\\\mathbf{W}}{\\\\sigma(\\\\mathbf{W})},\\n\\\\sigma(\\\\mathbf{W}) = \\\\max_{\\\\mathbf{h}: \\\\mathbf{h} \\\\ne 0} \\\\dfrac{\\\\|\\\\mathbf{W} \\\\mathbf{h}\\\\|_2}{\\\\|\\\\mathbf{h}\\\\|_2}WSN\\u200b=σ(W)W\\u200b,σ(W)=h:h\\ue020=0max\\u200b∥h∥2\\u200b∥Wh∥2\\u200b\\u200b  When applied on a vector, it simplifies to  x  S  N  =  x  ∥  x  ∥  2  \\\\mathbf{x}_{SN} = \\\\dfrac{\\\\mathbf{x}}{\\\\|\\\\mathbf{x}\\\\|_2}xSN\\u200b=∥x∥2\\u200bx\\u200b  Spectral normalization stabilizes the training of discriminators (critics)\\nin Generative Adversarial Networks (GANs) by reducing the Lipschitz constant\\nof the model.σ  \\\\sigmaσis approximated performing one iteration of the power method every time the weight is accessed. If the dimension of the\\nweight tensor is greater than 2, it is reshaped to 2D in power iteration\\nmethod to get spectral norm.  See Spectral Normalization for Generative Adversarial Networks .  Note  This function is implemented using the parametrization functionality\\nin register_parametrization() . It is a\\nreimplementation of torch.nn.utils.spectral_norm() .  Note  When this constraint is registered, the singular vectors associated to the largest\\nsingular value are estimated rather than sampled at random. These are then updated\\nperforming n_power_iterations of the power method whenever the tensor\\nis accessed with the module on training mode.  Note  If the _SpectralNorm module, i.e., module.parametrization.weight[idx] ,\\nis in training mode on removal, it will perform another power iteration.\\nIf you’d like to avoid this iteration, set the module to eval mode\\nbefore its removal.  Parameters  module ( nn.Module ) – containing module  name ( str  ,  optional ) – name of weight parameter. Default: \"weight\" .  n_power_iterations ( int  ,  optional ) – number of power iterations to\\ncalculate spectral norm. Default: 1 .  eps ( float  ,  optional ) – epsilon for numerical stability in\\ncalculating norms. Default: 1e-12 .  dim ( int  ,  optional ) – dimension corresponding to number of outputs.\\nDefault: 0 , except for modules that are instances of\\nConvTranspose{1,2,3}d, when it is 1  Returns  The original module with a new parametrization registered to the specified\\nweight  Return type  Module  Example:  >>>snm=spectral_norm(nn.Linear(20,40))>>>snmParametrizedLinear(in_features=20, out_features=40, bias=True(parametrizations): ModuleDict((weight): ParametrizationList((0): _SpectralNorm())))>>>torch.linalg.matrix_norm(snm.weight,2)tensor(1.0081, grad_fn=<AmaxBackward0>)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html#torch-nn-utils-parametrizations-spectral-norm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html#torch-nn-utils-parametrizations-spectral-norm', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrizations.html#spectral_norm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrizations.py#L527', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm', 'https://en.wikipedia.org/wiki/Power_iteration', 'https://arxiv.org/abs/1802.05957', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html#torch.nn.utils.spectral_norm', 'https://en.wikipedia.org/wiki/Power_iteration', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.weight_norm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a4f'), 'title': 'parametrize.register_parametrization', 'page_text': 'torch.nn.utils.parametrize.register_parametrization [LINK_1]  torch.nn.utils.parametrize.register_parametrization( module , tensor_name , parametrization , * , unsafe=False ) [source]  [source]  [LINK_2]  Register a parametrization to a tensor in a module.  Assume that tensor_name=\"weight\" for simplicity. When accessing module.weight ,\\nthe module will return the parametrized version parametrization(module.weight) .\\nIf the original tensor requires a gradient, the backward pass will differentiate\\nthrough parametrization , and the optimizer will update the tensor accordingly.  The first time that a module registers a parametrization, this function will add an attribute parametrizations to the module of type ParametrizationList .  The list of parametrizations on the tensor weight will be accessible under module.parametrizations.weight .  The original tensor will be accessible under module.parametrizations.weight.original .  Parametrizations may be concatenated by registering several parametrizations\\non the same attribute.  The training mode of a registered parametrization is updated on registration\\nto match the training mode of the host module  Parametrized parameters and buffers have an inbuilt caching system that can be activated\\nusing the context manager cached() .  A parametrization may optionally implement a method with signature  defright_inverse(self,X:Tensor)->Union[Tensor,Sequence[Tensor]]  This method is called on the unparametrized tensor when the first parametrization\\nis registered to compute the initial value of the original tensor.\\nIf this method is not implemented, the original tensor will be just the unparametrized tensor.  If all the parametrizations registered on a tensor implement right_inverse it is possible\\nto initialize a parametrized tensor by assigning to it, as shown in the example below.  It is possible for the first parametrization to depend on several inputs.\\nThis may be implemented returning a tuple of tensors from right_inverse (see the example implementation of a RankOne parametrization below).  In this case, the unconstrained tensors are also located under module.parametrizations.weight with names original0 , original1 ,…  Note  If unsafe=False (default) both the forward and right_inverse methods will be called\\nonce to perform a number of consistency checks.\\nIf unsafe=True, then right_inverse will be called if the tensor is not parametrized,\\nand nothing will be called otherwise.  Note  In most situations, right_inverse will be a function such that forward(right_inverse(X))==X (see right inverse ).\\nSometimes, when the parametrization is not surjective, it may be reasonable\\nto relax this.  Warning  If a parametrization depends on several inputs, register_parametrization() will register a number of new parameters. If such parametrization is registered\\nafter the optimizer is created, these new parameters will need to be added manually\\nto the optimizer. See torch.Optimizer.add_param_group() .  Parameters  module ( nn.Module ) – module on which to register the parametrization  tensor_name ( str ) – name of the parameter or buffer on which to register\\nthe parametrization  parametrization ( nn.Module ) – the parametrization to register  Keyword Arguments  unsafe ( bool ) – a boolean flag that denotes whether the parametrization\\nmay change the dtype and shape of the tensor. Default: False Warning: the parametrization is not checked for consistency upon registration.\\nEnable this flag at your own risk.  Raises  ValueError – if the module does not have a parameter or a buffer named tensor_name  Return type  Module  Examples  >>>importtorch>>>importtorch.nnasnn>>>importtorch.nn.utils.parametrizeasP>>>>>>classSymmetric(nn.Module):>>>defforward(self,X):>>>returnX.triu()+X.triu(1).T# Return a symmetric matrix>>>>>>defright_inverse(self,A):>>>returnA.triu()>>>>>>m=nn.Linear(5,5)>>>P.register_parametrization(m,\"weight\",Symmetric())>>>print(torch.allclose(m.weight,m.weight.T))# m.weight is now symmetricTrue>>>A=torch.rand(5,5)>>>A=A+A.T# A is now symmetric>>>m.weight=A# Initialize the weight to be the symmetric matrix A>>>print(torch.allclose(m.weight,A))True  >>>classRankOne(nn.Module):>>>defforward(self,x,y):>>># Form a rank 1 matrix multiplying two vectors>>>returnx.unsqueeze(-1)@y.unsqueeze(-2)>>>>>>defright_inverse(self,Z):>>># Project Z onto the rank 1 matrices>>>U,S,Vh=torch.linalg.svd(Z,full_matrices=False)>>># Return rescaled singular vectors>>>s0_sqrt=S[0].sqrt().unsqueeze(-1)>>>returnU[...,:,0]*s0_sqrt,Vh[...,0,:]*s0_sqrt>>>>>>linear_rank_one=P.register_parametrization(nn.Linear(4,4),\"weight\",RankOne())>>>print(torch.linalg.matrix_rank(linear_rank_one.weight).item())1\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch-nn-utils-parametrize-register-parametrization\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch-nn-utils-parametrize-register-parametrization', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html#register_parametrization', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrize.py#L417', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached', 'https://en.wikipedia.org/wiki/Inverse_function#Right_inverses', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/exceptions.html#ValueError', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a50'), 'title': 'parametrize.remove_parametrizations', 'page_text': 'torch.nn.utils.parametrize.remove_parametrizations [LINK_1]  torch.nn.utils.parametrize.remove_parametrizations( module , tensor_name , leave_parametrized=True ) [source]  [source]  [LINK_2]  Remove the parametrizations on a tensor in a module.  If leave_parametrized=True , module[tensor_name] will be set to\\nits current output. In this case, the parametrization shall not change the dtype of the tensor.  If leave_parametrized=False , module[tensor_name] will be set to\\nthe unparametrised tensor in module.parametrizations[tensor_name].original .\\nThis is only possible when the parametrization depends on just one tensor.  Parameters  module ( nn.Module ) – module from which remove the parametrization  tensor_name ( str ) – name of the parametrization to be removed  leave_parametrized ( bool  ,  optional ) – leave the attribute tensor_name parametrized.\\nDefault: True  Returns  module  Return type  Module  Raises  ValueError – if module[tensor_name] is not parametrized  ValueError – if leave_parametrized=False and the parametrization depends on several tensors\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch-nn-utils-parametrize-remove-parametrizations\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch-nn-utils-parametrize-remove-parametrizations', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html#remove_parametrizations', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrize.py#L653', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/exceptions.html#ValueError', 'https://docs.python.org/3/library/exceptions.html#ValueError', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a51'), 'title': 'parametrize.cached', 'page_text': 'torch.nn.utils.parametrize.cached [LINK_1]  torch.nn.utils.parametrize.cached() [source]  [source]  [LINK_2]  Context manager that enables the caching system within parametrizations registered with register_parametrization() .  The value of the parametrized objects is computed and cached the first time\\nthey are required when this context manager is active. The cached values are\\ndiscarded when leaving the context manager.  This is useful when using a parametrized parameter more than once in the forward pass.\\nAn example of this is when parametrizing the recurrent kernel of an RNN or when\\nsharing weights.  The simplest way to activate the cache is by wrapping the forward pass of the neural network  importtorch.nn.utils.parametrizeasP...withP.cached():output=model(inputs)  in training and evaluation. One may also wrap the parts of the modules that use\\nseveral times the parametrized tensors. For example, the loop of an RNN with a\\nparametrized recurrent kernel:  withP.cached():forxinxs:out_rnn=self.rnn_cell(x,out_rnn)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html#torch-nn-utils-parametrize-cached\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html#torch-nn-utils-parametrize-cached', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html#cached', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrize.py#L31', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.is_parametrized.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.remove_parametrizations.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a52'), 'title': 'parametrize.is_parametrized', 'page_text': 'torch.nn.utils.parametrize.is_parametrized [LINK_1]  torch.nn.utils.parametrize.is_parametrized( module , tensor_name=None ) [source]  [source]  [LINK_2]  Determine if a module has a parametrization.  Parameters  module ( nn.Module ) – module to query  tensor_name ( str  ,  optional ) – name of the parameter in the module\\nDefault: None  Returns  True if module has a parametrization for the parameter named tensor_name ,\\nor if it has any parametrization when tensor_name is None ;\\notherwise False  Return type  bool\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.is_parametrized.html#torch-nn-utils-parametrize-is-parametrized\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.is_parametrized.html#torch-nn-utils-parametrize-is-parametrized', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html#is_parametrized', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrize.py#L631', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.cached.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a53'), 'title': 'parametrize.ParametrizationList', 'page_text': 'ParametrizationList [LINK_1]  class torch.nn.utils.parametrize.ParametrizationList( modules , original , unsafe=False ) [source]  [source]  [LINK_2]  A sequential container that holds and manages the original parameters or buffers of a parametrized torch.nn.Module .  It is the type of module.parametrizations[tensor_name] when module[tensor_name] has been parametrized with register_parametrization() .  If the first registered parametrization has a right_inverse that returns one tensor or\\ndoes not have a right_inverse (in which case we assume that right_inverse is the identity),\\nit will hold the tensor under the name original .\\nIf it has a right_inverse that returns more than one tensor, these will be registered as original0 , original1 , …  Warning  This class is used internally by register_parametrization() . It is documented\\nhere for completeness. It shall not be instantiated by the user.  Parameters  modules ( sequence ) – sequence of modules representing the parametrizations  original ( Parameter  or  Tensor ) – parameter or buffer that is parametrized  unsafe ( bool ) – a boolean flag that denotes whether the parametrization\\nmay change the dtype and shape of the tensor. Default: False Warning: the parametrization is not checked for consistency upon registration.\\nEnable this flag at your own risk.  right_inverse( value ) [source]  [source]  [LINK_3]  Call the right_inverse methods of the parametrizations in the inverse registration order.  Then, it stores the result in self.original if right_inverse outputs one tensor\\nor in self.original0 , self.original1 , … if it outputs several.  Parameters  value ( Tensor ) – Value to which initialize the module\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#parametrizationlist\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList.right_inverse', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#parametrizationlist', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html#ParametrizationList', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrize.py#L92', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization', 'https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html#ParametrizationList.right_inverse', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/parametrize.py#L232', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList.right_inverse', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.stateless.functional_call.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.is_parametrized.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a54'), 'title': 'stateless.functional_call', 'page_text': 'torch.nn.utils.stateless.functional_call [LINK_1]  torch.nn.utils.stateless.functional_call( module , parameters_and_buffers , args=None , kwargs=None , * , tie_weights=True , strict=False ) [source]  [source]  [LINK_2]  Perform a functional call on the module by replacing the module parameters and buffers with the provided ones.  Warning  This API is deprecated as of PyTorch 2.0 and will be removed in a future\\nversion of PyTorch. Please use torch.func.functional_call() instead,\\nwhich is a drop-in replacement for this API.  Note  If the module has active parametrizations, passing a value in the parameters_and_buffers argument with the name set to the regular parameter\\nname will completely disable the parametrization.\\nIf you want to apply the parametrization function to the value passed\\nplease set the key as {submodule_name}.parametrizations.{parameter_name}.original .  Note  If the module performs in-place operations on parameters/buffers, these will be reflected\\nin the parameters_and_buffers input.  Example:  >>>a={\\'foo\\':torch.zeros(())}>>>mod=Foo()# does self.foo = self.foo + 1>>>print(mod.foo)# tensor(0.)>>>functional_call(mod,a,torch.ones(()))>>>print(mod.foo)# tensor(0.)>>>print(a[\\'foo\\'])# tensor(1.)  Note  If the module has tied weights, whether or not functional_call respects the tying is determined by the\\ntie_weights flag.  Example:  >>>a={\\'foo\\':torch.zeros(())}>>>mod=Foo()# has both self.foo and self.foo_tied which are tied. Returns x + self.foo + self.foo_tied>>>print(mod.foo)# tensor(1.)>>>mod(torch.zeros(()))# tensor(2.)>>>functional_call(mod,a,torch.zeros(()))# tensor(0.) since it will change self.foo_tied too>>>functional_call(mod,a,torch.zeros(()),tie_weights=False)# tensor(1.)--self.foo_tied is not updated>>>new_a={\\'foo\\':torch.zeros(()),\\'foo_tied\\':torch.zeros(())}>>>functional_call(mod,new_a,torch.zeros())# tensor(0.)  Parameters  module ( torch.nn.Module ) – the module to call  parameters_and_buffers ( dict  of  str and Tensor ) – the parameters that will be used in\\nthe module call.  args ( Any  or  tuple ) – arguments to be passed to the module call. If not a tuple, considered a single argument.  kwargs ( dict ) – keyword arguments to be passed to the module call  tie_weights ( bool  ,  optional ) – If True, then parameters and buffers tied in the original model will be treated as\\ntied in the reparamaterized version. Therefore, if True and different values are passed for the tied\\nparameters and buffers, it will error. If False, it will not respect the originally tied parameters and\\nbuffers unless the values passed for both weights are the same. Default: True.  strict ( bool  ,  optional ) – If True, then the parameters and buffers passed in must match the parameters and\\nbuffers in the original module. Therefore, if True and there are any missing or unexpected keys, it will\\nerror. Default: False.  Returns  the result of calling module .  Return type  Any\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.stateless.functional_call.html#torch-nn-utils-stateless-functional-call\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.stateless.functional_call.html#torch-nn-utils-stateless-functional-call', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/stateless.html#functional_call', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/stateless.py#L180', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call', 'https://pytorch.org/docs/stable/generated/torch.func.functional_call.html#torch.func.functional_call', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://docs.python.org/3/library/stdtypes.html#tuple', 'https://docs.python.org/3/library/stdtypes.html#dict', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrize.ParametrizationList.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a55'), 'title': 'nn.utils.rnn.PackedSequence', 'page_text': 'PackedSequence [LINK_1]  class torch.nn.utils.rnn.PackedSequence( data , batch_sizes=None , sorted_indices=None , unsorted_indices=None ) [source]  [source]  [LINK_2]  Holds the data and list of batch_sizes of a packed sequence.  All RNN modules accept packed sequences as inputs.  Note  Instances of this class should never be created manually. They are meant\\nto be instantiated by functions like pack_padded_sequence() .  Batch sizes represent the number elements at each sequence step in\\nthe batch, not the varying sequence lengths passed to pack_padded_sequence() .  For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1] .  Variables  data ( Tensor ) – Tensor containing packed sequence  batch_sizes ( Tensor ) – Tensor of integers holding\\ninformation about the batch size at each sequence step  sorted_indices ( Tensor  ,  optional ) – Tensor of integers holding how this PackedSequence is constructed from sequences.  unsorted_indices ( Tensor  ,  optional ) – Tensor of integers holding how this\\nto recover the original sequences with correct order.  Return type  Self  Note  data can be on arbitrary device and of arbitrary dtype. sorted_indices and unsorted_indices must be torch.int64 tensors on the same device as data .  However, batch_sizes should always be a CPU torch.int64 tensor.  This invariant is maintained throughout PackedSequence class,\\nand all functions that construct a PackedSequence in PyTorch\\n(i.e., they only pass in tensors conforming to this constraint).  batch_sizes :  Tensor  [LINK_3]  Alias for field number 1  count( value , / ) [LINK_4]  Return number of occurrences of value.  data :  Tensor  [LINK_5]  Alias for field number 0  index( value , start=0 , stop=9223372036854775807 , / ) [LINK_6]  Return first index of value.  Raises ValueError if the value is not present.  property is_cuda :  bool  [LINK_7]  Return true if self.data stored on a gpu.  is_pinned() [source]  [source]  [LINK_8]  Return true if self.data stored on in pinned memory.  Return type  bool  sorted_indices :  Optional [ Tensor ]  [LINK_9]  Alias for field number 2  to( dtype: dtype , non_blocking: bool = ... , copy: bool = ... )→Self [source]  [source]  [LINK_10]  to( device: Optional [ Union [ str ,  device ,  int ]] = ... , dtype: Optional [ dtype ] = ... , non_blocking: bool = ... , copy: bool = ... )→Self  to( other: Tensor , non_blocking: bool = ... , copy: bool = ... )→Self  Perform dtype and/or device conversion on self.data .  It has similar signature as torch.Tensor.to() , except optional\\narguments like non_blocking and copy should be passed as kwargs,\\nnot args, or they will not apply to the index tensors.  Note  If the self.data Tensor already has the correct torch.dtype and torch.device , then self is returned.\\nOtherwise, returns a copy with the desired configuration.  unsorted_indices :  Optional [ Tensor ]  [LINK_11]  Alias for field number 3\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#packedsequence\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.batch_sizes\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.count\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.data\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.index\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.is_cuda\\n [LINK_8]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.is_pinned\\n [LINK_9]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.sorted_indices\\n [LINK_10]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.to\\n [LINK_11]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.unsorted_indices', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#packedsequence', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#PackedSequence', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L48', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.batch_sizes', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.data', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.sorted_indices', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.unsorted_indices', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.data', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.batch_sizes', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.batch_sizes', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.count', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.data', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.index', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.is_cuda', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#PackedSequence.is_pinned', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L219', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.is_pinned', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.sorted_indices', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#PackedSequence.to', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L140', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.to', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/typing.html#typing.Union', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype', 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.device', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.unsorted_indices', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.stateless.functional_call.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a56'), 'title': 'nn.utils.rnn.pack_padded_sequence', 'page_text': 'torch.nn.utils.rnn.pack_padded_sequence [LINK_1]  torch.nn.utils.rnn.pack_padded_sequence( input , lengths , batch_first=False , enforce_sorted=True ) [source]  [source]  [LINK_2]  Packs a Tensor containing padded sequences of variable length.  input can be of size TxBx* (if batch_first is False )\\nor BxTx* (if batch_first is True ) where T is the length\\nof the longest sequence, B is the batch size, and * is any number of dimensions\\n(including 0).  For unsorted sequences, use enforce_sorted = False . If enforce_sorted is True , the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest\\none. enforce_sorted = True is only necessary for ONNX export.  It is an inverse operation to pad_packed_sequence() , and hence pad_packed_sequence() can be used to recover the underlying tensor packed in PackedSequence .  Note  This function accepts any input that has at least two dimensions. You\\ncan apply it to pack the labels, and use the output of the RNN with\\nthem to compute the loss directly. A Tensor can be retrieved from\\na PackedSequence object by accessing its .data attribute.  Parameters  input ( Tensor ) – padded batch of variable length sequences.  lengths ( Tensor  or  list  (  int  ) ) – list of sequence lengths of each batch\\nelement (must be on the CPU if provided as a tensor).  batch_first ( bool  ,  optional ) – if True , the input is expected in BxTx* format, TxBx* otherwise.  enforce_sorted ( bool  ,  optional ) – if True , the input is expected to\\ncontain sequences sorted by length in a decreasing order. If False , the input will get sorted unconditionally. Default: True .  Returns  a PackedSequence object  Return type  PackedSequence\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch-nn-utils-rnn-pack-padded-sequence\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch-nn-utils-rnn-pack-padded-sequence', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#pack_padded_sequence', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L280', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a57'), 'title': 'nn.utils.rnn.pad_packed_sequence', 'page_text': 'torch.nn.utils.rnn.pad_packed_sequence [LINK_1]  torch.nn.utils.rnn.pad_packed_sequence( sequence , batch_first=False , padding_value=0.0 , total_length=None ) [source]  [source]  [LINK_2]  Pad a packed batch of variable length sequences.  It is an inverse operation to pack_padded_sequence() .  The returned Tensor’s data will be of size TxBx* (if batch_first is False )\\nor BxTx* (if batch_first is True ) , where T is the length of the longest\\nsequence and B is the batch size.  Example  >>>fromtorch.nn.utils.rnnimportpack_padded_sequence,pad_packed_sequence>>>seq=torch.tensor([[1,2,0],[3,0,0],[4,5,6]])>>>lens=[2,1,3]>>>packed=pack_padded_sequence(seq,lens,batch_first=True,enforce_sorted=False)>>>packedPackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]),sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))>>>seq_unpacked,lens_unpacked=pad_packed_sequence(packed,batch_first=True)>>>seq_unpackedtensor([[1, 2, 0],[3, 0, 0],[4, 5, 6]])>>>lens_unpackedtensor([2, 1, 3])  Note  total_length is useful to implement the packsequence->recurrentnetwork->unpacksequence pattern in a Module wrapped in DataParallel .\\nSee this FAQ section for\\ndetails.  Parameters  sequence ( PackedSequence ) – batch to pad  batch_first ( bool  ,  optional ) – if True , the output will be in BxTx* format, TxBx* otherwise.  padding_value ( float  ,  optional ) – values for padded elements.  total_length ( int  ,  optional ) – if not None , the output will be padded to\\nhave length total_length . This method will throw ValueError if total_length is less than the max sequence length in sequence .  Returns  Tuple of Tensor containing the padded sequence, and a Tensor\\ncontaining the list of lengths of each sequence in the batch.\\nBatch elements will be re-ordered as they were ordered originally when\\nthe batch was passed to pack_padded_sequence or pack_sequence .  Return type  Tuple [ Tensor , Tensor ]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch-nn-utils-rnn-pad-packed-sequence\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch-nn-utils-rnn-pad-packed-sequence', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#pad_packed_sequence', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L345', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module', 'https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel', 'https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/exceptions.html#ValueError', 'https://docs.python.org/3/library/typing.html#typing.Tuple', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a58'), 'title': 'nn.utils.rnn.pad_sequence', 'page_text': 'torch.nn.utils.rnn.pad_sequence [LINK_1]  torch.nn.utils.rnn.pad_sequence( sequences , batch_first=False , padding_value=0.0 , padding_side=\\'right\\' ) [source]  [source]  [LINK_2]  Pad a list of variable length Tensors with padding_value .  pad_sequence stacks a list of Tensors along a new dimension, and pads them\\nto equal length. sequences can be list of sequences with size Lx* ,\\nwhere L is length of the sequence and * is any number of dimensions\\n(including 0). If batch_first is False , the output is of size TxBx* , and BxTx* otherwise, where B is the batch size\\n(the number of elements in sequences ), T is the length of the longest\\nsequence.  Example  >>>fromtorch.nn.utils.rnnimportpad_sequence>>>a=torch.ones(25,300)>>>b=torch.ones(22,300)>>>c=torch.ones(15,300)>>>pad_sequence([a,b,c]).size()torch.Size([25, 3, 300])  Note  This function returns a Tensor of size TxBx* or BxTx* where T is the length of the longest sequence. This function assumes\\ntrailing dimensions and type of all the Tensors in sequences are same.  Parameters  sequences ( list  [  Tensor  ] ) – list of variable length sequences.  batch_first ( bool  ,  optional ) – if True , the output will be in BxTx* format, TxBx* otherwise.  padding_value ( float  ,  optional ) – value for padded elements. Default: 0.  padding_side ( str  ,  optional ) – the side to pad the sequences on.\\nDefault: “right”.  Returns  Tensor of size TxBx* if batch_first is False .\\nTensor of size BxTx* otherwise  Return type  Tensor\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html#torch-nn-utils-rnn-pad-sequence\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html#torch-nn-utils-rnn-pad-sequence', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#pad_sequence', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L421', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a59'), 'title': 'nn.utils.rnn.pack_sequence', 'page_text': 'torch.nn.utils.rnn.pack_sequence [LINK_1]  torch.nn.utils.rnn.pack_sequence( sequences , enforce_sorted=True ) [source]  [source]  [LINK_2]  Packs a list of variable length Tensors.  Consecutive call of the next functions: pad_sequence , pack_padded_sequence .  sequences should be a list of Tensors of size Lx* , where L is\\nthe length of a sequence and * is any number of trailing dimensions,\\nincluding zero.  For unsorted sequences, use enforce_sorted = False . If enforce_sorted is True , the sequences should be sorted in the order of decreasing length. enforce_sorted=True is only necessary for ONNX export.  Example  >>>fromtorch.nn.utils.rnnimportpack_sequence>>>a=torch.tensor([1,2,3])>>>b=torch.tensor([4,5])>>>c=torch.tensor([6])>>>pack_sequence([a,b,c])PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)  Parameters  sequences ( list  [  Tensor  ] ) – A list of sequences of decreasing length.  enforce_sorted ( bool  ,  optional ) – if True , checks that the input\\ncontains sequences sorted by length in a decreasing order. If False , this condition is not checked. Default: True .  Returns  a PackedSequence object  Return type  PackedSequence\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch-nn-utils-rnn-pack-sequence\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch-nn-utils-rnn-pack-sequence', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#pack_sequence', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L535', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpack_sequence.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a5a'), 'title': 'nn.utils.rnn.unpack_sequence', 'page_text': 'torch.nn.utils.rnn.unpack_sequence [LINK_1]  torch.nn.utils.rnn.unpack_sequence( packed_sequences ) [source]  [source]  [LINK_2]  Unpack PackedSequence into a list of variable length Tensors.  packed_sequences should be a PackedSequence object.  Example  >>>fromtorch.nn.utils.rnnimportpack_sequence,unpack_sequence>>>a=torch.tensor([1,2,3])>>>b=torch.tensor([4,5])>>>c=torch.tensor([6])>>>sequences=[a,b,c]>>>print(sequences)[tensor([1, 2, 3]), tensor([4, 5]), tensor([6])]>>>packed_sequences=pack_sequence(sequences)>>>print(packed_sequences)PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)>>>unpacked_sequences=unpack_sequence(packed_sequences)>>>print(unpacked_sequences)[tensor([1, 2, 3]), tensor([4, 5]), tensor([6])]  Parameters  packed_sequences ( PackedSequence ) – A PackedSequence object.  Returns  a list of Tensor objects  Return type  List [ Tensor ]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpack_sequence.html#torch-nn-utils-rnn-unpack-sequence\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpack_sequence.html#torch.nn.utils.rnn.unpack_sequence', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpack_sequence.html#torch-nn-utils-rnn-unpack-sequence', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#unpack_sequence', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L574', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpack_sequence.html#torch.nn.utils.rnn.unpack_sequence', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence', 'https://docs.python.org/3/library/typing.html#typing.List', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpad_sequence.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a5b'), 'title': 'nn.utils.rnn.unpad_sequence', 'page_text': 'torch.nn.utils.rnn.unpad_sequence [LINK_1]  torch.nn.utils.rnn.unpad_sequence( padded_sequences , lengths , batch_first=False ) [source]  [source]  [LINK_2]  Unpad padded Tensor into a list of variable length Tensors.  unpad_sequence unstacks padded Tensor into a list of variable length Tensors.  Example  >>>fromtorch.nn.utils.rnnimportpad_sequence,unpad_sequence>>>a=torch.ones(25,300)>>>b=torch.ones(22,300)>>>c=torch.ones(15,300)>>>sequences=[a,b,c]>>>padded_sequences=pad_sequence(sequences)>>>lengths=torch.as_tensor([v.size(0)forvinsequences])>>>unpadded_sequences=unpad_sequence(padded_sequences,lengths)>>>torch.allclose(sequences[0],unpadded_sequences[0])True>>>torch.allclose(sequences[1],unpadded_sequences[1])True>>>torch.allclose(sequences[2],unpadded_sequences[2])True  Parameters  padded_sequences ( Tensor ) – padded sequences.  lengths ( Tensor ) – length of original (unpadded) sequences.  batch_first ( bool  ,  optional ) – whether batch dimension first or not. Default: False.  Returns  a list of Tensor objects  Return type  List [ Tensor ]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpad_sequence.html#torch-nn-utils-rnn-unpad-sequence\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpad_sequence.html#torch.nn.utils.rnn.unpad_sequence', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpad_sequence.html#torch-nn-utils-rnn-unpad-sequence', 'https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#unpad_sequence', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/rnn.py#L486', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpad_sequence.html#torch.nn.utils.rnn.unpad_sequence', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://docs.python.org/3/library/functions.html#bool', 'https://docs.python.org/3/library/typing.html#typing.List', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpack_sequence.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a5c'), 'title': 'nn.Flatten', 'page_text': 'Flatten [LINK_1]  class torch.nn.Flatten( start_dim=1 , end_dim=-1 ) [source]  [source]  [LINK_2]  Flattens a contiguous range of dims into a tensor.  For use with Sequential , see torch.flatten() for details.  Shape:  Input:(  ∗  ,  S  start  ,  .  .  .  ,  S  i  ,  .  .  .  ,  S  end  ,  ∗  )  (*, S_{\\\\text{start}},..., S_{i}, ..., S_{\\\\text{end}}, *)(∗,Sstart\\u200b,...,Si\\u200b,...,Send\\u200b,∗),’\\nwhereS  i  S_{i}Si\\u200bis the size at dimensioni  iiand∗  *∗means any\\nnumber of dimensions including none.  Output:(  ∗  ,  ∏  i  =  start  end  S  i  ,  ∗  )  (*, \\\\prod_{i=\\\\text{start}}^{\\\\text{end}} S_{i}, *)(∗,∏i=startend\\u200bSi\\u200b,∗).  Parameters  start_dim ( int ) – first dim to flatten (default = 1).  end_dim ( int ) – last dim to flatten (default = -1).  Examples::  >>>input=torch.randn(32,1,5,5)>>># With default parameters>>>m=nn.Flatten()>>>output=m(input)>>>output.size()torch.Size([32, 25])>>># With non-default parameters>>>m=nn.Flatten(0,2)>>>output=m(input)>>>output.size()torch.Size([160, 5])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#flatten\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#flatten', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/flatten.html#Flatten', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/flatten.py#L13', 'https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten', 'https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/functions.html#int', 'https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html', 'https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.unpad_sequence.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a5d'), 'title': 'nn.Unflatten', 'page_text': 'Unflatten [LINK_1]  class torch.nn.Unflatten( dim , unflattened_size ) [source]  [source]  [LINK_2]  Unflattens a tensor dim expanding it to a desired shape. For use with Sequential .  dim specifies the dimension of the input tensor to be unflattened, and it can\\nbe either int or str when Tensor or NamedTensor is used, respectively.  unflattened_size is the new shape of the unflattened dimension of the tensor and it can be\\na tuple of ints or a list of ints or torch.Size for Tensor input;  a NamedShape (tuple of (name, size) tuples) for NamedTensor input.  Shape:  Input:(  ∗  ,  S  dim  ,  ∗  )  (*, S_{\\\\text{dim}}, *)(∗,Sdim\\u200b,∗), whereS  dim  S_{\\\\text{dim}}Sdim\\u200bis the size at\\ndimension dim and∗  *∗means any number of dimensions including none.  Output:(  ∗  ,  U  1  ,  .  .  .  ,  U  n  ,  ∗  )  (*, U_1, ..., U_n, *)(∗,U1\\u200b,...,Un\\u200b,∗), whereU  UU= unflattened_size and∏  i  =  1  n  U  i  =  S  dim  \\\\prod_{i=1}^n U_i = S_{\\\\text{dim}}∏i=1n\\u200bUi\\u200b=Sdim\\u200b.  Parameters  dim ( Union  [  int  ,  str  ] ) – Dimension to be unflattened  unflattened_size ( Union  [  torch.Size  ,  Tuple  ,  List  ,  NamedShape  ] ) – New shape of the unflattened dimension  Examples  >>>input=torch.randn(2,50)>>># With tuple of ints>>>m=nn.Sequential(>>>nn.Linear(50,50),>>>nn.Unflatten(1,(2,5,5))>>>)>>>output=m(input)>>>output.size()torch.Size([2, 2, 5, 5])>>># With torch.Size>>>m=nn.Sequential(>>>nn.Linear(50,50),>>>nn.Unflatten(1,torch.Size([2,5,5]))>>>)>>>output=m(input)>>>output.size()torch.Size([2, 2, 5, 5])>>># With namedshape (tuple of tuples)>>>input=torch.randn(2,50,names=(\\'N\\',\\'features\\'))>>>unflatten=nn.Unflatten(\\'features\\',((\\'C\\',2),(\\'H\\',5),(\\'W\\',5)))>>>output=unflatten(input)>>>output.size()torch.Size([2, 2, 5, 5])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html#unflatten\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html#torch.nn.Unflatten', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html#unflatten', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/flatten.html#Unflatten', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/flatten.py#L59', 'https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html#torch.nn.Unflatten', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/size.html#torch.Size', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a5e'), 'title': 'nn.modules.lazy.LazyModuleMixin', 'page_text': 'LazyModuleMixin [LINK_1]  class torch.nn.modules.lazy.LazyModuleMixin( *args , **kwargs ) [source]  [source]  [LINK_2]  A mixin for modules that lazily initialize parameters, also known as “lazy modules”.  Modules that lazily initialize parameters, or “lazy modules”,\\nderive the shapes of their parameters from the first input(s)\\nto their forward method. Until that first forward they contain torch.nn.UninitializedParameter s that should not be accessed\\nor used, and afterward they contain regular torch.nn.Parameter s.\\nLazy modules are convenient since they don’t require computing some\\nmodule arguments, like the in_features argument of a\\ntypical torch.nn.Linear .  After construction, networks with lazy modules should first\\nbe converted to the desired dtype and placed on the expected device.\\nThis is because lazy modules only perform shape inference so the usual dtype\\nand device placement behavior applies.\\nThe lazy modules should then perform “dry runs” to initialize all the components in the module.\\nThese “dry runs” send inputs of the correct size, dtype, and device through\\nthe network and to each one of its lazy modules. After this the network can be used as usual.  >>>classLazyMLP(torch.nn.Module):...def__init__(self)->None:...super().__init__()...self.fc1=torch.nn.LazyLinear(10)...self.relu1=torch.nn.ReLU()...self.fc2=torch.nn.LazyLinear(1)...self.relu2=torch.nn.ReLU()......defforward(self,input):...x=self.relu1(self.fc1(input))...y=self.relu2(self.fc2(x))...returny>>># constructs a network with lazy modules>>>lazy_mlp=LazyMLP()>>># transforms the network\\'s device and dtype>>># NOTE: these transforms can and should be applied after construction and before any \\'dry runs\\'>>>lazy_mlp=lazy_mlp.cuda().double()>>>lazy_mlpLazyMLP( (fc1): LazyLinear(in_features=0, out_features=10, bias=True)(relu1): ReLU()(fc2): LazyLinear(in_features=0, out_features=1, bias=True)(relu2): ReLU())>>># performs a dry run to initialize the network\\'s lazy modules>>>lazy_mlp(torch.ones(10,10).cuda())>>># after initialization, LazyLinear modules become regular Linear modules>>>lazy_mlpLazyMLP((fc1): Linear(in_features=10, out_features=10, bias=True)(relu1): ReLU()(fc2): Linear(in_features=10, out_features=1, bias=True)(relu2): ReLU())>>># attaches an optimizer, since parameters can now be used as usual>>>optim=torch.optim.SGD(mlp.parameters(),lr=0.01)  A final caveat when using lazy modules is that the order of initialization of a network’s\\nparameters may change, since the lazy modules are always initialized after other modules.\\nFor example, if the LazyMLP class defined above had a torch.nn.LazyLinear module\\nfirst and then a regular torch.nn.Linear second, the second module would be\\ninitialized on construction and the first module would be initialized during the first dry run.\\nThis can cause the parameters of a network using lazy modules to be initialized differently\\nthan the parameters of a network without lazy modules as the order of parameter initializations,\\nwhich often depends on a stateful random number generator, is different.\\nCheck Reproducibility for more details.  Lazy modules can be serialized with a state dict like other modules. For example:  >>>lazy_mlp=LazyMLP()>>># The state dict shows the uninitialized parameters>>>lazy_mlp.state_dict()OrderedDict([(\\'fc1.weight\\', Uninitialized parameter),(\\'fc1.bias\\',tensor([-1.8832e+25,  4.5636e-41, -1.8832e+25,  4.5636e-41, -6.1598e-30,4.5637e-41, -1.8788e+22,  4.5636e-41, -2.0042e-31,  4.5637e-41])),(\\'fc2.weight\\', Uninitialized parameter),(\\'fc2.bias\\', tensor([0.0019]))])  Lazy modules can load regular torch.nn.Parameter s (i.e. you can serialize/deserialize\\ninitialized LazyModules and they will remain initialized)  >>>full_mlp=LazyMLP()>>># Dry run to initialize another module>>>full_mlp.forward(torch.ones(10,1))>>># Load an initialized state into a lazy module>>>lazy_mlp.load_state_dict(full_mlp.state_dict())>>># The state dict now holds valid values>>>lazy_mlp.state_dict()OrderedDict([(\\'fc1.weight\\',tensor([[-0.3837],[ 0.0907],[ 0.6708],[-0.5223],[-0.9028],[ 0.2851],[-0.4537],[ 0.6813],[ 0.5766],[-0.8678]])),(\\'fc1.bias\\',tensor([-1.8832e+25,  4.5636e-41, -1.8832e+25,  4.5636e-41, -6.1598e-30,4.5637e-41, -1.8788e+22,  4.5636e-41, -2.0042e-31,  4.5637e-41])),(\\'fc2.weight\\',tensor([[ 0.1320,  0.2938,  0.0679,  0.2793,  0.1088, -0.1795, -0.2301,  0.2807,0.2479,  0.1091]])),(\\'fc2.bias\\', tensor([0.0019]))])  Note, however, that the loaded parameters will not be replaced when doing a “dry run” if they are initialized\\nwhen the state is loaded. This prevents using initialized modules in different contexts.  has_uninitialized_params() [source]  [source]  [LINK_3]  Check if a module has parameters that are not initialized.  initialize_parameters( *args , **kwargs ) [source]  [source]  [LINK_4]  Initialize parameters according to the input batch properties.  This adds an interface to isolate parameter initialization from the\\nforward pass when doing parameter shape inference.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#lazymodulemixin\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#lazymodulemixin', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/lazy.html#LazyModuleMixin', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/lazy.py#L63', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/notes/randomness.html', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/lazy.html#LazyModuleMixin.has_uninitialized_params', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/lazy.py#L250', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/lazy.html#LazyModuleMixin.initialize_parameters', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/lazy.py#L240', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html', 'https://pytorch.org/docs/stable/generated/torch.nn.Unflatten.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n",
      "{'_id': ObjectId('67f0968102ab5481fe017a5f'), 'title': 'nn.modules.normalization.RMSNorm', 'page_text': 'RMSNorm [LINK_1]  class torch.nn.modules.normalization.RMSNorm( normalized_shape , eps=None , elementwise_affine=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Root Mean Square Layer Normalization over a mini-batch of inputs.  This layer implements the operation as described in\\nthe paper Root Mean Square Layer Normalization  y  i  =  x  i  R  M  S  (  x  )  ∗  γ  i  ,  where  RMS  (  x  )  =  ϵ  +  1  n  ∑  i  =  1  n  x  i  2  y_i = \\\\frac{x_i}{\\\\mathrm{RMS}(x)} * \\\\gamma_i, \\\\quad\\n\\\\text{where} \\\\quad \\\\text{RMS}(x) = \\\\sqrt{\\\\epsilon + \\\\frac{1}{n} \\\\sum_{i=1}^{n} x_i^2}yi\\u200b=RMS(x)xi\\u200b\\u200b∗γi\\u200b,whereRMS(x)=ϵ+n1\\u200bi=1∑n\\u200bxi2\\u200b\\u200b  The RMS is taken over the last D dimensions, where D is the dimension of normalized_shape . For example, if normalized_shape is (3,5) (a 2-dimensional shape), the RMS is computed over\\nthe last 2 dimensions of the input.  Parameters  normalized_shape ( int  or  list  or  torch.Size ) – input shape from an expected input\\nof size  [  ∗  ×  normalized_shape  [  0  ]  ×  normalized_shape  [  1  ]  ×  …  ×  normalized_shape  [  −  1  ]  ]  [* \\\\times \\\\text{normalized\\\\_shape}[0] \\\\times \\\\text{normalized\\\\_shape}[1]\\n    \\\\times \\\\ldots \\\\times \\\\text{normalized\\\\_shape}[-1]][∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]  If a single integer is used, it is treated as a singleton list, and this module will\\nnormalize over the last dimension which is expected to be of that specific size.  eps ( Optional  [  float  ] ) – a value added to the denominator for numerical stability. Default: torch.finfo(x.dtype).eps()  elementwise_affine ( bool ) – a boolean value that when set to True , this module\\nhas learnable per-element affine parameters initialized to ones (for weights). Default: True .  Shape:  Input:(  N  ,  ∗  )  (N, *)(N,∗)  Output:(  N  ,  ∗  )  (N, *)(N,∗)(same shape as input)  Examples:  >>>rms_norm=nn.RMSNorm([2,3])>>>input=torch.randn(2,2,3)>>>rms_norm(input)  extra_repr() [source]  [source]  [LINK_3]  Extra information about the module.  Return type  str  forward( x ) [source]  [source]  [LINK_4]  Runs forward pass.  Return type  Tensor  reset_parameters() [source]  [source]  [LINK_5]  Resets parameters based on their initialization used in __init__.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\\n\\nif (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }\\n\\n [LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#rmsnorm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.extra_repr\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.forward\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.reset_parameters', 'page_links': ['https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#rmsnorm', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#RMSNorm', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L321', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm', 'https://arxiv.org/pdf/1910.07467.pdf', 'https://docs.python.org/3/library/functions.html#int', 'https://docs.python.org/3/library/stdtypes.html#list', 'https://pytorch.org/docs/stable/size.html#torch.Size', 'https://docs.python.org/3/library/typing.html#typing.Optional', 'https://docs.python.org/3/library/functions.html#float', 'https://docs.python.org/3/library/functions.html#bool', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#RMSNorm.extra_repr', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L403', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.extra_repr', 'https://docs.python.org/3/library/stdtypes.html#str', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#RMSNorm.forward', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L397', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.forward', 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor', 'https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#RMSNorm.reset_parameters', 'https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/normalization.py#L390', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.reset_parameters', 'https://pytorch.org/docs/stable/nn.functional.html', 'https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html', 'http://sphinx-doc.org/', 'https://github.com/rtfd/sphinx_rtd_theme', 'https://readthedocs.org']}\n"
     ]
    }
   ],
   "source": [
    "for doc in collection.find():\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
