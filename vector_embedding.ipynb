{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to your MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"myDatabase\"]\n",
    "collection = db[\"myCollection\"]\n",
    "\n",
    "# Fetch your scraped documents (assume stored under 'content' field)\n",
    "documents = [doc['page_text'] for doc in collection.find()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk and Preprocess the data\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "docs = splitter.create_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Buffer [LINK_1]  class torch.nn.parameter.Buffer( data=None , * , persistent=True ) [source]  [source]  [LINK_2]  A kind of Tensor that should not be considered a model\\nparameter. For example, BatchNorm’s running_mean is not a parameter, but is part of the module’s state.  Buffers are Tensor subclasses, that have a\\nvery special property when used with Module s – when they’re\\nassigned as Module attributes they are automatically added to the list of'),\n",
       " Document(metadata={}, page_content='its buffers, and will appear e.g. in buffers() iterator.\\nAssigning a Tensor doesn’t have such effect. One can still assign a Tensor as explicitly by using\\nthe register_buffer() function.  Parameters  data ( Tensor ) – buffer tensor.  persistent ( bool  ,  optional ) – whether the buffer is part of the module’s state_dict . Default: True\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.Buffer.html#buffer\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.Buffer.html#torch.nn.parameter.Buffer'),\n",
       " Document(metadata={}, page_content='Parameter [LINK_1]  class torch.nn.parameter.Parameter( data=None , requires_grad=True ) [source]  [source]  [LINK_2]  A kind of Tensor that is to be considered a module parameter.  Parameters are Tensor subclasses, that have a\\nvery special property when used with Module s - when they’re\\nassigned as Module attributes they are automatically added to the list of\\nits parameters, and will appear e.g. in parameters() iterator.\\nAssigning a Tensor doesn’t have such effect. This is because one might'),\n",
       " Document(metadata={}, page_content='want to cache some temporary state, like last hidden state of the RNN, in\\nthe model. If there was no such class as Parameter , these\\ntemporaries would get registered too.  Parameters  data ( Tensor ) – parameter tensor.  requires_grad ( bool  ,  optional ) – if the parameter requires gradient. Note that\\nthe torch.no_grad() context does NOT affect the default behavior of'),\n",
       " Document(metadata={}, page_content='Parameter creation–the Parameter will still have requires_grad=True in no_grad mode. See Locally disabling gradient computation for more\\ndetails. Default: True\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter'),\n",
       " Document(metadata={}, page_content='UninitializedParameter [LINK_1]  class torch.nn.parameter.UninitializedParameter( requires_grad=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A parameter that is not initialized.  Uninitialized Parameters are a special case of torch.nn.Parameter where the shape of the data is still unknown.  Unlike a torch.nn.Parameter , uninitialized parameters\\nhold no data and attempting to access some properties, like their shape,'),\n",
       " Document(metadata={}, page_content=\"will throw a runtime error. The only operations that can be performed on a uninitialized\\nparameter are changing its datatype, moving it to a different device and\\nconverting it to a regular torch.nn.Parameter .  The default device or dtype to use when the parameter is materialized can be set\\nduring construction using e.g. device='cuda' .  cls_to_become [source]  [LINK_3]  alias of Parameter\"),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#uninitializedparameter\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter.cls_to_become'),\n",
       " Document(metadata={}, page_content='UninitializedBuffer [LINK_1]  class torch.nn.parameter.UninitializedBuffer( requires_grad=False , device=None , dtype=None , persistent=True ) [source]  [source]  [LINK_2]  A buffer that is not initialized.  Uninitialized Buffer is a a special case of torch.Tensor where the shape of the data is still unknown.  Unlike a torch.Tensor , uninitialized parameters\\nhold no data and attempting to access some properties, like their shape,'),\n",
       " Document(metadata={}, page_content=\"will throw a runtime error. The only operations that can be performed on a uninitialized\\nparameter are changing its datatype, moving it to a different device and\\nconverting it to a regular torch.Tensor .  The default device or dtype to use when the buffer is materialized can be set\\nduring construction using e.g. device='cuda' .\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\"),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html#uninitializedbuffer\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer'),\n",
       " Document(metadata={}, page_content='Module [LINK_1]  class torch.nn.Module( *args , **kwargs ) [source]  [source]  [LINK_2]  Base class for all neural network modules.  Your models should also subclass this class.  Modules can also contain other Modules, allowing them to be nested in'),\n",
       " Document(metadata={}, page_content='a tree structure. You can assign the submodules as regular attributes:  importtorch.nnasnnimporttorch.nn.functionalasFclassModel(nn.Module):def__init__(self)->None:super().__init__()self.conv1=nn.Conv2d(1,20,5)self.conv2=nn.Conv2d(20,20,5)defforward(self,x):x=F.relu(self.conv1(x))returnF.relu(self.conv2(x))  Submodules assigned in this way will be registered, and will also have their'),\n",
       " Document(metadata={}, page_content='parameters converted when you call to() , etc.  Note  As per the example above, an __init__() call to the parent class\\nmust be made before assignment on the child.  Variables  training ( bool ) – Boolean represents whether this module is in training or'),\n",
       " Document(metadata={}, page_content='evaluation mode.  add_module( name , module ) [source]  [source]  [LINK_3]  Add a child module to the current module.  The module can be accessed as an attribute using the given name.  Parameters  name ( str ) – name of the child module. The child module can be'),\n",
       " Document(metadata={}, page_content='accessed from this module using the given name  module ( Module ) – child module to be added to the module.  apply( fn ) [source]  [source]  [LINK_4]  Apply fn recursively to every submodule (as returned by .children() ) as well as self.  Typical use includes initializing the parameters of a model'),\n",
       " Document(metadata={}, page_content='(see also torch.nn.init ).  Parameters  fn ( Module -> None) – function to be applied to each submodule  Returns  self  Return type  Module  Example:  >>>@torch.no_grad()>>>definit_weights(m):>>>print(m)>>>iftype(m)==nn.Linear:>>>m.weight.fill_(1.0)>>>print(m.weight)>>>net=nn.Sequential(nn.Linear(2,2),nn.Linear(2,2))>>>net.apply(init_weights)Linear(in_features=2, out_features=2, bias=True)Parameter containing:tensor([[1., 1.],[1., 1.]], requires_grad=True)Linear(in_features=2, out_features=2,'),\n",
       " Document(metadata={}, page_content='out_features=2, bias=True)Parameter containing:tensor([[1., 1.],[1., 1.]], requires_grad=True)Sequential((0): Linear(in_features=2, out_features=2, bias=True)(1): Linear(in_features=2, out_features=2, bias=True))  bfloat16() [source]  [source]  [LINK_5]  Casts all floating point parameters and buffers to bfloat16 datatype.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  buffers( recurse=True ) [source]  [source]  [LINK_6]  Return an iterator over module'),\n",
       " Document(metadata={}, page_content='[LINK_6]  Return an iterator over module buffers.  Parameters  recurse ( bool ) – if True, then yields buffers of this module'),\n",
       " Document(metadata={}, page_content='and all submodules. Otherwise, yields only buffers that'),\n",
       " Document(metadata={}, page_content=\"are direct members of this module.  Yields  torch.Tensor – module buffer  Return type  Iterator [ Tensor ]  Example:  >>>forbufinmodel.buffers():>>>print(type(buf),buf.size())<class 'torch.Tensor'> (20L,)<class 'torch.Tensor'> (20L, 1L, 5L, 5L)  children() [source]  [source]  [LINK_7]  Return an iterator over immediate children modules.  Yields  Module – a child module  Return type  Iterator [ Module ]  compile( *args , **kwargs ) [source]  [source]  [LINK_8]  Compile this Module’s forward\"),\n",
       " Document(metadata={}, page_content='[source]  [LINK_8]  Compile this Module’s forward using torch.compile() .  This Module’s __call__ method is compiled and all arguments are passed as-is'),\n",
       " Document(metadata={}, page_content='to torch.compile() .  See torch.compile() for details on the arguments for this function.  cpu() [source]  [source]  [LINK_9]  Move all model parameters and buffers to the CPU.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  cuda( device=None ) [source]  [source]  [LINK_10]  Move all model parameters and buffers to the GPU.  This also makes associated parameters and buffers different objects. So'),\n",
       " Document(metadata={}, page_content='it should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.  Note  This method modifies the module in-place.  Parameters  device ( int  ,  optional ) – if specified, all parameters will be'),\n",
       " Document(metadata={}, page_content='copied to that device  Returns  self  Return type  Module  double() [source]  [source]  [LINK_11]  Casts all floating point parameters and buffers to double datatype.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  eval() [source]  [source]  [LINK_12]  Set the module in evaluation mode.  This has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation'),\n",
       " Document(metadata={}, page_content='mode, i.e. whether they are affected, e.g. Dropout , BatchNorm ,\\netc.  This is equivalent with self.train(False) .  See Locally disabling gradient computation for a comparison between .eval() and several similar mechanisms that may be confused with it.  Returns  self  Return type  Module  extra_repr() [source]  [source]  [LINK_13]  Return the extra representation of the module.  To print customized extra information, you should re-implement'),\n",
       " Document(metadata={}, page_content='this method in your own modules. Both single-line and multi-line\\nstrings are acceptable.  Return type  str  float() [source]  [source]  [LINK_14]  Casts all floating point parameters and buffers to float datatype.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  forward( *input ) [source]  [LINK_15]  Define the computation performed at every call.  Should be overridden by all subclasses.  Note  Although the recipe for forward pass needs to be defined within'),\n",
       " Document(metadata={}, page_content='this function, one should call the Module instance afterwards\\ninstead of this since the former takes care of running the\\nregistered hooks while the latter silently ignores them.  get_buffer( target ) [source]  [source]  [LINK_16]  Return the buffer given by target if it exists, otherwise throw an error.  See the docstring for get_submodule for a more detailed\\nexplanation of this method’s functionality as well as how to'),\n",
       " Document(metadata={}, page_content='correctly specify target .  Parameters  target ( str ) – The fully-qualified string name of the buffer\\nto look for. (See get_submodule for how to specify a\\nfully-qualified string.)  Returns  The buffer referenced by target  Return type  torch.Tensor  Raises  AttributeError – If the target string references an invalid\\n    path or resolves to something that is not a'),\n",
       " Document(metadata={}, page_content='path or resolves to something that is not a\\n    buffer  get_extra_state() [source]  [source]  [LINK_17]  Return any extra state to include in the module’s state_dict.  Implement this and a corresponding set_extra_state() for your module\\nif you need to store extra state. This function is called when building the\\nmodule’s state_dict() .  Note that extra state should be picklable to ensure working serialization\\nof the state_dict. We only provide backwards compatibility guarantees'),\n",
       " Document(metadata={}, page_content='for serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.  Returns  Any extra state to store in the module’s state_dict  Return type  object  get_parameter( target ) [source]  [source]  [LINK_18]  Return the parameter given by target if it exists, otherwise throw an error.  See the docstring for get_submodule for a more detailed\\nexplanation of this method’s functionality as well as how to'),\n",
       " Document(metadata={}, page_content='correctly specify target .  Parameters  target ( str ) – The fully-qualified string name of the Parameter\\nto look for. (See get_submodule for how to specify a\\nfully-qualified string.)  Returns  The Parameter referenced by target  Return type  torch.nn.Parameter  Raises  AttributeError – If the target string references an invalid'),\n",
       " Document(metadata={}, page_content='path or resolves to something that is not an nn.Parameter  get_submodule( target ) [source]  [source]  [LINK_19]  Return the submodule given by target if it exists, otherwise throw an error.  For example, let’s say you have an nn.Module  A that\\nlooks like this:  A(\\n    (net_b): Module(\\n        (net_c): Module(\\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n        )\\n        (linear): Linear(in_features=100, out_features=200, bias=True)\\n    )'),\n",
       " Document(metadata={}, page_content=')\\n)  (The diagram shows an nn.Module  A . A which has a nested\\nsubmodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .)  To check whether or not we have the linear submodule, we\\nwould call get_submodule(\"net_b.linear\") . To check whether\\nwe have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") .  The runtime of get_submodule is bounded by the degree'),\n",
       " Document(metadata={}, page_content='of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in\\nthe number of transitive modules. So, for a simple check to see\\nif some submodule exists, get_submodule should always be\\nused.  Parameters  target ( str ) – The fully-qualified string name of the submodule\\nto look for. (See above example for how to specify a'),\n",
       " Document(metadata={}, page_content='fully-qualified string.)  Returns  The submodule referenced by target  Return type  torch.nn.Module  Raises  AttributeError – If the target string references an invalid'),\n",
       " Document(metadata={}, page_content='path or resolves to something that is not an nn.Module  half() [source]  [source]  [LINK_20]  Casts all floating point parameters and buffers to half datatype.  Note  This method modifies the module in-place.  Returns  self  Return type  Module  ipu( device=None ) [source]  [source]  [LINK_21]  Move all model parameters and buffers to the IPU.  This also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will'),\n",
       " Document(metadata={}, page_content='live on IPU while being optimized.  Note  This method modifies the module in-place.  Parameters  device ( int  ,  optional ) – if specified, all parameters will be\\ncopied to that device  Returns  self  Return type  Module  load_state_dict( state_dict , strict=True , assign=False ) [source]  [source]  [LINK_22]  Copy parameters and buffers from state_dict into this module and its descendants.  If strict is True , then\\nthe keys of state_dict must exactly match the keys returned'),\n",
       " Document(metadata={}, page_content='by this module’s state_dict() function.  Warning  If assign is True the optimizer must be created after\\nthe call to load_state_dict unless get_swap_module_params_on_conversion() is True .  Parameters  state_dict ( dict ) – a dict containing parameters and\\npersistent buffers.  strict ( bool  ,  optional ) – whether to strictly enforce that the keys'),\n",
       " Document(metadata={}, page_content='in state_dict match the keys returned by this module’s state_dict() function. Default: True  assign ( bool  ,  optional ) – When set to False , the properties of the tensors\\nin the current module are preserved whereas setting it to True preserves\\nproperties of the Tensors in the state dict. The only'),\n",
       " Document(metadata={}, page_content='exception is the requires_grad field of Default:``False`  Returns  missing_keys is a list of str containing any keys that are expected  by this module but missing from the provided state_dict .  unexpected_keys is a list of str containing the keys that are not  expected by this module but present in the provided state_dict .  Return type  NamedTuple with missing_keys and unexpected_keys fields  Note  If a parameter or buffer is registered as None and its corresponding key'),\n",
       " Document(metadata={}, page_content='exists in state_dict , load_state_dict() will raise a RuntimeError .  modules() [source]  [source]  [LINK_23]  Return an iterator over all modules in the network.  Yields  Module – a module in the network  Return type  Iterator [ Module ]  Note  Duplicate modules are returned only once. In the following'),\n",
       " Document(metadata={}, page_content=\"example, l will be returned only once.  Example:  >>>l=nn.Linear(2,2)>>>net=nn.Sequential(l,l)>>>foridx,minenumerate(net.modules()):...print(idx,'->',m)0 -> Sequential((0): Linear(in_features=2, out_features=2, bias=True)(1): Linear(in_features=2, out_features=2, bias=True))1 -> Linear(in_features=2, out_features=2, bias=True)  mtia( device=None ) [source]  [source]  [LINK_24]  Move all model parameters and buffers to the MTIA.  This also makes associated parameters and buffers different\"),\n",
       " Document(metadata={}, page_content='makes associated parameters and buffers different objects. So'),\n",
       " Document(metadata={}, page_content='it should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.  Note  This method modifies the module in-place.  Parameters  device ( int  ,  optional ) – if specified, all parameters will be'),\n",
       " Document(metadata={}, page_content=\"copied to that device  Returns  self  Return type  Module  named_buffers( prefix='' , recurse=True , remove_duplicate=True ) [source]  [source]  [LINK_25]  Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters  prefix ( str ) – prefix to prepend to all buffer names.  recurse ( bool  ,  optional ) – if True, then yields buffers of this module\\nand all submodules. Otherwise, yields only buffers that\"),\n",
       " Document(metadata={}, page_content=\"are direct members of this module. Defaults to True.  remove_duplicate ( bool  ,  optional ) – whether to remove the duplicated buffers in the result. Defaults to True.  Yields  (str, torch.Tensor) – Tuple containing the name and buffer  Return type  Iterator [ Tuple [ str , Tensor ]]  Example:  >>>forname,bufinself.named_buffers():>>>ifnamein['running_var']:>>>print(buf.size())  named_children() [source]  [source]  [LINK_26]  Return an iterator over immediate children modules, yielding both\"),\n",
       " Document(metadata={}, page_content=\"over immediate children modules, yielding both the name of the module as well as the module itself.  Yields  (str, Module) – Tuple containing a name and child module  Return type  Iterator [ Tuple [ str , Module ]]  Example:  >>>forname,moduleinmodel.named_children():>>>ifnamein['conv4','conv5']:>>>print(module)  named_modules( memo=None , prefix='' , remove_duplicate=True ) [source]  [source]  [LINK_27]  Return an iterator over all modules in the network, yielding both the name of the module\"),\n",
       " Document(metadata={}, page_content='the network, yielding both the name of the module as well as the module itself.  Parameters  memo ( Optional  [  Set  [  Module  ]  ] ) – a memo to store the set of modules already added to the result  prefix ( str ) – a prefix that will be added to the name of the module  remove_duplicate ( bool ) – whether to remove the duplicated module instances in the result'),\n",
       " Document(metadata={}, page_content='or not  Yields  (str, Module) – Tuple of name and module  Note  Duplicate modules are returned only once. In the following'),\n",
       " Document(metadata={}, page_content=\"example, l will be returned only once.  Example:  >>>l=nn.Linear(2,2)>>>net=nn.Sequential(l,l)>>>foridx,minenumerate(net.named_modules()):...print(idx,'->',m)0 -> ('', Sequential((0): Linear(in_features=2, out_features=2, bias=True)(1): Linear(in_features=2, out_features=2, bias=True)))1 -> ('0', Linear(in_features=2, out_features=2, bias=True))  named_parameters( prefix='' , recurse=True , remove_duplicate=True ) [source]  [source]  [LINK_28]  Return an iterator over module parameters,\"),\n",
       " Document(metadata={}, page_content='Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters  prefix ( str ) – prefix to prepend to all parameter names.  recurse ( bool ) – if True, then yields parameters of this module'),\n",
       " Document(metadata={}, page_content='and all submodules. Otherwise, yields only parameters that\\nare direct members of this module.  remove_duplicate ( bool  ,  optional ) – whether to remove the duplicated'),\n",
       " Document(metadata={}, page_content=\"parameters in the result. Defaults to True.  Yields  (str, Parameter) – Tuple containing the name and parameter  Return type  Iterator [ Tuple [ str , Parameter ]]  Example:  >>>forname,paraminself.named_parameters():>>>ifnamein['bias']:>>>print(param.size())  parameters( recurse=True ) [source]  [source]  [LINK_29]  Return an iterator over module parameters.  This is typically passed to an optimizer.  Parameters  recurse ( bool ) – if True, then yields parameters of this module\"),\n",
       " Document(metadata={}, page_content=\"and all submodules. Otherwise, yields only parameters that\\nare direct members of this module.  Yields  Parameter – module parameter  Return type  Iterator [ Parameter ]  Example:  >>>forparaminmodel.parameters():>>>print(type(param),param.size())<class 'torch.Tensor'> (20L,)<class 'torch.Tensor'> (20L, 1L, 5L, 5L)  register_backward_hook( hook ) [source]  [source]  [LINK_30]  Register a backward hook on the module.  This function is deprecated in favor of register_full_backward_hook() and\"),\n",
       " Document(metadata={}, page_content='the behavior of this function will change in future versions.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_buffer( name , tensor , persistent=True ) [source]  [source]  [LINK_31]  Add a buffer to the module.  This is typically used to register a buffer that should not to be'),\n",
       " Document(metadata={}, page_content='considered a model parameter. For example, BatchNorm’s running_mean is not a parameter, but is part of the module’s state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting persistent to False . The\\nonly difference between a persistent buffer and a non-persistent buffer'),\n",
       " Document(metadata={}, page_content='is that the latter will not be a part of this module’s state_dict .  Buffers can be accessed as attributes using given names.  Parameters  name ( str ) – name of the buffer. The buffer can be accessed\\nfrom this module using the given name  tensor ( Tensor  or  None ) – buffer to be registered. If None , then operations\\nthat run on buffers, such as cuda , are ignored. If None ,'),\n",
       " Document(metadata={}, page_content=\"the buffer is not included in the module’s state_dict .  persistent ( bool ) – whether the buffer is part of this module’s state_dict .  Example:  >>>self.register_buffer('running_mean',torch.zeros(num_features))  register_forward_hook( hook , * , prepend=False , with_kwargs=False , always_call=False ) [source]  [source]  [LINK_32]  Register a forward hook on the module.  The hook will be called every time after forward() has computed an output.  If with_kwargs is False or not specified, the\"),\n",
       " Document(metadata={}, page_content='If with_kwargs is False or not specified, the input contains only'),\n",
       " Document(metadata={}, page_content='the positional arguments given to the module. Keyword arguments won’t be\\npassed to the hooks and only to the forward . The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after forward() is called. The hook\\nshould have the following signature:  hook(module,args,output)->Noneormodifiedoutput  If with_kwargs is True , the forward hook will be passed the kwargs given to the forward function and be expected to return the'),\n",
       " Document(metadata={}, page_content='output possibly modified. The hook should have the following signature:  hook(module,args,kwargs,output)->Noneormodifiedoutput  Parameters  hook ( Callable ) – The user defined hook to be registered.  prepend ( bool ) – If True , the provided hook will be fired\\nbefore all existing forward hooks on this torch.nn.modules.Module . Otherwise, the provided hook will be fired after all existing forward hooks on'),\n",
       " Document(metadata={}, page_content='this torch.nn.modules.Module . Note that global forward hooks registered with register_module_forward_hook() will fire before all hooks\\nregistered by this method.\\nDefault: False  with_kwargs ( bool ) – If True , the hook will be passed the\\nkwargs given to the forward function.\\nDefault: False  always_call ( bool ) – If True the hook will be run regardless of\\nwhether an exception is raised while calling the Module.'),\n",
       " Document(metadata={}, page_content='Default: False  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_forward_pre_hook( hook , * , prepend=False , with_kwargs=False ) [source]  [source]  [LINK_33]  Register a forward pre-hook on the module.  The hook will be called every time before forward() is invoked.  If with_kwargs is false or not specified, the input contains only'),\n",
       " Document(metadata={}, page_content='the positional arguments given to the module. Keyword arguments won’t be\\npassed to the hooks and only to the forward . The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature:  hook(module,args)->Noneormodifiedinput  If with_kwargs is true, the forward pre-hook will be passed the'),\n",
       " Document(metadata={}, page_content='kwargs given to the forward function. And if the hook modifies the\\ninput, both the args and kwargs should be returned. The hook should have\\nthe following signature:  hook(module,args,kwargs)->Noneoratupleofmodifiedinputandkwargs  Parameters  hook ( Callable ) – The user defined hook to be registered.  prepend ( bool ) – If true, the provided hook will be fired before'),\n",
       " Document(metadata={}, page_content='all existing forward_pre hooks on this torch.nn.modules.Module . Otherwise, the provided hook will be fired after all existing forward_pre hooks\\non this torch.nn.modules.Module . Note that global forward_pre hooks registered with register_module_forward_pre_hook() will fire before all\\nhooks registered by this method.\\nDefault: False  with_kwargs ( bool ) – If true, the hook will be passed the kwargs\\ngiven to the forward function.'),\n",
       " Document(metadata={}, page_content='given to the forward function.\\nDefault: False  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_full_backward_hook( hook , prepend=False ) [source]  [source]  [LINK_34]  Register a backward hook on the module.  The hook will be called every time the gradients with respect to a module\\nare computed, i.e. the hook will execute if and only if the gradients with'),\n",
       " Document(metadata={}, page_content='respect to module outputs are computed. The hook should have the following\\nsignature:  hook(module,grad_input,grad_output)->tuple(Tensor)orNone  The grad_input and grad_output are tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the input that will be used in place of grad_input in\\nsubsequent computations. grad_input will only correspond to the inputs given'),\n",
       " Document(metadata={}, page_content='as positional arguments and all kwarg arguments are ignored. Entries\\nin grad_input and grad_output will be None for all non-Tensor\\narguments.  For technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module’s forward function.  Warning  Modifying inputs or outputs inplace is not allowed when using backward hooks and'),\n",
       " Document(metadata={}, page_content='will raise an error.  Parameters  hook ( Callable ) – The user-defined hook to be registered.  prepend ( bool ) – If true, the provided hook will be fired before\\nall existing backward hooks on this torch.nn.modules.Module . Otherwise, the provided hook will be fired after all existing backward hooks on\\nthis torch.nn.modules.Module . Note that global backward hooks registered with register_module_full_backward_hook() will fire before'),\n",
       " Document(metadata={}, page_content='all hooks registered by this method.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_full_backward_pre_hook( hook , prepend=False ) [source]  [source]  [LINK_35]  Register a backward pre-hook on the module.  The hook will be called every time the gradients for the module are computed.'),\n",
       " Document(metadata={}, page_content='The hook should have the following signature:  hook(module,grad_output)->tuple[Tensor]orNone  The grad_output is a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the output that will be used in place of grad_output in\\nsubsequent computations. Entries in grad_output will be None for\\nall non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will'),\n",
       " Document(metadata={}, page_content='receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module’s forward function.  Warning  Modifying inputs inplace is not allowed when using backward hooks and\\nwill raise an error.  Parameters  hook ( Callable ) – The user-defined hook to be registered.  prepend ( bool ) – If true, the provided hook will be fired before'),\n",
       " Document(metadata={}, page_content='all existing backward_pre hooks on this torch.nn.modules.Module . Otherwise, the provided hook will be fired after all existing backward_pre hooks\\non this torch.nn.modules.Module . Note that global backward_pre hooks registered with register_module_full_backward_pre_hook() will fire before'),\n",
       " Document(metadata={}, page_content='all hooks registered by this method.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_load_state_dict_post_hook( hook ) [source]  [source]  [LINK_36]  Register a post-hook to be run after module’s load_state_dict() is called.  It should have the following signature::  hook(module, incompatible_keys) -> None  The module argument is the current module that this hook is registered'),\n",
       " Document(metadata={}, page_content='on, and the incompatible_keys argument is a NamedTuple consisting'),\n",
       " Document(metadata={}, page_content='of attributes missing_keys and unexpected_keys . missing_keys is a list of str containing the missing keys and unexpected_keys is a list of str containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling load_state_dict() with strict=True are affected by modifications the hook makes to missing_keys or unexpected_keys , as expected. Additions to either'),\n",
       " Document(metadata={}, page_content='set of keys will result in an error being thrown when strict=True , and'),\n",
       " Document(metadata={}, page_content='clearing out both missing and unexpected keys will avoid an error.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  register_load_state_dict_pre_hook( hook ) [source]  [source]  [LINK_37]  Register a pre-hook to be run before module’s load_state_dict() is called.  It should have the following signature::  hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) ->'),\n",
       " Document(metadata={}, page_content='missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950  Parameters  hook ( Callable ) – Callable hook that will be invoked before'),\n",
       " Document(metadata={}, page_content='loading the state dict.  register_module( name , module ) [source]  [source]  [LINK_38]  Alias for add_module() .  register_parameter( name , param ) [source]  [source]  [LINK_39]  Add a parameter to the module.  The parameter can be accessed as an attribute using given name.  Parameters  name ( str ) – name of the parameter. The parameter can be accessed'),\n",
       " Document(metadata={}, page_content='from this module using the given name  param ( Parameter  or  None ) – parameter to be added to the module. If None , then operations that run on parameters, such as cuda ,\\nare ignored. If None , the parameter is not included in the'),\n",
       " Document(metadata={}, page_content='module’s state_dict .  register_state_dict_post_hook( hook ) [source]  [source]  [LINK_40]  Register a post-hook for the state_dict() method.  It should have the following signature::  hook(module, state_dict, prefix, local_metadata) -> None  The registered hooks can modify the state_dict inplace.  register_state_dict_pre_hook( hook ) [source]  [source]  [LINK_41]  Register a pre-hook for the state_dict() method.  It should have the following signature::  hook(module, prefix, keep_vars) -> None'),\n",
       " Document(metadata={}, page_content='hook(module, prefix, keep_vars) -> None  The registered hooks can be used to perform pre-processing before the state_dict call is made.  requires_grad_( requires_grad=True ) [source]  [source]  [LINK_42]  Change if autograd should record operations on parameters in this module.  This method sets the parameters’ requires_grad attributes'),\n",
       " Document(metadata={}, page_content='in-place.  This method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).  See Locally disabling gradient computation for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it.  Parameters  requires_grad ( bool ) – whether autograd should record operations on'),\n",
       " Document(metadata={}, page_content='parameters in this module. Default: True .  Returns  self  Return type  Module  set_extra_state( state ) [source]  [source]  [LINK_43]  Set extra state contained in the loaded state_dict .  This function is called from load_state_dict() to handle any extra state'),\n",
       " Document(metadata={}, page_content='found within the state_dict . Implement this function and a corresponding get_extra_state() for your module if you need to store extra state within its state_dict .  Parameters  state ( dict ) – Extra state from the state_dict  set_submodule( target , module ) [source]  [source]  [LINK_44]  Set the submodule given by target if it exists, otherwise throw an error.  For example, let’s say you have an nn.Module  A that\\nlooks like this:  A(\\n    (net_b): Module(\\n        (net_c): Module('),\n",
       " Document(metadata={}, page_content='(net_b): Module(\\n        (net_c): Module(\\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n        )\\n        (linear): Linear(in_features=100, out_features=200, bias=True)\\n    )\\n)  (The diagram shows an nn.Module  A . A has a nested\\nsubmodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .)  To overide the Conv2d with a new submodule Linear , you'),\n",
       " Document(metadata={}, page_content='would call set_submodule(\"net_b.net_c.conv\",nn.Linear(33,16)) .  Parameters  target ( str ) – The fully-qualified string name of the submodule\\nto look for. (See above example for how to specify a\\nfully-qualified string.)  module ( Module ) – The module to set the submodule to.  Raises  ValueError – If the target string is empty  AttributeError – If the target string references an invalid'),\n",
       " Document(metadata={}, page_content=\"path or resolves to something that is not an nn.Module  share_memory() [source]  [source]  [LINK_45]  See torch.Tensor.share_memory_() .  Return type  T  state_dict( * , destination: T_destination , prefix: str = '' , keep_vars: bool = False )→T_destination [source]  [source]  [LINK_46]  state_dict( * , prefix: str = '' , keep_vars: bool = False )→Dict [ str ,  Any ]  Return a dictionary containing references to the whole state of the module.  Both parameters and persistent buffers (e.g.\"),\n",
       " Document(metadata={}, page_content='Both parameters and persistent buffers (e.g. running averages) are'),\n",
       " Document(metadata={}, page_content='included. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to None are not included.  Note  The returned object is a shallow copy. It contains references\\nto the module’s parameters and buffers.  Warning  Currently state_dict() also accepts positional arguments for destination , prefix and keep_vars in order. However,\\nthis is being deprecated and keyword arguments will be enforced in\\nfuture releases.  Warning  Please avoid the use of argument destination as it is not'),\n",
       " Document(metadata={}, page_content=\"designed for end-users.  Parameters  destination ( dict  ,  optional ) – If provided, the state of module will\\nbe updated into the dict and the same object is returned.\\nOtherwise, an OrderedDict will be created and returned.\\nDefault: None .  prefix ( str  ,  optional ) – a prefix added to parameter and buffer\\nnames to compose the keys in state_dict. Default: '' .  keep_vars ( bool  ,  optional ) – by default the Tensor s\\nreturned in the state dict are detached from autograd. If it’s\"),\n",
       " Document(metadata={}, page_content='set to True , detaching will not be performed.'),\n",
       " Document(metadata={}, page_content=\"Default: False .  Returns  a dictionary containing a whole state of the module  Return type  dict  Example:  >>>module.state_dict().keys()['bias', 'weight']  to( device: Optional [ Union [ str ,  device ,  int ]] = ... , dtype: Optional [ dtype ] = ... , non_blocking: bool = ... )→Self [source]  [source]  [LINK_47]  to( dtype: dtype , non_blocking: bool = ... )→Self  to( tensor: Tensor , non_blocking: bool = ... )→Self  Move and/or cast the parameters and buffers.  This can be called as  to(\"),\n",
       " Document(metadata={}, page_content='and buffers.  This can be called as  to( device=None , dtype=None , non_blocking=False ) [source]  [source]  to( dtype , non_blocking=False ) [source]  [source]  to( tensor , non_blocking=False ) [source]  [source]  to( memory_format=torch.channels_last ) [source]  [source]  Its signature is similar to torch.Tensor.to() , but only accepts'),\n",
       " Document(metadata={}, page_content='floating point or complex dtype s. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device , if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving CPU Tensors with'),\n",
       " Document(metadata={}, page_content='pinned memory to CUDA devices.  See below for examples.  Note  This method modifies the module in-place.  Parameters  device ( torch.device ) – the desired device of the parameters\\nand buffers in this module  dtype ( torch.dtype ) – the desired floating point or complex dtype of\\nthe parameters and buffers in this module  tensor ( torch.Tensor ) – Tensor whose dtype and device are the desired'),\n",
       " Document(metadata={}, page_content='dtype and device for all parameters and buffers in this module  memory_format ( torch.memory_format ) – the desired memory\\nformat for 4D parameters and buffers in this module (keyword'),\n",
       " Document(metadata={}, page_content='only argument)  Returns  self  Return type  Module  Examples:  >>>linear=nn.Linear(2,2)>>>linear.weightParameter containing:tensor([[ 0.1913, -0.3420],[-0.5113, -0.2325]])>>>linear.to(torch.double)Linear(in_features=2, out_features=2, bias=True)>>>linear.weightParameter containing:tensor([[ 0.1913, -0.3420],[-0.5113, -0.2325]], dtype=torch.float64)>>>gpu1=torch.device(\"cuda:1\")>>>linear.to(gpu1,dtype=torch.half,non_blocking=True)Linear(in_features=2, out_features=2,'),\n",
       " Document(metadata={}, page_content='out_features=2, bias=True)>>>linear.weightParameter containing:tensor([[ 0.1914, -0.3420],[-0.5112, -0.2324]], dtype=torch.float16, device=\\'cuda:1\\')>>>cpu=torch.device(\"cpu\")>>>linear.to(cpu)Linear(in_features=2, out_features=2, bias=True)>>>linear.weightParameter containing:tensor([[ 0.1914, -0.3420],[-0.5112, -0.2324]], dtype=torch.float16)>>>linear=nn.Linear(2,2,bias=None).to(torch.cdouble)>>>linear.weightParameter containing:tensor([[ 0.3741+0.j,  0.2382+0.j],[ 0.5593+0.j, -0.4443+0.j]],'),\n",
       " Document(metadata={}, page_content='0.2382+0.j],[ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)>>>linear(torch.ones(3,2,dtype=torch.cdouble))tensor([[0.6122+0.j, 0.1150+0.j],[0.6122+0.j, 0.1150+0.j],[0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)  to_empty( * , device , recurse=True ) [source]  [source]  [LINK_48]  Move the parameters and buffers to the specified device without copying storage.  Parameters  device ( torch.device ) – The desired device of the parameters'),\n",
       " Document(metadata={}, page_content='and buffers in this module.  recurse ( bool ) – Whether parameters and buffers of submodules should\\nbe recursively moved to the specified device.  Returns  self  Return type  Module  train( mode=True ) [source]  [source]  [LINK_49]  Set the module in training mode.  This has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g. Dropout , BatchNorm ,'),\n",
       " Document(metadata={}, page_content='etc.  Parameters  mode ( bool ) – whether to set training mode ( True ) or evaluation'),\n",
       " Document(metadata={}, page_content='mode ( False ). Default: True .  Returns  self  Return type  Module  type( dst_type ) [source]  [source]  [LINK_50]  Casts all parameters and buffers to dst_type .  Note  This method modifies the module in-place.  Parameters  dst_type ( type  or  string ) – the desired type  Returns  self  Return type  Module  xpu( device=None ) [source]  [source]  [LINK_51]  Move all model parameters and buffers to the XPU.  This also makes associated parameters and buffers different objects. So'),\n",
       " Document(metadata={}, page_content='it should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.  Note  This method modifies the module in-place.  Parameters  device ( int  ,  optional ) – if specified, all parameters will be'),\n",
       " Document(metadata={}, page_content='copied to that device  Returns  self  Return type  Module  zero_grad( set_to_none=True ) [source]  [source]  [LINK_52]  Reset gradients of all model parameters.  See similar function under torch.optim.Optimizer for more context.  Parameters  set_to_none ( bool ) – instead of setting to zero, set the grads to None.\\nSee torch.optim.Optimizer.zero_grad() for details.\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.add_module\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply\\n [LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.bfloat16'),\n",
       " Document(metadata={}, page_content='[LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.children\\n [LINK_8]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.compile\\n [LINK_9]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cpu\\n [LINK_10]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda'),\n",
       " Document(metadata={}, page_content='[LINK_11]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.double\\n [LINK_12]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval\\n [LINK_13]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.extra_repr\\n [LINK_14]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.float'),\n",
       " Document(metadata={}, page_content='[LINK_15]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward\\n [LINK_16]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_buffer\\n [LINK_17]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_extra_state\\n [LINK_18]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_parameter'),\n",
       " Document(metadata={}, page_content='[LINK_19]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_submodule\\n [LINK_20]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half\\n [LINK_21]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.ipu\\n [LINK_22]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict'),\n",
       " Document(metadata={}, page_content='[LINK_23]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.modules\\n [LINK_24]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.mtia\\n [LINK_25]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers\\n [LINK_26]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_children'),\n",
       " Document(metadata={}, page_content='[LINK_27]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_modules\\n [LINK_28]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters\\n [LINK_29]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters\\n [LINK_30]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_backward_hook'),\n",
       " Document(metadata={}, page_content='[LINK_31]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\\n [LINK_32]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook\\n [LINK_33]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook\\n [LINK_34]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook'),\n",
       " Document(metadata={}, page_content='[LINK_35]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook\\n [LINK_36]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_post_hook\\n [LINK_37]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_pre_hook\\n [LINK_38]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_module'),\n",
       " Document(metadata={}, page_content='[LINK_39]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_parameter\\n [LINK_40]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_post_hook\\n [LINK_41]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_pre_hook\\n [LINK_42]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.requires_grad_'),\n",
       " Document(metadata={}, page_content='[LINK_43]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.set_extra_state\\n [LINK_44]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.set_submodule\\n [LINK_45]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.share_memory\\n [LINK_46]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict'),\n",
       " Document(metadata={}, page_content='[LINK_47]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to\\n [LINK_48]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to_empty\\n [LINK_49]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train\\n [LINK_50]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.type\\n [LINK_51]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.xpu'),\n",
       " Document(metadata={}, page_content='[LINK_52]  : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.zero_grad'),\n",
       " Document(metadata={}, page_content='Sequential [LINK_1]  class torch.nn.Sequential( *args: Module ) [source]  [source]  [LINK_2]  class torch.nn.Sequential( arg: OrderedDict [ str ,  Module ] )  A sequential container.  Modules will be added to it in the order they are passed in the\\nconstructor. Alternatively, an OrderedDict of modules can be\\npassed in. The forward() method of Sequential accepts any\\ninput and forwards it to the first module it contains. It then\\n“chains” outputs to inputs sequentially for each subsequent module,'),\n",
       " Document(metadata={}, page_content='finally returning the output of the last module.  The value a Sequential provides over manually calling a sequence\\nof modules is that it allows treating the whole container as a\\nsingle module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are\\neach a registered submodule of the Sequential ).  What’s the difference between a Sequential and a torch.nn.ModuleList ? A ModuleList is exactly what it'),\n",
       " Document(metadata={}, page_content='sounds like–a list for storing Module s! On the other hand,'),\n",
       " Document(metadata={}, page_content='the layers in a Sequential are connected in a cascading way.  Example:  # Using Sequential to create a small model. When `model` is run,# input will first be passed to `Conv2d(1,20,5)`. The output of# `Conv2d(1,20,5)` will be used as the input to the first# `ReLU`; the output of the first `ReLU` will become the input# for `Conv2d(20,64,5)`. Finally, the output of# `Conv2d(20,64,5)` will be used as input to the second'),\n",
       " Document(metadata={}, page_content=\"will be used as input to the second `ReLU`model=nn.Sequential(nn.Conv2d(1,20,5),nn.ReLU(),nn.Conv2d(20,64,5),nn.ReLU())# Using Sequential with OrderedDict. This is functionally the# same as the above codemodel=nn.Sequential(OrderedDict([('conv1',nn.Conv2d(1,20,5)),('relu1',nn.ReLU()),('conv2',nn.Conv2d(20,64,5)),('relu2',nn.ReLU())]))  append( module ) [source]  [source]  [LINK_3]  Append a given module to the end.  Parameters  module ( nn.Module ) – module to append  Return type  Sequential\"),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#sequential\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential.append'),\n",
       " Document(metadata={}, page_content='ModuleList [LINK_1]  class torch.nn.ModuleList( modules=None ) [source]  [source]  [LINK_2]  Holds submodules in a list.  ModuleList can be indexed like a regular Python list, but'),\n",
       " Document(metadata={}, page_content='modules it contains are properly registered, and will be visible by all Module methods.  Parameters  modules ( iterable  ,  optional ) – an iterable of modules to add  Example:  classMyModule(nn.Module):def__init__(self)->None:super().__init__()self.linears=nn.ModuleList([nn.Linear(10,10)foriinrange(10)])defforward(self,x):# ModuleList can act as an iterable, or be indexed using intsfori,linenumerate(self.linears):x=self.linears[i//2](x)+l(x)returnx  append( module ) [source]  [source]'),\n",
       " Document(metadata={}, page_content='append( module ) [source]  [source]  [LINK_3]  Append a given module to the end of the list.  Parameters  module ( nn.Module ) – module to append  Return type  ModuleList  extend( modules ) [source]  [source]  [LINK_4]  Append modules from a Python iterable to the end of the list.  Parameters  modules ( iterable ) – iterable of modules to append  Return type  Self  insert( index , module ) [source]  [source]  [LINK_5]  Insert a given module before a given index in the list.  Parameters  index'),\n",
       " Document(metadata={}, page_content='a given index in the list.  Parameters  index ( int ) – index to insert.  module ( nn.Module ) – module to insert'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#modulelist\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.append\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.extend'),\n",
       " Document(metadata={}, page_content='[LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList.insert'),\n",
       " Document(metadata={}, page_content='ModuleDict [LINK_1]  class torch.nn.ModuleDict( modules=None ) [source]  [source]  [LINK_2]  Holds submodules in a dictionary.  ModuleDict can be indexed like a regular Python dictionary,'),\n",
       " Document(metadata={}, page_content='but modules it contains are properly registered, and will be visible by all Module methods.  ModuleDict is an ordered dictionary that respects  the order of insertion, and  in update() , the order of the merged OrderedDict , dict (started from Python 3.6) or another ModuleDict (the argument to update() ).  Note that update() with other unordered mapping\\ntypes (e.g., Python’s plain dict before Python version 3.6) does not'),\n",
       " Document(metadata={}, page_content='preserve the order of the merged mapping.  Parameters  modules ( iterable  ,  optional ) – a mapping (dictionary) of (string: module)'),\n",
       " Document(metadata={}, page_content=\"or an iterable of key-value pairs of type (string, module)  Example:  classMyModule(nn.Module):def__init__(self)->None:super().__init__()self.choices=nn.ModuleDict({'conv':nn.Conv2d(10,10,3),'pool':nn.MaxPool2d(3)})self.activations=nn.ModuleDict([['lrelu',nn.LeakyReLU()],['prelu',nn.PReLU()]])defforward(self,x,choice,act):x=self.choices[choice](x)x=self.activations[act](x)returnx  clear() [source]  [source]  [LINK_3]  Remove all items from the ModuleDict.  items() [source]  [source]  [LINK_4]\"),\n",
       " Document(metadata={}, page_content='items() [source]  [source]  [LINK_4]  Return an iterable of the ModuleDict key/value pairs.  Return type  Iterable [ Tuple [ str , Module ]]  keys() [source]  [source]  [LINK_5]  Return an iterable of the ModuleDict keys.  Return type  Iterable [ str ]  pop( key ) [source]  [source]  [LINK_6]  Remove key from the ModuleDict and return its module.  Parameters  key ( str ) – key to pop from the ModuleDict  Return type  Module  update( modules ) [source]  [source]  [LINK_7]  Update the ModuleDict'),\n",
       " Document(metadata={}, page_content='[source]  [LINK_7]  Update the ModuleDict with key-value pairs from a mapping, overwriting existing keys.  Note  If modules is an OrderedDict , a ModuleDict , or'),\n",
       " Document(metadata={}, page_content='an iterable of key-value pairs, the order of new elements in it is preserved.  Parameters  modules ( iterable ) – a mapping (dictionary) from string to Module ,\\nor an iterable of key-value pairs of type (string, Module )  values() [source]  [source]  [LINK_8]  Return an iterable of the ModuleDict values.  Return type  Iterable [ Module ]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#moduledict\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.clear\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.items'),\n",
       " Document(metadata={}, page_content='[LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.keys\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.pop\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.update\\n [LINK_8]  : https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.values'),\n",
       " Document(metadata={}, page_content='ParameterList [LINK_1]  class torch.nn.ParameterList( values=None ) [source]  [source]  [LINK_2]  Holds parameters in a list.  ParameterList can be used like a regular Python\\nlist, but Tensors that are Parameter are properly registered,'),\n",
       " Document(metadata={}, page_content='and will be visible by all Module methods.  Note that the constructor, assigning an element of the list, the append() method and the extend() method will convert any Tensor into Parameter .  Parameters  parameters ( iterable  ,  optional ) – an iterable of elements to add to the list.  Example:  classMyModule(nn.Module):def__init__(self)->None:super().__init__()self.params=nn.ParameterList([nn.Parameter(torch.randn(10,10))foriinrange(10)])defforward(self,x):# ParameterList can act as an'),\n",
       " Document(metadata={}, page_content='ParameterList can act as an iterable, or be indexed using intsfori,pinenumerate(self.params):x=self.params[i//2].mm(x)+p.mm(x)returnx  append( value ) [source]  [source]  [LINK_3]  Append a given value at the end of the list.  Parameters  value ( Any ) – value to append  Return type  ParameterList  extend( values ) [source]  [source]  [LINK_4]  Append values from a Python iterable to the end of the list.  Parameters  values ( iterable ) – iterable of values to append  Return type  Self'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#parameterlist\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.append\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList.extend'),\n",
       " Document(metadata={}, page_content='ParameterDict [LINK_1]  class torch.nn.ParameterDict( parameters=None ) [source]  [source]  [LINK_2]  Holds parameters in a dictionary.  ParameterDict can be indexed like a regular Python dictionary, but Parameters it\\ncontains are properly registered, and will be visible by all Module methods.\\nOther objects are treated as would be done by a regular Python dictionary  ParameterDict is an ordered dictionary. update() with other unordered mapping'),\n",
       " Document(metadata={}, page_content='types (e.g., Python’s plain dict ) does not preserve the order of the\\nmerged mapping. On the other hand, OrderedDict or another ParameterDict will preserve their ordering.  Note that the constructor, assigning an element of the dictionary and the update() method will convert any Tensor into Parameter .  Parameters  values ( iterable  ,  optional ) – a mapping (dictionary) of\\n(string : Any) or an iterable of key-value pairs'),\n",
       " Document(metadata={}, page_content=\"of type (string, Any)  Example:  classMyModule(nn.Module):def__init__(self)->None:super().__init__()self.params=nn.ParameterDict({'left':nn.Parameter(torch.randn(5,10)),'right':nn.Parameter(torch.randn(5,10))})defforward(self,x,choice):x=self.params[choice].mm(x)returnx  clear() [source]  [source]  [LINK_3]  Remove all items from the ParameterDict.  copy() [source]  [source]  [LINK_4]  Return a copy of this ParameterDict instance.  Return type  ParameterDict  fromkeys( keys , default=None )\"),\n",
       " Document(metadata={}, page_content='ParameterDict  fromkeys( keys , default=None ) [source]  [source]  [LINK_5]  Return a new ParameterDict with the keys provided.  Parameters  keys ( iterable  ,  string ) – keys to make the new ParameterDict from  default ( Parameter  ,  optional ) – value to set for all keys  Return type  ParameterDict  get( key , default=None ) [source]  [source]  [LINK_6]  Return the parameter associated with key if present. Otherwise return default if provided, None if not.  Parameters  key ( str ) – key to'),\n",
       " Document(metadata={}, page_content='None if not.  Parameters  key ( str ) – key to get from the ParameterDict  default ( Parameter  ,  optional ) – value to return if key not present  Return type  Any  items() [source]  [source]  [LINK_7]  Return an iterable of the ParameterDict key/value pairs.  Return type  Iterable [ Tuple [ str , Any ]]  keys() [source]  [source]  [LINK_8]  Return an iterable of the ParameterDict keys.  Return type  Iterable [ str ]  pop( key ) [source]  [source]  [LINK_9]  Remove key from the ParameterDict'),\n",
       " Document(metadata={}, page_content='[LINK_9]  Remove key from the ParameterDict and return its parameter.  Parameters  key ( str ) – key to pop from the ParameterDict  Return type  Any  popitem() [source]  [source]  [LINK_10]  Remove and return the last inserted (key, parameter) pair from the ParameterDict.  Return type  Tuple [ str , Any ]  setdefault( key , default=None ) [source]  [source]  [LINK_11]  Set the default for a key in the Parameterdict.  If key is in the ParameterDict, return its value.'),\n",
       " Document(metadata={}, page_content='If not, insert key with a parameter default and return default . default defaults to None .  Parameters  key ( str ) – key to set default for  default ( Any ) – the parameter set to the key  Return type  Any  update( parameters ) [source]  [source]  [LINK_12]  Update the ParameterDict with key-value pairs from parameters , overwriting existing keys.  Note  If parameters is an OrderedDict , a ParameterDict , or'),\n",
       " Document(metadata={}, page_content='an iterable of key-value pairs, the order of new elements in it is preserved.  Parameters  parameters ( iterable ) – a mapping (dictionary) from string to Parameter , or an iterable of\\nkey-value pairs of type (string, Parameter )  values() [source]  [source]  [LINK_13]  Return an iterable of the ParameterDict values.  Return type  Iterable [ Any ]\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#parameterdict\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.clear\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.copy'),\n",
       " Document(metadata={}, page_content='[LINK_5]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.fromkeys\\n [LINK_6]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.get\\n [LINK_7]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.items\\n [LINK_8]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.keys'),\n",
       " Document(metadata={}, page_content='[LINK_9]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.pop\\n [LINK_10]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.popitem\\n [LINK_11]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.setdefault\\n [LINK_12]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.update'),\n",
       " Document(metadata={}, page_content='[LINK_13]  : https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.values'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_forward_pre_hook [LINK_1]  torch.nn.modules.module.register_module_forward_pre_hook( hook ) [source]  [source]  [LINK_2]  Register a forward pre-hook common to all modules.  Warning  This adds global state to the nn.module module\\nand it is only intended for debugging/profiling purposes.  The hook will be called every time before forward() is invoked.'),\n",
       " Document(metadata={}, page_content='It should have the following signature:  hook(module,input)->Noneormodifiedinput  The input contains only the positional arguments given to the module.\\nKeyword arguments won’t be passed to the hooks and only to the forward .\\nThe hook can modify the input. User can either return a tuple or a\\nsingle modified value in the hook. We will wrap the value into a tuple'),\n",
       " Document(metadata={}, page_content='if a single value is returned(unless that value is already a tuple).  This hook has precedence over the specific module hooks registered with register_forward_pre_hook .  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);'),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch-nn-modules-module-register-module-forward-pre-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_forward_hook [LINK_1]  torch.nn.modules.module.register_module_forward_hook( hook , * , with_kwargs=False , always_call=False ) [source]  [source]  [LINK_2]  Register a global forward hook for all the modules.  Warning  This adds global state to the nn.module module\\nand it is only intended for debugging/profiling purposes.  The hook will be called every time after forward() has computed an output.'),\n",
       " Document(metadata={}, page_content='It should have the following signature:  hook(module,input,output)->Noneormodifiedoutput  The input contains only the positional arguments given to the module.\\nKeyword arguments won’t be passed to the hooks and only to the forward .\\nYou can optionally modify the output of the module by returning a new value'),\n",
       " Document(metadata={}, page_content='that will replace the output from the forward() function.  Parameters  hook ( Callable ) – The user defined hook to be registered.  always_call ( bool ) – If True the hook will be run regardless of\\nwhether an exception is raised while calling the Module.'),\n",
       " Document(metadata={}, page_content='Default: False  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle  This hook will be executed before specific module hooks registered with register_forward_hook .\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);'),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html#torch-nn-modules-module-register-module-forward-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_backward_hook [LINK_1]  torch.nn.modules.module.register_module_backward_hook( hook ) [source]  [source]  [LINK_2]  Register a backward hook common to all the modules.  This function is deprecated in favor of torch.nn.modules.module.register_module_full_backward_hook() and the behavior of this function will change in future versions.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type'),\n",
       " Document(metadata={}, page_content='hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html#torch-nn-modules-module-register-module-backward-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_full_backward_pre_hook [LINK_1]  torch.nn.modules.module.register_module_full_backward_pre_hook( hook ) [source]  [source]  [LINK_2]  Register a backward pre-hook common to all the modules.  Warning  This adds global state to the nn.module module\\nand it is only intended for debugging/profiling purposes.  Hooks registered using this function behave in the same way as those\\nregistered by torch.nn.Module.register_full_backward_pre_hook() .'),\n",
       " Document(metadata={}, page_content='Refer to its documentation for more details.  Hooks registered using this function will be called before hooks registered\\nusing torch.nn.Module.register_full_backward_pre_hook() .  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch-nn-modules-module-register-module-full-backward-pre-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_full_backward_hook [LINK_1]  torch.nn.modules.module.register_module_full_backward_hook( hook ) [source]  [source]  [LINK_2]  Register a backward hook common to all the modules.  Warning  This adds global state to the nn.module module\\nand it is only intended for debugging/profiling purposes.  Hooks registered using this function behave in the same way as those\\nregistered by torch.nn.Module.register_full_backward_hook() .'),\n",
       " Document(metadata={}, page_content='Refer to its documentation for more details.  Hooks registered using this function will be called before hooks registered\\nusing torch.nn.Module.register_full_backward_hook() .  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch-nn-modules-module-register-module-full-backward-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_buffer_registration_hook [LINK_1]  torch.nn.modules.module.register_module_buffer_registration_hook( hook ) [source]  [source]  [LINK_2]  Register a buffer registration hook common to all modules.  Warning  This adds global state to the nn.Module module  The hook will be called every time register_buffer() is invoked.'),\n",
       " Document(metadata={}, page_content='It should have the following signature:  hook(module,name,buffer)->Noneornewbuffer  The hook can modify the input or return a single modified value in the hook.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);'),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch-nn-modules-module-register-module-buffer-registration-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_module_registration_hook [LINK_1]  torch.nn.modules.module.register_module_module_registration_hook( hook ) [source]  [source]  [LINK_2]  Register a module registration hook common to all modules.  Warning  This adds global state to the nn.Module module  The hook will be called every time register_module() is invoked.'),\n",
       " Document(metadata={}, page_content='It should have the following signature:  hook(module,name,submodule)->Noneornewsubmodule  The hook can modify the input or return a single modified value in the hook.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);'),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch-nn-modules-module-register-module-module-registration-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_parameter_registration_hook [LINK_1]  torch.nn.modules.module.register_module_parameter_registration_hook( hook ) [source]  [source]  [LINK_2]  Register a parameter registration hook common to all modules.  Warning  This adds global state to the nn.Module module  The hook will be called every time register_parameter() is invoked.'),\n",
       " Document(metadata={}, page_content='It should have the following signature:  hook(module,name,param)->Noneornewparameter  The hook can modify the input or return a single modified value in the hook.  Returns  a handle that can be used to remove the added hook by calling handle.remove()  Return type  torch.utils.hooks.RemovableHandle\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);'),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch-nn-modules-module-register-module-parameter-registration-hook\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook'),\n",
       " Document(metadata={}, page_content=\"Conv1d [LINK_1]  class torch.nn.Conv1d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 1D convolution over an input signal composed of several input\"),\n",
       " Document(metadata={}, page_content='planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  in  ,  L  )  (N, C_{\\\\text{in}}, L)(N,Cin\\u200b,L)and output(  N  ,  C  out  ,  L  out  )  (N, C_{\\\\text{out}}, L_{\\\\text{out}})(N,Cout\\u200b,Lout\\u200b)can be\\nprecisely described as:  out  (  N  i  ,  C  out  j  )  =  bias  (  C  out  j  )  +  ∑  k  =  0  C  i  n  −  1  weight  (  C  out  j  ,  k  )  ⋆  input  (  N  i  ,  k  )  \\\\text{out}(N_i, C_{\\\\text{out}_j}) = \\\\text{bias}(C_{\\\\text{out}_j}) +'),\n",
       " Document(metadata={}, page_content='\\\\sum_{k = 0}^{C_{in} - 1} \\\\text{weight}(C_{\\\\text{out}_j}, k)'),\n",
       " Document(metadata={}, page_content='\\\\star \\\\text{input}(N_i, k)out(Ni\\u200b,Coutj\\u200b\\u200b)=bias(Coutj\\u200b\\u200b)+k=0∑Cin\\u200b−1\\u200bweight(Coutj\\u200b\\u200b,k)⋆input(Ni\\u200b,k)  where⋆  \\\\star⋆is the valid cross-correlation operator,N  NNis a batch size,C  CCdenotes a number of channels,L  LLis a length of signal sequence.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation, a single'),\n",
       " Document(metadata={}, page_content='number or a one-element tuple.  padding controls the amount of padding applied to the input. It\\ncan be either a string {‘valid’, ‘same’} or a tuple of ints giving the\\namount of implicit padding applied on both sides.  dilation controls the spacing between the kernel points; also'),\n",
       " Document(metadata={}, page_content='known as the à trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently'),\n",
       " Document(metadata={}, page_content='concatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  Note  When groups == in_channels and out_channels == K * in_channels ,\\nwhere K is a positive integer, this operation is also known as a “depthwise convolution”.  In other words, for an input of size(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b),'),\n",
       " Document(metadata={}, page_content='a depthwise convolution with a depthwise multiplier K can be performed with the arguments(  C  in  =  C  in  ,  C  out  =  C  in  ×  K  ,  .  .  .  ,  groups  =  C  in  )  (C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})(Cin\\u200b=Cin\\u200b,Cout\\u200b=Cin\\u200b×K,...,groups=Cin\\u200b).  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable,'),\n",
       " Document(metadata={}, page_content=\"to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Note  padding='valid' is the same as no padding. padding='same' pads\"),\n",
       " Document(metadata={}, page_content='the input so the output has the shape as the input. However, this mode'),\n",
       " Document(metadata={}, page_content='doesn’t support any stride values other than 1.  Note  This module supports complex data types i.e. complex32,complex64,complex128 .  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  ,  tuple  or  str  ,  optional ) – Padding added to both sides'),\n",
       " Document(metadata={}, page_content='str  ,  optional ) – Padding added to both sides of'),\n",
       " Document(metadata={}, page_content='the input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel\\nelements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the'),\n",
       " Document(metadata={}, page_content=\"output. Default: True  padding_mode ( str  ,  optional ) – 'zeros' , 'reflect' , 'replicate' or 'circular' . Default: 'zeros'  Shape:  Input:(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b)or(  C  i  n  ,  L  i  n  )  (C_{in}, L_{in})(Cin\\u200b,Lin\\u200b)  Output:(  N  ,  C  o  u  t  ,  L  o  u  t  )  (N, C_{out}, L_{out})(N,Cout\\u200b,Lout\\u200b)or(  C  o  u  t  ,  L  o  u  t  )  (C_{out}, L_{out})(Cout\\u200b,Lout\\u200b), where  L  o  u  t  =  ⌊  L  i  n  +  2  ×  padding  −  dilation  ×  (  kernel_size  −\"),\n",
       " Document(metadata={}, page_content='2  ×  padding  −  dilation  ×  (  kernel_size  −  1  )  −  1  stride  +  1  ⌋  L_{out} = \\\\left\\\\lfloor\\\\frac{L_{in} + 2 \\\\times \\\\text{padding} - \\\\text{dilation}'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size} - 1) - 1}{\\\\text{stride}} + 1\\\\right\\\\rfloorLout\\u200b=⌊strideLin\\u200b+2×padding−dilation×(kernel_size−1)−1\\u200b+1⌋  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  out_channels  ,  in_channels  groups  ,  kernel_size  )  (\\\\text{out\\\\_channels},\\n\\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}}, \\\\text{kernel\\\\_size})(out_channels,groupsin_channels\\u200b,kernel_size).'),\n",
       " Document(metadata={}, page_content='The values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  kernel_size  k = \\\\frac{groups}{C_\\\\text{in} * \\\\text{kernel\\\\_size}}k=Cin\\u200b∗kernel_sizegroups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape\\n(out_channels). If bias is True , then the values of these weights are'),\n",
       " Document(metadata={}, page_content='sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  kernel_size  k = \\\\frac{groups}{C_\\\\text{in} * \\\\text{kernel\\\\_size}}k=Cin\\u200b∗kernel_sizegroups\\u200b  Examples:  >>>m=nn.Conv1d(16,33,3,stride=2)>>>input=torch.randn(20,16,50)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);'),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d'),\n",
       " Document(metadata={}, page_content=\"Conv2d [LINK_1]  class torch.nn.Conv2d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 2D convolution over an input signal composed of several input\"),\n",
       " Document(metadata={}, page_content='planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  in  ,  H  ,  W  )  (N, C_{\\\\text{in}}, H, W)(N,Cin\\u200b,H,W)and output(  N  ,  C  out  ,  H  out  ,  W  out  )  (N, C_{\\\\text{out}}, H_{\\\\text{out}}, W_{\\\\text{out}})(N,Cout\\u200b,Hout\\u200b,Wout\\u200b)can be precisely described as:  out  (  N  i  ,  C  out  j  )  =  bias  (  C  out  j  )  +  ∑  k  =  0  C  in  −  1  weight  (  C  out  j  ,  k  )  ⋆  input  (  N  i  ,  k  )  \\\\text{out}(N_i, C_{\\\\text{out}_j}) ='),\n",
       " Document(metadata={}, page_content='i  ,  k  )  \\\\text{out}(N_i, C_{\\\\text{out}_j}) = \\\\text{bias}(C_{\\\\text{out}_j}) +'),\n",
       " Document(metadata={}, page_content='\\\\sum_{k = 0}^{C_{\\\\text{in}} - 1} \\\\text{weight}(C_{\\\\text{out}_j}, k) \\\\star \\\\text{input}(N_i, k)out(Ni\\u200b,Coutj\\u200b\\u200b)=bias(Coutj\\u200b\\u200b)+k=0∑Cin\\u200b−1\\u200bweight(Coutj\\u200b\\u200b,k)⋆input(Ni\\u200b,k)  where⋆  \\\\star⋆is the valid 2D cross-correlation operator,N  NNis a batch size,C  CCdenotes a number of channels,H  HHis a height of input planes in pixels, andW  WWis'),\n",
       " Document(metadata={}, page_content='width in pixels.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation, a single\\nnumber or a tuple.  padding controls the amount of padding applied to the input. It\\ncan be either a string {‘valid’, ‘same’} or an int / a tuple of ints giving the'),\n",
       " Document(metadata={}, page_content='amount of implicit padding applied on both sides.  dilation controls the spacing between the kernel points; also\\nknown as the à trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv'),\n",
       " Document(metadata={}, page_content='layers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.  At groups= in_channels , each input channel is convolved with'),\n",
       " Document(metadata={}, page_content='its own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  The parameters kernel_size , stride , padding , dilation can either be:  a single int – in which case the same value is used for the height and width dimension  a tuple of two ints – in which case, the first int is used for the height dimension,\\nand the second int for the width dimension  Note  When groups == in_channels and out_channels == K * in_channels ,'),\n",
       " Document(metadata={}, page_content='where K is a positive integer, this operation is also known as a “depthwise convolution”.  In other words, for an input of size(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b),'),\n",
       " Document(metadata={}, page_content='a depthwise convolution with a depthwise multiplier K can be performed with the arguments(  C  in  =  C  in  ,  C  out  =  C  in  ×  K  ,  .  .  .  ,  groups  =  C  in  )  (C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})(Cin\\u200b=Cin\\u200b,Cout\\u200b=Cin\\u200b×K,...,groups=Cin\\u200b).  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable,'),\n",
       " Document(metadata={}, page_content=\"to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Note  padding='valid' is the same as no padding. padding='same' pads\"),\n",
       " Document(metadata={}, page_content='the input so the output has the shape as the input. However, this mode'),\n",
       " Document(metadata={}, page_content='doesn’t support any stride values other than 1.  Note  This module supports complex data types i.e. complex32,complex64,complex128 .  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  ,  tuple  or  str  ,  optional ) – Padding added to all four'),\n",
       " Document(metadata={}, page_content='str  ,  optional ) – Padding added to all four sides of'),\n",
       " Document(metadata={}, page_content='the input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the'),\n",
       " Document(metadata={}, page_content=\"output. Default: True  padding_mode ( str  ,  optional ) – 'zeros' , 'reflect' , 'replicate' or 'circular' . Default: 'zeros'  Shape:  Input:(  N  ,  C  i  n  ,  H  i  n  ,  W  i  n  )  (N, C_{in}, H_{in}, W_{in})(N,Cin\\u200b,Hin\\u200b,Win\\u200b)or(  C  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in}, H_{in}, W_{in})(Cin\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, H_{out}, W_{out})(N,Cout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C_{out}, H_{out},\"),\n",
       " Document(metadata={}, page_content='H  o  u  t  ,  W  o  u  t  )  (C_{out}, H_{out}, W_{out})(Cout\\u200b,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  0  ]  −  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  −  1  stride  [  0  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[0]Hin\\u200b+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  1  ]  −  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  −  1  stride  [  1  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[1]Win\\u200b+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1\\u200b+1⌋  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  out_channels  ,  in_channels  groups  ,  (\\\\text{out\\\\_channels}, \\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}},(out_channels,groupsin_channels\\u200b,kernel_size[0]  ,  kernel_size[1]  )  \\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]})kernel_size[0],kernel_size[1]).'),\n",
       " Document(metadata={}, page_content='The values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  ∏  i  =  0  1  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}k=Cin\\u200b∗∏i=01\\u200bkernel_size[i]groups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape\\n(out_channels). If bias is True ,\\nthen the values of these weights are'),\n",
       " Document(metadata={}, page_content='sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  ∏  i  =  0  1  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}k=Cin\\u200b∗∏i=01\\u200bkernel_size[i]groups\\u200b  Examples  >>># With square kernels and equal stride>>>m=nn.Conv2d(16,33,3,stride=2)>>># non-square kernels and unequal stride and with padding>>>m=nn.Conv2d(16,33,(3,5),stride=(2,1),padding=(4,2))>>># non-square kernels and unequal stride and'),\n",
       " Document(metadata={}, page_content='non-square kernels and unequal stride and with padding and dilation>>>m=nn.Conv2d(16,33,(3,5),stride=(2,1),padding=(4,2),dilation=(3,1))>>>input=torch.randn(20,16,50,100)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d'),\n",
       " Document(metadata={}, page_content=\"Conv3d [LINK_1]  class torch.nn.Conv3d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 3D convolution over an input signal composed of several input\"),\n",
       " Document(metadata={}, page_content='planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  i  n  ,  D  ,  H  ,  W  )  (N, C_{in}, D, H, W)(N,Cin\\u200b,D,H,W)and output(  N  ,  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b)can be precisely described as:  o  u  t  (  N  i  ,  C  o  u  t  j  )  =  b  i  a  s  (  C  o  u  t  j  )  +  ∑  k  =  0  C  i  n  −  1  w  e  i  g  h  t  (  C  o  u  t  j  ,  k  )  ⋆  i  n  p  u  t  (  N'),\n",
       " Document(metadata={}, page_content='C  o  u  t  j  ,  k  )  ⋆  i  n  p  u  t  (  N  i  ,  k  )  out(N_i, C_{out_j}) = bias(C_{out_j}) +'),\n",
       " Document(metadata={}, page_content='\\\\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\\\star input(N_i, k)out(Ni\\u200b,Coutj\\u200b\\u200b)=bias(Coutj\\u200b\\u200b)+k=0∑Cin\\u200b−1\\u200bweight(Coutj\\u200b\\u200b,k)⋆input(Ni\\u200b,k)  where⋆  \\\\star⋆is the valid 3D cross-correlation operator  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation.  padding controls the amount of padding applied to the input. It'),\n",
       " Document(metadata={}, page_content='can be either a string {‘valid’, ‘same’} or a tuple of ints giving the\\namount of implicit padding applied on both sides.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.'),\n",
       " Document(metadata={}, page_content='It is harder to describe, but this link has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently'),\n",
       " Document(metadata={}, page_content='concatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  The parameters kernel_size , stride , padding , dilation can either be:  a single int – in which case the same value is used for the depth, height and width dimension  a tuple of three ints – in which case, the first int is used for the depth dimension,'),\n",
       " Document(metadata={}, page_content='the second int for the height dimension and the third int for the width dimension  Note  When groups == in_channels and out_channels == K * in_channels ,\\nwhere K is a positive integer, this operation is also known as a “depthwise convolution”.  In other words, for an input of size(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b),'),\n",
       " Document(metadata={}, page_content='a depthwise convolution with a depthwise multiplier K can be performed with the arguments(  C  in  =  C  in  ,  C  out  =  C  in  ×  K  ,  .  .  .  ,  groups  =  C  in  )  (C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})(Cin\\u200b=Cin\\u200b,Cout\\u200b=Cin\\u200b×K,...,groups=Cin\\u200b).  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable,'),\n",
       " Document(metadata={}, page_content=\"to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Note  padding='valid' is the same as no padding. padding='same' pads\"),\n",
       " Document(metadata={}, page_content='the input so the output has the shape as the input. However, this mode'),\n",
       " Document(metadata={}, page_content='doesn’t support any stride values other than 1.  Note  This module supports complex data types i.e. complex32,complex64,complex128 .  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  ,  tuple  or  str  ,  optional ) – Padding added to all six'),\n",
       " Document(metadata={}, page_content='or  str  ,  optional ) – Padding added to all six sides of'),\n",
       " Document(metadata={}, page_content=\"the input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  padding_mode ( str  ,  optional ) – 'zeros' , 'reflect' , 'replicate' or 'circular' . Default: 'zeros'  Shape:  Input:(  N  ,  C  i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N,\"),\n",
       " Document(metadata={}, page_content='i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\\u200b,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in}, D_{in}, H_{in}, W_{in})(Cin\\u200b,Din\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C_{out}, D_{out}, H_{out}, W_{out})(Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b),'),\n",
       " Document(metadata={}, page_content='where  D  o  u  t  =  ⌊  D  i  n  +  2  ×  padding  [  0  ]  −  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  −  1  stride  [  0  ]  +  1  ⌋  D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorDout\\u200b=⌊stride[0]Din\\u200b+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1\\u200b+1⌋  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  1  ]  −  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  −  1  stride  [  1  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[1]Hin\\u200b+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  2  ]  −  dilation  [  2  ]  ×  (  kernel_size  [  2  ]  −  1  )  −  1  stride  [  2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] - \\\\text{dilation}[2]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[2] - 1) - 1}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[2]Win\\u200b+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1\\u200b+1⌋  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  out_channels  ,  in_channels  groups  ,  (\\\\text{out\\\\_channels}, \\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}},(out_channels,groupsin_channels\\u200b,kernel_size[0]  ,  kernel_size[1]  ,  kernel_size[2]  )  \\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]},'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]}, \\\\text{kernel\\\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]).'),\n",
       " Document(metadata={}, page_content='The values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  ∏  i  =  0  2  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}k=Cin\\u200b∗∏i=02\\u200bkernel_size[i]groups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape (out_channels). If bias is True ,\\nthen the values of these weights are'),\n",
       " Document(metadata={}, page_content='sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  in  ∗  ∏  i  =  0  2  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}k=Cin\\u200b∗∏i=02\\u200bkernel_size[i]groups\\u200b  Examples:  >>># With square kernels and equal stride>>>m=nn.Conv3d(16,33,3,stride=2)>>># non-square kernels and unequal stride and with'),\n",
       " Document(metadata={}, page_content='non-square kernels and unequal stride and with padding>>>m=nn.Conv3d(16,33,(3,5,2),stride=(2,1,1),padding=(4,2,0))>>>input=torch.randn(20,16,10,50,100)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#conv3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d'),\n",
       " Document(metadata={}, page_content=\"ConvTranspose1d [LINK_1]  class torch.nn.ConvTranspose1d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 1D transposed convolution operator over an input image\\ncomposed of several input planes.  This module can be seen as the gradient of Conv1d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\"),\n",
       " Document(metadata={}, page_content='a deconvolution (although it is not an actual deconvolution operation as it does\\nnot compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation.  padding controls the amount of implicit zero padding on both'),\n",
       " Document(metadata={}, page_content='sides for dilation*(kernel_size-1)-padding number of points. See note\\nbelow for details.  output_padding controls the additional size added to one side\\nof the output shape. See note below for details.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.'),\n",
       " Document(metadata={}, page_content='It is harder to describe, but the link here has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently'),\n",
       " Document(metadata={}, page_content='concatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  Note  The padding argument effectively adds dilation*(kernel_size-1)-padding amount of zero padding to both sizes of the input. This is set so that\\nwhen a Conv1d and a ConvTranspose1d are initialized with same parameters, they are inverses of each other in'),\n",
       " Document(metadata={}, page_content='regard to the input and output shapes. However, when stride>1 , Conv1d maps multiple input shapes to the same output\\nshape. output_padding is provided to resolve this ambiguity by\\neffectively increasing the calculated output shape on one side. Note\\nthat output_padding is only used to find output shape, but does\\nnot actually add zero-padding to output.  Note  In some circumstances when using the CUDA backend with CuDNN, this operator'),\n",
       " Document(metadata={}, page_content='may select a nondeterministic algorithm to increase performance. If this is\\nundesirable, you can try to make the operation deterministic (potentially at\\na performance cost) by setting torch.backends.cudnn.deterministic=True .'),\n",
       " Document(metadata={}, page_content='Please see the notes on Reproducibility for background.  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding'),\n",
       " Document(metadata={}, page_content='will be added to both sides of the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side'),\n",
       " Document(metadata={}, page_content='of the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  Shape:  Input:(  N  ,  C  i  n  ,  L  i  n  )  (N, C_{in}, L_{in})(N,Cin\\u200b,Lin\\u200b)or(  C  i  n  ,  L  i  n  )  (C_{in}, L_{in})(Cin\\u200b,Lin\\u200b)  Output:(  N  ,  C  o  u  t  ,  L  o  u'),\n",
       " Document(metadata={}, page_content='Output:(  N  ,  C  o  u  t  ,  L  o  u  t  )  (N, C_{out}, L_{out})(N,Cout\\u200b,Lout\\u200b)or(  C  o  u  t  ,  L  o  u  t  )  (C_{out}, L_{out})(Cout\\u200b,Lout\\u200b), where  L  o  u  t  =  (  L  i  n  −  1  )  ×  stride  −  2  ×  padding  +  dilation  ×  (  kernel_size  −  1  )  +  output_padding  +  1  L_{out} = (L_{in} - 1) \\\\times \\\\text{stride} - 2 \\\\times \\\\text{padding} + \\\\text{dilation}'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size} - 1) + \\\\text{output\\\\_padding} + 1Lout\\u200b=(Lin\\u200b−1)×stride−2×padding+dilation×(kernel_size−1)+output_padding+1  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  in_channels  ,  out_channels  groups  ,  (\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},(in_channels,groupsout_channels\\u200b,kernel_size  )  \\\\text{kernel\\\\_size})kernel_size).'),\n",
       " Document(metadata={}, page_content='The values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  kernel_size  k = \\\\frac{groups}{C_\\\\text{out} * \\\\text{kernel\\\\_size}}k=Cout\\u200b∗kernel_sizegroups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape (out_channels).\\nIf bias is True , then the values of these weights are'),\n",
       " Document(metadata={}, page_content='sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  kernel_size  k = \\\\frac{groups}{C_\\\\text{out} * \\\\text{kernel\\\\_size}}k=Cout\\u200b∗kernel_sizegroups\\u200b\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#convtranspose1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d'),\n",
       " Document(metadata={}, page_content=\"ConvTranspose2d [LINK_1]  class torch.nn.ConvTranspose2d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 2D transposed convolution operator over an input image\\ncomposed of several input planes.  This module can be seen as the gradient of Conv2d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\"),\n",
       " Document(metadata={}, page_content='a deconvolution (although it is not an actual deconvolution operation as it does\\nnot compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation.  padding controls the amount of implicit zero padding on both'),\n",
       " Document(metadata={}, page_content='sides for dilation*(kernel_size-1)-padding number of points. See note\\nbelow for details.  output_padding controls the additional size added to one side\\nof the output shape. See note below for details.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.'),\n",
       " Document(metadata={}, page_content='It is harder to describe, but the link here has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently'),\n",
       " Document(metadata={}, page_content='concatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  The parameters kernel_size , stride , padding , output_padding can either be:  a single int – in which case the same value is used for the height and width dimensions  a tuple of two ints – in which case, the first int is used for the height dimension,'),\n",
       " Document(metadata={}, page_content='and the second int for the width dimension  Note  The padding argument effectively adds dilation*(kernel_size-1)-padding amount of zero padding to both sizes of the input. This is set so that\\nwhen a Conv2d and a ConvTranspose2d are initialized with same parameters, they are inverses of each other in\\nregard to the input and output shapes. However, when stride>1 , Conv2d maps multiple input shapes to the same output\\nshape. output_padding is provided to resolve this ambiguity by'),\n",
       " Document(metadata={}, page_content='effectively increasing the calculated output shape on one side. Note\\nthat output_padding is only used to find output shape, but does'),\n",
       " Document(metadata={}, page_content='not actually add zero-padding to output.  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int )'),\n",
       " Document(metadata={}, page_content='channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding'),\n",
       " Document(metadata={}, page_content='will be added to both sides of each dimension in the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side'),\n",
       " Document(metadata={}, page_content='of each dimension in the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  Shape:  Input:(  N  ,  C  i  n  ,  H  i  n  ,  W  i  n  )  (N, C_{in}, H_{in}, W_{in})(N,Cin\\u200b,Hin\\u200b,Win\\u200b)or(  C  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in},'),\n",
       " Document(metadata={}, page_content='C  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in}, H_{in}, W_{in})(Cin\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, H_{out}, W_{out})(N,Cout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C_{out}, H_{out}, W_{out})(Cout\\u200b,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride  [  0  ]  −  2  ×  padding  [  0  ]  +  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  +  output_padding  [  0  ]  +  1  H_{out} = (H_{in} - 1)'),\n",
       " Document(metadata={}, page_content='[  0  ]  +  1  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{dilation}[0]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[0] - 1) + \\\\text{output\\\\_padding}[0] + 1Hout\\u200b=(Hin\\u200b−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1  W  o  u  t  =  (  W  i  n  −  1  )  ×  stride  [  1  ]  −  2  ×  padding  [  1  ]  +  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  +  output_padding  [  1  ]  +  1  W_{out} = (W_{in} - 1) \\\\times \\\\text{stride}[1] - 2 \\\\times \\\\text{padding}[1] + \\\\text{dilation}[1]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[1] - 1) + \\\\text{output\\\\_padding}[1] + 1Wout\\u200b=(Win\\u200b−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  in_channels  ,  out_channels  groups  ,  (\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},(in_channels,groupsout_channels\\u200b,kernel_size[0]  ,  kernel_size[1]  )  \\\\text{kernel\\\\_size[0]},'),\n",
       " Document(metadata={}, page_content=',  kernel_size[1]  )  \\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]})kernel_size[0],kernel_size[1]).'),\n",
       " Document(metadata={}, page_content='The values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  ∏  i  =  0  1  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}k=Cout\\u200b∗∏i=01\\u200bkernel_size[i]groups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape (out_channels)\\nIf bias is True , then the values of these weights are'),\n",
       " Document(metadata={}, page_content='sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  ∏  i  =  0  1  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}k=Cout\\u200b∗∏i=01\\u200bkernel_size[i]groups\\u200b  Examples:  >>># With square kernels and equal stride>>>m=nn.ConvTranspose2d(16,33,3,stride=2)>>># non-square kernels and unequal stride and with'),\n",
       " Document(metadata={}, page_content='non-square kernels and unequal stride and with padding>>>m=nn.ConvTranspose2d(16,33,(3,5),stride=(2,1),padding=(4,2))>>>input=torch.randn(20,16,50,100)>>>output=m(input)>>># exact output size can be also specified as an argument>>>input=torch.randn(1,16,12,12)>>>downsample=nn.Conv2d(16,16,3,stride=2,padding=1)>>>upsample=nn.ConvTranspose2d(16,16,3,stride=2,padding=1)>>>h=downsample(input)>>>h.size()torch.Size([1, 16, 6,'),\n",
       " Document(metadata={}, page_content='16, 6, 6])>>>output=upsample(h,output_size=input.size())>>>output.size()torch.Size([1, 16, 12, 12])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#convtranspose2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d'),\n",
       " Document(metadata={}, page_content=\"ConvTranspose3d [LINK_1]  class torch.nn.ConvTranspose3d( in_channels , out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies a 3D transposed convolution operator over an input image composed of several input\\nplanes.\\nThe transposed convolution operator multiplies each input value element-wise by a learnable kernel,\"),\n",
       " Document(metadata={}, page_content='and sums over the outputs from all input feature planes.  This module can be seen as the gradient of Conv3d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\\na deconvolution (although it is not an actual deconvolution operation as it does'),\n",
       " Document(metadata={}, page_content='not compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.  This module supports TensorFloat32 .  On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  stride controls the stride for the cross-correlation.  padding controls the amount of implicit zero padding on both\\nsides for dilation*(kernel_size-1)-padding number of points. See note'),\n",
       " Document(metadata={}, page_content='below for details.  output_padding controls the additional size added to one side\\nof the output shape. See note below for details.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.'),\n",
       " Document(metadata={}, page_content='It is harder to describe, but the link here has a nice visualization of what dilation does.  groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups . For example,  At groups=1, all inputs are convolved to all outputs.  At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently'),\n",
       " Document(metadata={}, page_content='concatenated.  At groups= in_channels , each input channel is convolved with\\nits own set of filters (of sizeout_channels  in_channels  \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}in_channelsout_channels\\u200b).  The parameters kernel_size , stride , padding , output_padding can either be:  a single int – in which case the same value is used for the depth, height and width dimensions  a tuple of three ints – in which case, the first int is used for the depth dimension,'),\n",
       " Document(metadata={}, page_content='the second int for the height dimension and the third int for the width dimension  Note  The padding argument effectively adds dilation*(kernel_size-1)-padding amount of zero padding to both sizes of the input. This is set so that\\nwhen a Conv3d and a ConvTranspose3d are initialized with same parameters, they are inverses of each other in\\nregard to the input and output shapes. However, when stride>1 , Conv3d maps multiple input shapes to the same output'),\n",
       " Document(metadata={}, page_content='shape. output_padding is provided to resolve this ambiguity by\\neffectively increasing the calculated output shape on one side. Note\\nthat output_padding is only used to find output shape, but does'),\n",
       " Document(metadata={}, page_content='not actually add zero-padding to output.  Note  In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic=True . See Reproducibility for more information.  Parameters  in_channels ( int ) – Number of channels in the input image  out_channels ( int )'),\n",
       " Document(metadata={}, page_content='channels in the input image  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding'),\n",
       " Document(metadata={}, page_content='will be added to both sides of each dimension in the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side'),\n",
       " Document(metadata={}, page_content='of each dimension in the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  Shape:  Input:(  N  ,  C  i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\\u200b,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  i  n  ,  D  i  n  ,'),\n",
       " Document(metadata={}, page_content='C  i  n  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C_{in}, D_{in}, H_{in}, W_{in})(Cin\\u200b,Din\\u200b,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  o  u  t  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C_{out}, D_{out}, H_{out}, W_{out})(Cout\\u200b,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  (  D  i  n  −  1  )  ×  stride  [  0  ]  −  2  ×  padding  [  0  ]  +  dilation  [  0  ]  ×  ('),\n",
       " Document(metadata={}, page_content='×  padding  [  0  ]  +  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  +  output_padding  [  0  ]  +  1  D_{out} = (D_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{dilation}[0]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[0] - 1) + \\\\text{output\\\\_padding}[0] + 1Dout\\u200b=(Din\\u200b−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride  [  1  ]  −  2  ×  padding  [  1  ]  +  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  +  output_padding  [  1  ]  +  1  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[1] - 2 \\\\times \\\\text{padding}[1] + \\\\text{dilation}[1]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[1] - 1) + \\\\text{output\\\\_padding}[1] + 1Hout\\u200b=(Hin\\u200b−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1  W  o  u  t  =  (  W  i  n  −  1  )  ×  stride  [  2  ]  −  2  ×  padding  [  2  ]  +  dilation  [  2  ]  ×  (  kernel_size  [  2  ]  −  1  )  +  output_padding  [  2  ]  +  1  W_{out} = (W_{in} - 1) \\\\times \\\\text{stride}[2] - 2 \\\\times \\\\text{padding}[2] + \\\\text{dilation}[2]'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size}[2] - 1) + \\\\text{output\\\\_padding}[2] + 1Wout\\u200b=(Win\\u200b−1)×stride[2]−2×padding[2]+dilation[2]×(kernel_size[2]−1)+output_padding[2]+1  Variables  weight ( Tensor ) – the learnable weights of the module of shape(  in_channels  ,  out_channels  groups  ,  (\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},(in_channels,groupsout_channels\\u200b,kernel_size[0]  ,  kernel_size[1]  ,  kernel_size[2]  )  \\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]},'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]}, \\\\text{kernel\\\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]).'),\n",
       " Document(metadata={}, page_content='The values of these weights are sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  ∏  i  =  0  2  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}k=Cout\\u200b∗∏i=02\\u200bkernel_size[i]groups\\u200b  bias ( Tensor ) – the learnable bias of the module of shape (out_channels)\\nIf bias is True , then the values of these weights are'),\n",
       " Document(metadata={}, page_content='sampled fromU  (  −  k  ,  k  )  \\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})U(−k\\u200b,k\\u200b)wherek  =  g  r  o  u  p  s  C  out  ∗  ∏  i  =  0  2  kernel_size  [  i  ]  k = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}k=Cout\\u200b∗∏i=02\\u200bkernel_size[i]groups\\u200b  Examples:  >>># With square kernels and equal stride>>>m=nn.ConvTranspose3d(16,33,3,stride=2)>>># non-square kernels and unequal stride and with'),\n",
       " Document(metadata={}, page_content='non-square kernels and unequal stride and with padding>>>m=nn.ConvTranspose3d(16,33,(3,5,2),stride=(2,1,1),padding=(0,4,2))>>>input=torch.randn(20,16,10,50,100)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#convtranspose3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d'),\n",
       " Document(metadata={}, page_content=\"LazyConv1d [LINK_1]  class torch.nn.LazyConv1d( out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.Conv1d module with lazy initialization of the in_channels argument.  The in_channels argument of the Conv1d is inferred from the input.size(1) .\"),\n",
       " Document(metadata={}, page_content='The attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – Zero-padding added to both sides of'),\n",
       " Document(metadata={}, page_content='the input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel\\nelements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the'),\n",
       " Document(metadata={}, page_content=\"output. Default: True  padding_mode ( str  ,  optional ) – 'zeros' , 'reflect' , 'replicate' or 'circular' . Default: 'zeros'  See also  torch.nn.Conv1d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of Conv1d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\"),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#lazyconv1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d.cls_to_become'),\n",
       " Document(metadata={}, page_content=\"LazyConv2d [LINK_1]  class torch.nn.LazyConv2d( out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.Conv2d module with lazy initialization of the in_channels argument.  The in_channels argument of the Conv2d that is inferred from the input.size(1) .\"),\n",
       " Document(metadata={}, page_content='The attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – Zero-padding added to both sides of'),\n",
       " Document(metadata={}, page_content='the input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel\\nelements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the'),\n",
       " Document(metadata={}, page_content=\"output. Default: True  padding_mode ( str  ,  optional ) – 'zeros' , 'reflect' , 'replicate' or 'circular' . Default: 'zeros'  See also  torch.nn.Conv2d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of Conv2d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\"),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#lazyconv2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d.cls_to_become'),\n",
       " Document(metadata={}, page_content=\"LazyConv3d [LINK_1]  class torch.nn.LazyConv3d( out_channels , kernel_size , stride=1 , padding=0 , dilation=1 , groups=1 , bias=True , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.Conv3d module with lazy initialization of the in_channels argument.  The in_channels argument of the Conv3d that is inferred from\\nthe input.size(1) .\"),\n",
       " Document(metadata={}, page_content='the input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation'),\n",
       " Document(metadata={}, page_content='on lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – Zero-padding added to both sides of\\nthe input. Default: 0  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel'),\n",
       " Document(metadata={}, page_content=\"elements. Default: 1  groups ( int  ,  optional ) – Number of blocked connections from input\\nchannels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the\\noutput. Default: True  padding_mode ( str  ,  optional ) – 'zeros' , 'reflect' , 'replicate' or 'circular' . Default: 'zeros'  See also  torch.nn.Conv3d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of Conv3d\"),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#lazyconv3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d.cls_to_become'),\n",
       " Document(metadata={}, page_content=\"LazyConvTranspose1d [LINK_1]  class torch.nn.LazyConvTranspose1d( out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument.  The in_channels argument of the ConvTranspose1d that is inferred from\\nthe input.size(1) .\"),\n",
       " Document(metadata={}, page_content='the input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation'),\n",
       " Document(metadata={}, page_content='on lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding\\nwill be added to both sides of the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side'),\n",
       " Document(metadata={}, page_content='of the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  See also  torch.nn.ConvTranspose1d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of ConvTranspose1d'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#lazyconvtranspose1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d.cls_to_become'),\n",
       " Document(metadata={}, page_content=\"LazyConvTranspose2d [LINK_1]  class torch.nn.LazyConvTranspose2d( out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument.  The in_channels argument of the ConvTranspose2d is inferred from\\nthe input.size(1) .\"),\n",
       " Document(metadata={}, page_content='the input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation'),\n",
       " Document(metadata={}, page_content='on lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding'),\n",
       " Document(metadata={}, page_content='will be added to both sides of each dimension in the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side'),\n",
       " Document(metadata={}, page_content='of each dimension in the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  See also  torch.nn.ConvTranspose2d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of ConvTranspose2d'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#lazyconvtranspose2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d.cls_to_become'),\n",
       " Document(metadata={}, page_content=\"LazyConvTranspose3d [LINK_1]  class torch.nn.LazyConvTranspose3d( out_channels , kernel_size , stride=1 , padding=0 , output_padding=0 , groups=1 , bias=True , dilation=1 , padding_mode='zeros' , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument.  The in_channels argument of the ConvTranspose3d is inferred from\\nthe input.size(1) .\"),\n",
       " Document(metadata={}, page_content='the input.size(1) .\\nThe attributes that will be lazily initialized are weight and bias .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation'),\n",
       " Document(metadata={}, page_content='on lazy modules and their limitations.  Parameters  out_channels ( int ) – Number of channels produced by the convolution  kernel_size ( int  or  tuple ) – Size of the convolving kernel  stride ( int  or  tuple  ,  optional ) – Stride of the convolution. Default: 1  padding ( int  or  tuple  ,  optional ) – dilation*(kernel_size-1)-padding zero-padding'),\n",
       " Document(metadata={}, page_content='will be added to both sides of each dimension in the input. Default: 0  output_padding ( int  or  tuple  ,  optional ) – Additional size added to one side'),\n",
       " Document(metadata={}, page_content='of each dimension in the output shape. Default: 0  groups ( int  ,  optional ) – Number of blocked connections from input channels to output channels. Default: 1  bias ( bool  ,  optional ) – If True , adds a learnable bias to the output. Default: True  dilation ( int  or  tuple  ,  optional ) – Spacing between kernel elements. Default: 1  See also  torch.nn.ConvTranspose3d and torch.nn.modules.lazy.LazyModuleMixin  cls_to_become [source]  [LINK_3]  alias of ConvTranspose3d'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#lazyconvtranspose3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d.cls_to_become'),\n",
       " Document(metadata={}, page_content='Unfold [LINK_1]  class torch.nn.Unfold( kernel_size , dilation=1 , padding=0 , stride=1 ) [source]  [source]  [LINK_2]  Extracts sliding local blocks from a batched input tensor.  Consider a batched input tensor of shape(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗),\\nwhereN  NNis the batch dimension,C  CCis the channel dimension,\\nand∗  *∗represent arbitrary spatial dimensions. This operation flattens\\neach sliding kernel_size -sized block within the spatial dimensions'),\n",
       " Document(metadata={}, page_content='of input into a column (i.e., last dimension) of a 3-D output tensor of shape(  N  ,  C  ×  ∏  (  kernel_size  )  ,  L  )  (N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)(N,C×∏(kernel_size),L), whereC  ×  ∏  (  kernel_size  )  C \\\\times \\\\prod(\\\\text{kernel\\\\_size})C×∏(kernel_size)is the total number of values\\nwithin each block (a block has∏  (  kernel_size  )  \\\\prod(\\\\text{kernel\\\\_size})∏(kernel_size)spatial\\nlocations each containing aC  CC-channeled vector), andL  LLis'),\n",
       " Document(metadata={}, page_content='the total number of such blocks:  L  =  ∏  d  ⌊  spatial_size  [  d  ]  +  2  ×  padding  [  d  ]  −  dilation  [  d  ]  ×  (  kernel_size  [  d  ]  −  1  )  −  1  stride  [  d  ]  +  1  ⌋  ,  L = \\\\prod_d \\\\left\\\\lfloor\\\\frac{\\\\text{spatial\\\\_size}[d] + 2 \\\\times \\\\text{padding}[d] %'),\n",
       " Document(metadata={}, page_content='- \\\\text{dilation}[d] \\\\times (\\\\text{kernel\\\\_size}[d] - 1) - 1}{\\\\text{stride}[d]} + 1\\\\right\\\\rfloor,L=d∏\\u200b⌊stride[d]spatial_size[d]+2×padding[d]−dilation[d]×(kernel_size[d]−1)−1\\u200b+1⌋,  wherespatial_size  \\\\text{spatial\\\\_size}spatial_sizeis formed by the spatial dimensions\\nof input (∗  *∗above), andd  ddis over all spatial\\ndimensions.  Therefore, indexing output at the last dimension (column dimension)\\ngives all values within a certain block.  The padding , stride and dilation arguments specify'),\n",
       " Document(metadata={}, page_content='how the sliding blocks are retrieved.  stride controls the stride for the sliding blocks.  padding controls the amount of implicit zero-paddings on both\\nsides for padding number of points for each dimension before\\nreshaping.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.'),\n",
       " Document(metadata={}, page_content='It is harder to describe, but this link has a nice visualization of what dilation does.  Parameters  kernel_size ( int  or  tuple ) – the size of the sliding blocks  dilation ( int  or  tuple  ,  optional ) – a parameter that controls the\\nstride of elements within the\\nneighborhood. Default: 1  padding ( int  or  tuple  ,  optional ) – implicit zero padding to be added on\\nboth sides of input. Default: 0  stride ( int  or  tuple  ,  optional ) – the stride of the sliding blocks in the input'),\n",
       " Document(metadata={}, page_content='spatial dimensions. Default: 1  If kernel_size , dilation , padding or stride is an int or a tuple of length 1, their values will be\\nreplicated across all spatial dimensions.  For the case of two input spatial dimensions this operation is sometimes\\ncalled im2col .  Note  Fold calculates each combined value in the resulting\\nlarge tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by'),\n",
       " Document(metadata={}, page_content='copying from the large tensor. So, if the blocks overlap, they are not\\ninverses of each other.  In general, folding and unfolding operations are related as\\nfollows. Consider Fold and Unfold instances created with the same\\nparameters:  >>>fold_params=dict(kernel_size=...,dilation=...,padding=...,stride=...)>>>fold=nn.Fold(output_size=...,**fold_params)>>>unfold=nn.Unfold(**fold_params)  Then for any (supported) input tensor the following'),\n",
       " Document(metadata={}, page_content='equality holds:  fold(unfold(input))==divisor*input  where divisor is a tensor that depends only on the shape\\nand dtype of the input :  >>>input_ones=torch.ones(input.shape,dtype=input.dtype)>>>divisor=fold(unfold(input_ones))  When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each\\nother (up to constant divisor).  Warning  Currently, only 4-D input tensors (batched image-like tensors) are'),\n",
       " Document(metadata={}, page_content='supported.  Shape:  Input:(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)  Output:(  N  ,  C  ×  ∏  (  kernel_size  )  ,  L  )  (N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)(N,C×∏(kernel_size),L)as described above  Examples:  >>>unfold=nn.Unfold(kernel_size=(2,3))>>>input=torch.randn(2,5,3,4)>>>output=unfold(input)>>># each patch contains 30 values (2x3=6 vectors, each of 5 channels)>>># 4 blocks (2x3 kernels) in total in the 3x4 input>>>output.size()torch.Size([2, 30, 4])>>># Convolution is equivalent'),\n",
       " Document(metadata={}, page_content='30, 4])>>># Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)>>>inp=torch.randn(1,3,10,12)>>>w=torch.randn(2,3,4,5)>>>inp_unf=torch.nn.functional.unfold(inp,(4,5))>>>out_unf=inp_unf.transpose(1,2).matmul(w.view(w.size(0),-1).t()).transpose(1,2)>>>out=torch.nn.functional.fold(out_unf,(7,8),(1,1))>>># or equivalently (and avoiding a copy),>>># out = out_unf.view(1, 2, 7, 8)>>>(torch.nn.functional.conv2d(inp,w)-out).abs().max()tensor(1.9073e-06)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#unfold\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold'),\n",
       " Document(metadata={}, page_content='Fold [LINK_1]  class torch.nn.Fold( output_size , kernel_size , dilation=1 , padding=0 , stride=1 ) [source]  [source]  [LINK_2]  Combines an array of sliding local blocks into a large containing tensor.  Consider a batched input tensor containing sliding local blocks,\\ne.g., patches of images, of shape(  N  ,  C  ×  ∏  (  kernel_size  )  ,  L  )  (N, C \\\\times  \\\\prod(\\\\text{kernel\\\\_size}), L)(N,C×∏(kernel_size),L),'),\n",
       " Document(metadata={}, page_content='whereN  NNis batch dimension,C  ×  ∏  (  kernel_size  )  C \\\\times \\\\prod(\\\\text{kernel\\\\_size})C×∏(kernel_size)is the number of values within a block (a block has∏  (  kernel_size  )  \\\\prod(\\\\text{kernel\\\\_size})∏(kernel_size)spatial locations each containing aC  CC-channeled vector), andL  LLis the total number of blocks. (This is exactly the\\nsame specification as the output shape of Unfold .) This\\noperation combines these local blocks into the large output tensor'),\n",
       " Document(metadata={}, page_content='of shape(  N  ,  C  ,  output_size  [  0  ]  ,  output_size  [  1  ]  ,  …  )  (N, C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)(N,C,output_size[0],output_size[1],…)by summing the overlapping values. Similar to Unfold , the'),\n",
       " Document(metadata={}, page_content='arguments must satisfy  L  =  ∏  d  ⌊  output_size  [  d  ]  +  2  ×  padding  [  d  ]  −  dilation  [  d  ]  ×  (  kernel_size  [  d  ]  −  1  )  −  1  stride  [  d  ]  +  1  ⌋  ,  L = \\\\prod_d \\\\left\\\\lfloor\\\\frac{\\\\text{output\\\\_size}[d] + 2 \\\\times \\\\text{padding}[d] %'),\n",
       " Document(metadata={}, page_content='- \\\\text{dilation}[d] \\\\times (\\\\text{kernel\\\\_size}[d] - 1) - 1}{\\\\text{stride}[d]} + 1\\\\right\\\\rfloor,L=d∏\\u200b⌊stride[d]output_size[d]+2×padding[d]−dilation[d]×(kernel_size[d]−1)−1\\u200b+1⌋,  whered  ddis over all spatial dimensions.  output_size describes the spatial shape of the large containing\\ntensor of the sliding local blocks. It is useful to resolve the ambiguity\\nwhen multiple input shapes map to same number of sliding blocks, e.g.,'),\n",
       " Document(metadata={}, page_content='with stride>0 .  The padding , stride and dilation arguments specify\\nhow the sliding blocks are retrieved.  stride controls the stride for the sliding blocks.  padding controls the amount of implicit zero-paddings on both\\nsides for padding number of points for each dimension before\\nreshaping.  dilation controls the spacing between the kernel points; also known as the à trous algorithm.'),\n",
       " Document(metadata={}, page_content='It is harder to describe, but this link has a nice visualization of what dilation does.  Parameters  output_size ( int  or  tuple ) – the shape of the spatial dimensions of the\\noutput (i.e., output.sizes()[2:] )  kernel_size ( int  or  tuple ) – the size of the sliding blocks  dilation ( int  or  tuple  ,  optional ) – a parameter that controls the\\nstride of elements within the\\nneighborhood. Default: 1  padding ( int  or  tuple  ,  optional ) – implicit zero padding to be added on'),\n",
       " Document(metadata={}, page_content='both sides of input. Default: 0  stride ( int  or  tuple ) – the stride of the sliding blocks in the input\\nspatial dimensions. Default: 1  If output_size , kernel_size , dilation , padding or stride is an int or a tuple of length 1 then\\ntheir values will be replicated across all spatial dimensions.  For the case of two output spatial dimensions this operation is sometimes\\ncalled col2im .  Note  Fold calculates each combined value in the resulting'),\n",
       " Document(metadata={}, page_content='large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by\\ncopying from the large tensor. So, if the blocks overlap, they are not\\ninverses of each other.  In general, folding and unfolding operations are related as\\nfollows. Consider Fold and Unfold instances created with the same'),\n",
       " Document(metadata={}, page_content='parameters:  >>>fold_params=dict(kernel_size=...,dilation=...,padding=...,stride=...)>>>fold=nn.Fold(output_size=...,**fold_params)>>>unfold=nn.Unfold(**fold_params)  Then for any (supported) input tensor the following\\nequality holds:  fold(unfold(input))==divisor*input  where divisor is a tensor that depends only on the shape'),\n",
       " Document(metadata={}, page_content='and dtype of the input :  >>>input_ones=torch.ones(input.shape,dtype=input.dtype)>>>divisor=fold(unfold(input_ones))  When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each'),\n",
       " Document(metadata={}, page_content='other (up to constant divisor).  Warning  Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.  Shape:  Input:(  N  ,  C  ×  ∏  (  kernel_size  )  ,  L  )  (N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)(N,C×∏(kernel_size),L)or(  C  ×  ∏  (  kernel_size  )  ,  L  )  (C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)(C×∏(kernel_size),L)  Output:(  N  ,  C  ,  output_size  [  0  ]  ,  output_size  [  1  ]  ,  …  )  (N, C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1],'),\n",
       " Document(metadata={}, page_content='\\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)(N,C,output_size[0],output_size[1],…)or(  C  ,  output_size  [  0  ]  ,  output_size  [  1  ]  ,  …  )  (C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)(C,output_size[0],output_size[1],…)as described above  Examples:  >>>fold=nn.Fold(output_size=(4,5),kernel_size=(2,2))>>>input=torch.randn(1,3*2*2,12)>>>output=fold(input)>>>output.size()torch.Size([1, 3, 4, 5])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#fold\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold'),\n",
       " Document(metadata={}, page_content='MaxPool1d [LINK_1]  class torch.nn.MaxPool1d( kernel_size , stride=None , padding=0 , dilation=1 , return_indices=False , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 1D max pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)and output(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)can be precisely described as:  o  u  t  (  N  i  ,  C  j  ,  k  )  =  max  \\u2061'),\n",
       " Document(metadata={}, page_content='o  u  t  (  N  i  ,  C  j  ,  k  )  =  max  \\u2061  m  =  0  ,  …  ,  kernel_size  −  1  i  n  p  u  t  (  N  i  ,  C  j  ,  s  t  r  i  d  e  ×  k  +  m  )  out(N_i, C_j, k) = \\\\max_{m=0, \\\\ldots, \\\\text{kernel\\\\_size} - 1}'),\n",
       " Document(metadata={}, page_content='input(N_i, C_j, stride \\\\times k + m)out(Ni\\u200b,Cj\\u200b,k)=m=0,…,kernel_size−1max\\u200binput(Ni\\u200b,Cj\\u200b,stride×k+m)  If padding is non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor padding number of points. dilation is the stride between the elements within the\\nsliding window. This link has a nice visualization of the pooling parameters.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding'),\n",
       " Document(metadata={}, page_content='or the input. Sliding windows that would start in the right padded region are ignored.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – The size of the sliding window, must be > 0.  stride ( Union  [  int  ,  Tuple  [  int  ]  ] ) – The stride of the sliding window, must be > 0. Default value is kernel_size .  padding ( Union  [  int  ,  Tuple  [  int  ]  ] ) – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.  dilation ('),\n",
       " Document(metadata={}, page_content='must be >= 0 and <= kernel_size / 2.  dilation ( Union  [  int  ,  Tuple  [  int  ]  ] ) – The stride between elements within a sliding window, must be > 0.  return_indices ( bool ) – If True , will return the argmax along with the max values.'),\n",
       " Document(metadata={}, page_content='Useful for torch.nn.MaxUnpool1d later  ceil_mode ( bool ) – If True , will use ceil instead of floor to compute the output shape. This'),\n",
       " Document(metadata={}, page_content='ensures that every element in the input tensor is covered by a sliding window.  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), where  L  o  u  t  =  ⌊  L  i  n  +  2  ×  padding  −  dilation  ×  (  kernel_size  −  1  )  −  1  stride  +  1  ⌋  L_{out} = \\\\left\\\\lfloor \\\\frac{L_{in} + 2 \\\\times \\\\text{padding} - \\\\text{dilation}'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size} - 1) - 1}{\\\\text{stride}} + 1\\\\right\\\\rfloorLout\\u200b=⌊strideLin\\u200b+2×padding−dilation×(kernel_size−1)−1\\u200b+1⌋  Examples:  >>># pool of size=3, stride=2>>>m=nn.MaxPool1d(3,stride=2)>>>input=torch.randn(20,16,50)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);'),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#maxpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d'),\n",
       " Document(metadata={}, page_content='MaxPool2d [LINK_1]  class torch.nn.MaxPool2d( kernel_size , stride=None , padding=0 , dilation=1 , return_indices=False , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 2D max pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W),'),\n",
       " Document(metadata={}, page_content='output(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)and kernel_size (  k  H  ,  k  W  )  (kH, kW)(kH,kW)can be precisely described as:  o  u  t  (  N  i  ,  C  j  ,  h  ,  w  )  =  max  \\u2061  m  =  0  ,  …  ,  k  H  −  1  max  \\u2061  n  =  0  ,  …  ,  k  W  −  1  input  (  N  i  ,  C  j  ,  stride[0]  ×  h  +  m  ,  stride[1]  ×  w  +  n  )  \\\\begin{aligned}\\n    out(N_i, C_j, h, w) ={} & \\\\max_{m=0, \\\\ldots, kH-1} \\\\max_{n=0, \\\\ldots, kW-1} \\\\\\\\'),\n",
       " Document(metadata={}, page_content='& \\\\text{input}(N_i, C_j, \\\\text{stride[0]} \\\\times h + m,\\n                                           \\\\text{stride[1]} \\\\times w + n)\\n\\\\end{aligned}out(Ni\\u200b,Cj\\u200b,h,w)=\\u200bm=0,…,kH−1max\\u200bn=0,…,kW−1max\\u200binput(Ni\\u200b,Cj\\u200b,stride[0]×h+m,stride[1]×w+n)\\u200b  If padding is non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor padding number of points. dilation controls the spacing between the kernel points.'),\n",
       " Document(metadata={}, page_content='It is harder to describe, but this link has a nice visualization of what dilation does.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding'),\n",
       " Document(metadata={}, page_content='or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride , padding , dilation can either be:  a single int – in which case the same value is used for the height and width dimension  a tuple of two ints – in which case, the first int is used for the height dimension,'),\n",
       " Document(metadata={}, page_content='and the second int for the width dimension  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the size of the window to take a max over  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – Implicit negative infinity padding to be added on both sides  dilation ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – a parameter that controls the stride'),\n",
       " Document(metadata={}, page_content=']  ] ) – a parameter that controls the stride of elements in the window  return_indices ( bool ) – if True , will return the max indices along with the outputs.'),\n",
       " Document(metadata={}, page_content='Useful for torch.nn.MaxUnpool2d later  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b)  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  ⌊  H  i  n  +'),\n",
       " Document(metadata={}, page_content='where  H  o  u  t  =  ⌊  H  i  n  +  2  ∗  padding[0]  −  dilation[0]  ×  (  kernel_size[0]  −  1  )  −  1  stride[0]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 * \\\\text{padding[0]} - \\\\text{dilation[0]}'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size[0]} - 1) - 1}{\\\\text{stride[0]}} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[0]Hin\\u200b+2∗padding[0]−dilation[0]×(kernel_size[0]−1)−1\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ∗  padding[1]  −  dilation[1]  ×  (  kernel_size[1]  −  1  )  −  1  stride[1]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 * \\\\text{padding[1]} - \\\\text{dilation[1]}'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size[1]} - 1) - 1}{\\\\text{stride[1]}} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[1]Win\\u200b+2∗padding[1]−dilation[1]×(kernel_size[1]−1)−1\\u200b+1⌋  Examples:  >>># pool of square window of size=3, stride=2>>>m=nn.MaxPool2d(3,stride=2)>>># pool of non-square window>>>m=nn.MaxPool2d((3,2),stride=(2,1))>>>input=torch.randn(20,16,50,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#maxpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d'),\n",
       " Document(metadata={}, page_content='MaxPool3d [LINK_1]  class torch.nn.MaxPool3d( kernel_size , stride=None , padding=0 , dilation=1 , return_indices=False , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 3D max pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W),'),\n",
       " Document(metadata={}, page_content='output(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)and kernel_size (  k  D  ,  k  H  ,  k  W  )  (kD, kH, kW)(kD,kH,kW)can be precisely described as:  out  (  N  i  ,  C  j  ,  d  ,  h  ,  w  )  =  max  \\u2061  k  =  0  ,  …  ,  k  D  −  1  max  \\u2061  m  =  0  ,  …  ,  k  H  −  1  max  \\u2061  n  =  0  ,  …  ,  k  W  −  1  input  (  N  i  ,  C  j  ,  stride[0]  ×  d  +  k  ,  stride[1]  ×  h  +  m  ,  stride[2]  ×  w  +  n  )'),\n",
       " Document(metadata={}, page_content='×  h  +  m  ,  stride[2]  ×  w  +  n  )  \\\\begin{aligned}'),\n",
       " Document(metadata={}, page_content='\\\\text{out}(N_i, C_j, d, h, w) ={} & \\\\max_{k=0, \\\\ldots, kD-1} \\\\max_{m=0, \\\\ldots, kH-1} \\\\max_{n=0, \\\\ldots, kW-1} \\\\\\\\\\n                                      & \\\\text{input}(N_i, C_j, \\\\text{stride[0]} \\\\times d + k,\\n                                                     \\\\text{stride[1]} \\\\times h + m, \\\\text{stride[2]} \\\\times w + n)'),\n",
       " Document(metadata={}, page_content='\\\\end{aligned}out(Ni\\u200b,Cj\\u200b,d,h,w)=\\u200bk=0,…,kD−1max\\u200bm=0,…,kH−1max\\u200bn=0,…,kW−1max\\u200binput(Ni\\u200b,Cj\\u200b,stride[0]×d+k,stride[1]×h+m,stride[2]×w+n)\\u200b  If padding is non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor padding number of points. dilation controls the spacing between the kernel points.'),\n",
       " Document(metadata={}, page_content='It is harder to describe, but this link has a nice visualization of what dilation does.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding'),\n",
       " Document(metadata={}, page_content='or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride , padding , dilation can either be:  a single int – in which case the same value is used for the depth, height and width dimension  a tuple of three ints – in which case, the first int is used for the depth dimension,'),\n",
       " Document(metadata={}, page_content='the second int for the height dimension and the third int for the width dimension  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the size of the window to take a max over  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – Implicit negative infinity padding to be added on all three sides  dilation ( Union  [  int  ,'),\n",
       " Document(metadata={}, page_content='on all three sides  dilation ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – a parameter that controls the stride of elements in the window  return_indices ( bool ) – if True , will return the max indices along with the outputs.'),\n",
       " Document(metadata={}, page_content='Useful for torch.nn.MaxUnpool3d later  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u'),\n",
       " Document(metadata={}, page_content='C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  ⌊  D  i  n  +  2  ×  padding  [  0  ]  −  dilation  [  0  ]  ×  (  kernel_size  [  0  ]  −  1  )  −  1  stride  [  0  ]  +  1  ⌋  D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0] \\\\times'),\n",
       " Document(metadata={}, page_content='(\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorDout\\u200b=⌊stride[0]Din\\u200b+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1\\u200b+1⌋  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  1  ]  −  dilation  [  1  ]  ×  (  kernel_size  [  1  ]  −  1  )  −  1  stride  [  1  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1] \\\\times'),\n",
       " Document(metadata={}, page_content='(\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[1]Hin\\u200b+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  2  ]  −  dilation  [  2  ]  ×  (  kernel_size  [  2  ]  −  1  )  −  1  stride  [  2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] - \\\\text{dilation}[2] \\\\times'),\n",
       " Document(metadata={}, page_content='(\\\\text{kernel\\\\_size}[2] - 1) - 1}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[2]Win\\u200b+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1\\u200b+1⌋  Examples:  >>># pool of square window of size=3, stride=2>>>m=nn.MaxPool3d(3,stride=2)>>># pool of non-square window>>>m=nn.MaxPool3d((3,2,2),stride=(2,1,2))>>>input=torch.randn(20,16,50,44,31)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#maxpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d'),\n",
       " Document(metadata={}, page_content='MaxUnpool1d [LINK_1]  class torch.nn.MaxUnpool1d( kernel_size , stride=None , padding=0 ) [source]  [source]  [LINK_2]  Computes a partial inverse of MaxPool1d .  MaxPool1d is not fully invertible, since the non-maximal values are lost.  MaxUnpool1d takes in as input the output of MaxPool1d including the indices of the maximal values and computes a partial inverse'),\n",
       " Document(metadata={}, page_content='in which all non-maximal values are set to zero.  Note  This operation may behave nondeterministically when the input indices has repeat values.\\nSee https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.  Note  MaxPool1d can map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument output_size in the forward call.'),\n",
       " Document(metadata={}, page_content='See the Inputs and Example below.  Parameters  kernel_size ( int  or  tuple ) – Size of the max pooling window.  stride ( int  or  tuple ) – Stride of the max pooling window.'),\n",
       " Document(metadata={}, page_content='It is set to kernel_size by default.  padding ( int  or  tuple ) – Padding that was added to the input  Inputs:  input : the input Tensor to invert  indices : the indices given out by MaxPool1d  output_size (optional): the targeted output size  Shape:  Input:(  N  ,  C  ,  H  i  n  )  (N, C, H_{in})(N,C,Hin\\u200b)or(  C  ,  H  i  n  )  (C, H_{in})(C,Hin\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  )  (N, C, H_{out})(N,C,Hout\\u200b)or(  C  ,  H  o  u  t  )  (C, H_{out})(C,Hout\\u200b), where  H  o  u  t  =  (  H  i  n'),\n",
       " Document(metadata={}, page_content='where  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride  [  0  ]  −  2  ×  padding  [  0  ]  +  kernel_size  [  0  ]  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{kernel\\\\_size}[0]Hout\\u200b=(Hin\\u200b−1)×stride[0]−2×padding[0]+kernel_size[0]  or as given by output_size in the call operator  Example:'),\n",
       " Document(metadata={}, page_content='by output_size in the call operator  Example:  >>>pool=nn.MaxPool1d(2,stride=2,return_indices=True)>>>unpool=nn.MaxUnpool1d(2,stride=2)>>>input=torch.tensor([[[1.,2,3,4,5,6,7,8]]])>>>output,indices=pool(input)>>>unpool(output,indices)tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])>>># Example showcasing the use of output_size>>>input=torch.tensor([[[1.,2,3,4,5,6,7,8,9]]])>>>output,indices=pool(input)>>>unpool(output,indices,output_size=input.size())tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,'),\n",
       " Document(metadata={}, page_content='0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])>>>unpool(output,indices)tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#maxunpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d'),\n",
       " Document(metadata={}, page_content='MaxUnpool2d [LINK_1]  class torch.nn.MaxUnpool2d( kernel_size , stride=None , padding=0 ) [source]  [source]  [LINK_2]  Computes a partial inverse of MaxPool2d .  MaxPool2d is not fully invertible, since the non-maximal values are lost.  MaxUnpool2d takes in as input the output of MaxPool2d including the indices of the maximal values and computes a partial inverse'),\n",
       " Document(metadata={}, page_content='in which all non-maximal values are set to zero.  Note  This operation may behave nondeterministically when the input indices has repeat values.\\nSee https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.  Note  MaxPool2d can map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument output_size in the forward call.'),\n",
       " Document(metadata={}, page_content='See the Inputs and Example below.  Parameters  kernel_size ( int  or  tuple ) – Size of the max pooling window.  stride ( int  or  tuple ) – Stride of the max pooling window.'),\n",
       " Document(metadata={}, page_content='It is set to kernel_size by default.  padding ( int  or  tuple ) – Padding that was added to the input  Inputs:  input : the input Tensor to invert  indices : the indices given out by MaxPool2d  output_size (optional): the targeted output size  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out},'),\n",
       " Document(metadata={}, page_content=',  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  (  H  i  n  −  1  )  ×  stride[0]  −  2  ×  padding[0]  +  kernel_size[0]  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride[0]} - 2 \\\\times \\\\text{padding[0]} + \\\\text{kernel\\\\_size[0]}Hout\\u200b=(Hin\\u200b−1)×stride[0]−2×padding[0]+kernel_size[0]  W  o  u  t  =  (  W  i  n  −  1  )  ×  stride[1]  −  2  ×  padding[1]  +  kernel_size[1]'),\n",
       " Document(metadata={}, page_content='−  2  ×  padding[1]  +  kernel_size[1]  W_{out} = (W_{in} - 1) \\\\times \\\\text{stride[1]} - 2 \\\\times \\\\text{padding[1]} + \\\\text{kernel\\\\_size[1]}Wout\\u200b=(Win\\u200b−1)×stride[1]−2×padding[1]+kernel_size[1]  or as given by output_size in the call operator  Example:  >>>pool=nn.MaxPool2d(2,stride=2,return_indices=True)>>>unpool=nn.MaxUnpool2d(2,stride=2)>>>input=torch.tensor([[[[1.,2.,3.,4.],[ 5.,  6.,  7.,  8.],[ 9., 10., 11., 12.],[13., 14., 15.,'),\n",
       " Document(metadata={}, page_content='7.,  8.],[ 9., 10., 11., 12.],[13., 14., 15., 16.]]]])>>>output,indices=pool(input)>>>unpool(output,indices)tensor([[[[  0.,   0.,   0.,   0.],[  0.,   6.,   0.,   8.],[  0.,   0.,   0.,   0.],[  0.,  14.,   0.,  16.]]]])>>># Now using output_size to resolve an ambiguous size for the inverse>>>input=torch.tensor([[[[1.,2.,3.,4.,5.],[ 6.,  7.,  8.,  9., 10.],[11., 12., 13., 14., 15.],[16., 17., 18., 19., 20.]]]])>>>output,indices=pool(input)>>># This call will not work without specifying'),\n",
       " Document(metadata={}, page_content='This call will not work without specifying output_size>>>unpool(output,indices,output_size=input.size())tensor([[[[ 0.,  0.,  0.,  0.,  0.],[ 0.,  7.,  0.,  9.,  0.],[ 0.,  0.,  0.,  0.,  0.],[ 0., 17.,  0., 19.,  0.]]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#maxunpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d'),\n",
       " Document(metadata={}, page_content='MaxUnpool3d [LINK_1]  class torch.nn.MaxUnpool3d( kernel_size , stride=None , padding=0 ) [source]  [source]  [LINK_2]  Computes a partial inverse of MaxPool3d .  MaxPool3d is not fully invertible, since the non-maximal values are lost. MaxUnpool3d takes in as input the output of MaxPool3d including the indices of the maximal values and computes a partial inverse'),\n",
       " Document(metadata={}, page_content='in which all non-maximal values are set to zero.  Note  This operation may behave nondeterministically when the input indices has repeat values.\\nSee https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.  Note  MaxPool3d can map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument output_size in the forward call.'),\n",
       " Document(metadata={}, page_content='See the Inputs section below.  Parameters  kernel_size ( int  or  tuple ) – Size of the max pooling window.  stride ( int  or  tuple ) – Stride of the max pooling window.'),\n",
       " Document(metadata={}, page_content='It is set to kernel_size by default.  padding ( int  or  tuple ) – Padding that was added to the input  Inputs:  input : the input Tensor to invert  indices : the indices given out by MaxPool3d  output_size (optional): the targeted output size  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u'),\n",
       " Document(metadata={}, page_content='Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  (  D  i  n  −  1  )  ×  stride[0]  −  2  ×  padding[0]  +  kernel_size[0]  D_{out} = (D_{in} - 1) \\\\times \\\\text{stride[0]} - 2 \\\\times \\\\text{padding[0]} + \\\\text{kernel\\\\_size[0]}Dout\\u200b=(Din\\u200b−1)×stride[0]−2×padding[0]+kernel_size[0]  H  o  u  t  ='),\n",
       " Document(metadata={}, page_content='H  o  u  t  =  (  H  i  n  −  1  )  ×  stride[1]  −  2  ×  padding[1]  +  kernel_size[1]  H_{out} = (H_{in} - 1) \\\\times \\\\text{stride[1]} - 2 \\\\times \\\\text{padding[1]} + \\\\text{kernel\\\\_size[1]}Hout\\u200b=(Hin\\u200b−1)×stride[1]−2×padding[1]+kernel_size[1]  W  o  u  t  =  (  W  i  n  −  1  )  ×  stride[2]  −  2  ×  padding[2]  +  kernel_size[2]  W_{out} = (W_{in} - 1) \\\\times \\\\text{stride[2]} - 2 \\\\times \\\\text{padding[2]} + \\\\text{kernel\\\\_size[2]}Wout\\u200b=(Win\\u200b−1)×stride[2]−2×padding[2]+kernel_size[2]  or as'),\n",
       " Document(metadata={}, page_content='or as given by output_size in the call operator  Example:  >>># pool of square window of size=3, stride=2>>>pool=nn.MaxPool3d(3,stride=2,return_indices=True)>>>unpool=nn.MaxUnpool3d(3,stride=2)>>>output,indices=pool(torch.randn(20,16,51,33,15))>>>unpooled_output=unpool(output,indices)>>>unpooled_output.size()torch.Size([20, 16, 51, 33, 15])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#maxunpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d'),\n",
       " Document(metadata={}, page_content='AvgPool1d [LINK_1]  class torch.nn.AvgPool1d( kernel_size , stride=None , padding=0 , ceil_mode=False , count_include_pad=True ) [source]  [source]  [LINK_2]  Applies a 1D average pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  L  )  (N, C, L)(N,C,L),'),\n",
       " Document(metadata={}, page_content='output(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)and kernel_size k  kkcan be precisely described as:  out  (  N  i  ,  C  j  ,  l  )  =  1  k  ∑  m  =  0  k  −  1  input  (  N  i  ,  C  j  ,  stride  ×  l  +  m  )  \\\\text{out}(N_i, C_j, l) = \\\\frac{1}{k} \\\\sum_{m=0}^{k-1}\\n                       \\\\text{input}(N_i, C_j, \\\\text{stride} \\\\times l + m)out(Ni\\u200b,Cj\\u200b,l)=k1\\u200bm=0∑k−1\\u200binput(Ni\\u200b,Cj\\u200b,stride×l+m)  If padding is non-zero, then the input is implicitly zero-padded on both sides'),\n",
       " Document(metadata={}, page_content='for padding number of points.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride , padding can each be'),\n",
       " Document(metadata={}, page_content='an int or a one-element tuple.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ]  ] ) – implicit zero padding to be added on both sides  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  count_include_pad ( bool ) – when True, will include the zero-padding in'),\n",
       " Document(metadata={}, page_content=') – when True, will include the zero-padding in the averaging calculation  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), where  L  o  u  t  =  ⌊  L  i  n  +  2  ×  padding  −  kernel_size  stride  +  1  ⌋  L_{out} = \\\\left\\\\lfloor \\\\frac{L_{in} +'),\n",
       " Document(metadata={}, page_content='2 \\\\times \\\\text{padding} - \\\\text{kernel\\\\_size}}{\\\\text{stride}} + 1\\\\right\\\\rfloorLout\\u200b=⌊strideLin\\u200b+2×padding−kernel_size\\u200b+1⌋  Per the note above, if ceil_mode is True and(  L  o  u  t  −  1  )  ×  stride  ≥  L  i  n  +  padding  (L_{out} - 1) \\\\times \\\\text{stride} \\\\geq L_{in}'),\n",
       " Document(metadata={}, page_content='+ \\\\text{padding}(Lout\\u200b−1)×stride≥Lin\\u200b+padding, we skip the last window as it would start in the right padded region, resulting inL  o  u  t  L_{out}Lout\\u200bbeing reduced by one.  Examples:  >>># pool with window of size=3, stride=2>>>m=nn.AvgPool1d(3,stride=2)>>>m(torch.tensor([[[1.,2,3,4,5,6,7]]]))tensor([[[2., 4., 6.]]])\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html#avgpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d'),\n",
       " Document(metadata={}, page_content='AvgPool2d [LINK_1]  class torch.nn.AvgPool2d( kernel_size , stride=None , padding=0 , ceil_mode=False , count_include_pad=True , divisor_override=None ) [source]  [source]  [LINK_2]  Applies a 2D average pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W),'),\n",
       " Document(metadata={}, page_content='output(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)and kernel_size (  k  H  ,  k  W  )  (kH, kW)(kH,kW)can be precisely described as:  o  u  t  (  N  i  ,  C  j  ,  h  ,  w  )  =  1  k  H  ∗  k  W  ∑  m  =  0  k  H  −  1  ∑  n  =  0  k  W  −  1  i  n  p  u  t  (  N  i  ,  C  j  ,  s  t  r  i  d  e  [  0  ]  ×  h  +  m  ,  s  t  r  i  d  e  [  1  ]  ×  w  +  n  )  out(N_i, C_j, h, w)  = \\\\frac{1}{kH * kW} \\\\sum_{m=0}^{kH-1} \\\\sum_{n=0}^{kW-1}'),\n",
       " Document(metadata={}, page_content='input(N_i, C_j, stride[0] \\\\times h + m, stride[1] \\\\times w + n)out(Ni\\u200b,Cj\\u200b,h,w)=kH∗kW1\\u200bm=0∑kH−1\\u200bn=0∑kW−1\\u200binput(Ni\\u200b,Cj\\u200b,stride[0]×h+m,stride[1]×w+n)  If padding is non-zero, then the input is implicitly zero-padded on both sides\\nfor padding number of points.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding'),\n",
       " Document(metadata={}, page_content='or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride , padding can either be:  a single int – in which case the same value is used for the height and width dimension  a tuple of two ints – in which case, the first int is used for the height dimension,'),\n",
       " Document(metadata={}, page_content='and the second int for the width dimension  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – implicit zero padding to be added on both sides  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  count_include_pad ( bool ) – when True,'),\n",
       " Document(metadata={}, page_content='shape  count_include_pad ( bool ) – when True, will include the zero-padding in the averaging calculation  divisor_override ( Optional  [  int  ] ) – if specified, it will be used as divisor, otherwise size of the pooling region will be used.  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out},'),\n",
       " Document(metadata={}, page_content=',  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  0  ]  −  kernel_size  [  0  ]  stride  [  0  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times \\\\text{padding}[0] -'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[0]Hin\\u200b+2×padding[0]−kernel_size[0]\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  1  ]  −  kernel_size  [  1  ]  stride  [  1  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times \\\\text{padding}[1] -'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[1]Win\\u200b+2×padding[1]−kernel_size[1]\\u200b+1⌋  Per the note above, if ceil_mode is True and(  H  o  u  t  −  1  )  ×  stride  [  0  ]  ≥  H  i  n  +  padding  [  0  ]  (H_{out} - 1)\\\\times \\\\text{stride}[0]\\\\geq H_{in}\\n+ \\\\text{padding}[0](Hout\\u200b−1)×stride[0]≥Hin\\u200b+padding[0], we skip the last window as it would start in the bottom padded region,'),\n",
       " Document(metadata={}, page_content='resulting inH  o  u  t  H_{out}Hout\\u200bbeing reduced by one.  The same applies forW  o  u  t  W_{out}Wout\\u200b.  Examples:  >>># pool of square window of size=3, stride=2>>>m=nn.AvgPool2d(3,stride=2)>>># pool of non-square window>>>m=nn.AvgPool2d((3,2),stride=(2,1))>>>input=torch.randn(20,16,50,32)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#avgpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d'),\n",
       " Document(metadata={}, page_content='AvgPool3d [LINK_1]  class torch.nn.AvgPool3d( kernel_size , stride=None , padding=0 , ceil_mode=False , count_include_pad=True , divisor_override=None ) [source]  [source]  [LINK_2]  Applies a 3D average pooling over an input signal composed of several input planes.  In the simplest case, the output value of the layer with input size(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W),'),\n",
       " Document(metadata={}, page_content='output(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)and kernel_size (  k  D  ,  k  H  ,  k  W  )  (kD, kH, kW)(kD,kH,kW)can be precisely described as:  out  (  N  i  ,  C  j  ,  d  ,  h  ,  w  )  =  ∑  k  =  0  k  D  −  1  ∑  m  =  0  k  H  −  1  ∑  n  =  0  k  W  −  1  input  (  N  i  ,  C  j  ,  stride  [  0  ]  ×  d  +  k  ,  stride  [  1  ]  ×  h  +  m  ,  stride  [  2  ]  ×  w  +  n  )  k  D  ×  k  H  ×  k  W'),\n",
       " Document(metadata={}, page_content='[  2  ]  ×  w  +  n  )  k  D  ×  k  H  ×  k  W  \\\\begin{aligned}'),\n",
       " Document(metadata={}, page_content='\\\\text{out}(N_i, C_j, d, h, w) ={} & \\\\sum_{k=0}^{kD-1} \\\\sum_{m=0}^{kH-1} \\\\sum_{n=0}^{kW-1} \\\\\\\\\\n                                      & \\\\frac{\\\\text{input}(N_i, C_j, \\\\text{stride}[0] \\\\times d + k,\\n                                              \\\\text{stride}[1] \\\\times h + m, \\\\text{stride}[2] \\\\times w + n)}\\n                                             {kD \\\\times kH \\\\times kW}'),\n",
       " Document(metadata={}, page_content='\\\\end{aligned}out(Ni\\u200b,Cj\\u200b,d,h,w)=\\u200bk=0∑kD−1\\u200bm=0∑kH−1\\u200bn=0∑kW−1\\u200bkD×kH×kWinput(Ni\\u200b,Cj\\u200b,stride[0]×d+k,stride[1]×h+m,stride[2]×w+n)\\u200b\\u200b  If padding is non-zero, then the input is implicitly zero-padded on all three sides\\nfor padding number of points.  Note  When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding'),\n",
       " Document(metadata={}, page_content='or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size , stride can either be:  a single int – in which case the same value is used for the depth, height and width dimension  a tuple of three ints – in which case, the first int is used for the depth dimension,'),\n",
       " Document(metadata={}, page_content='the second int for the height dimension and the third int for the width dimension  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  padding ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – implicit zero padding to be added on all three sides  ceil_mode ( bool ) – when True, will use ceil instead of floor'),\n",
       " Document(metadata={}, page_content=') – when True, will use ceil instead of floor to compute the output shape  count_include_pad ( bool ) – when True, will include the zero-padding in the averaging calculation  divisor_override ( Optional  [  int  ] ) – if specified, it will be used as divisor, otherwise kernel_size will be used  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in},'),\n",
       " Document(metadata={}, page_content='n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  ⌊  D  i  n  +  2  ×  padding  [  0  ]  −  kernel_size  [  0  ]  stride  [  0  ]  +  1  ⌋  D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] -'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorDout\\u200b=⌊stride[0]Din\\u200b+2×padding[0]−kernel_size[0]\\u200b+1⌋  H  o  u  t  =  ⌊  H  i  n  +  2  ×  padding  [  1  ]  −  kernel_size  [  1  ]  stride  [  1  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] -'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[1]Hin\\u200b+2×padding[1]−kernel_size[1]\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  +  2  ×  padding  [  2  ]  −  kernel_size  [  2  ]  stride  [  2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] -'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size}[2]}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[2]Win\\u200b+2×padding[2]−kernel_size[2]\\u200b+1⌋  Per the note above, if ceil_mode is True and(  D  o  u  t  −  1  )  ×  stride  [  0  ]  ≥  D  i  n  +  padding  [  0  ]  (D_{out} - 1)\\\\times \\\\text{stride}[0]\\\\geq D_{in}\\n+ \\\\text{padding}[0](Dout\\u200b−1)×stride[0]≥Din\\u200b+padding[0], we skip the last window as it would start in the padded region,'),\n",
       " Document(metadata={}, page_content='resulting inD  o  u  t  D_{out}Dout\\u200bbeing reduced by one.  The same applies forW  o  u  t  W_{out}Wout\\u200bandH  o  u  t  H_{out}Hout\\u200b.  Examples:  >>># pool of square window of size=3, stride=2>>>m=nn.AvgPool3d(3,stride=2)>>># pool of non-square window>>>m=nn.AvgPool3d((3,2,2),stride=(2,1,2))>>>input=torch.randn(20,16,50,44,31)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html#avgpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d'),\n",
       " Document(metadata={}, page_content='FractionalMaxPool2d [LINK_1]  class torch.nn.FractionalMaxPool2d( kernel_size , output_size=None , output_ratio=None , return_indices=False , _random_samples=None ) [source]  [source]  [LINK_2]  Applies a 2D fractional max pooling over an input signal composed of several input planes.  Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham  The max-pooling operation is applied ink  H  ×  k  W  kH \\\\times kWkH×kWregions by a stochastic'),\n",
       " Document(metadata={}, page_content='step size determined by the target output size.\\nThe number of output features is equal to the number of input planes.  Note  Exactly one of output_size or output_ratio must be defined.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the size of the window to take a max over.'),\n",
       " Document(metadata={}, page_content='Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)  output_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the target output size of the image of the form oH x oW .\\nCan be a tuple (oH, oW) or a single number oH for a square image oH x oH .'),\n",
       " Document(metadata={}, page_content='Note that we must havek  H  +  o  H  −  1  <  =  H  i  n  kH + oH - 1 <= H_{in}kH+oH−1<=Hin\\u200bandk  W  +  o  W  −  1  <  =  W  i  n  kW + oW - 1 <= W_{in}kW+oW−1<=Win\\u200b  output_ratio ( Union  [  float  ,  Tuple  [  float  ,  float  ]  ] ) – If one wants to have an output size as a ratio of the input size, this option can be given.\\nThis has to be a number or tuple in the range (0, 1).'),\n",
       " Document(metadata={}, page_content='Note that we must havek  H  +  (  o  u  t  p  u  t  _  r  a  t  i  o  _  H  ∗  H  i  n  )  −  1  <  =  H  i  n  kH + (output\\\\_ratio\\\\_H * H_{in}) - 1 <= H_{in}kH+(output_ratio_H∗Hin\\u200b)−1<=Hin\\u200bandk  W  +  (  o  u  t  p  u  t  _  r  a  t  i  o  _  W  ∗  W  i  n  )  −  1  <  =  W  i  n  kW + (output\\\\_ratio\\\\_W * W_{in}) - 1 <= W_{in}kW+(output_ratio_W∗Win\\u200b)−1<=Win\\u200b  return_indices ( bool ) – if True , will return the indices along with the outputs.'),\n",
       " Document(metadata={}, page_content='Useful to pass to nn.MaxUnpool2d() . Default: False  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where(  H  o  u  t  ,  W  o  u  t  )  =  output_size  (H_{out},'),\n",
       " Document(metadata={}, page_content='u  t  ,  W  o  u  t  )  =  output_size  (H_{out}, W_{out})=\\\\text{output\\\\_size}(Hout\\u200b,Wout\\u200b)=output_sizeor(  H  o  u  t  ,  W  o  u  t  )  =  output_ratio  ×  (  H  i  n  ,  W  i  n  )  (H_{out}, W_{out})=\\\\text{output\\\\_ratio} \\\\times (H_{in}, W_{in})(Hout\\u200b,Wout\\u200b)=output_ratio×(Hin\\u200b,Win\\u200b).  Examples  >>># pool of square window of size=3, and target output size 13x12>>>m=nn.FractionalMaxPool2d(3,output_size=(13,12))>>># pool of square window and target output size being half of input image'),\n",
       " Document(metadata={}, page_content='and target output size being half of input image size>>>m=nn.FractionalMaxPool2d(3,output_ratio=(0.5,0.5))>>>input=torch.randn(20,16,50,32)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html#fractionalmaxpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d'),\n",
       " Document(metadata={}, page_content='FractionalMaxPool3d [LINK_1]  class torch.nn.FractionalMaxPool3d( kernel_size , output_size=None , output_ratio=None , return_indices=False , _random_samples=None ) [source]  [source]  [LINK_2]  Applies a 3D fractional max pooling over an input signal composed of several input planes.  Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham  The max-pooling operation is applied ink  T  ×  k  H  ×  k  W  kT \\\\times kH \\\\times kWkT×kH×kWregions by a stochastic'),\n",
       " Document(metadata={}, page_content='step size determined by the target output size.\\nThe number of output features is equal to the number of input planes.  Note  Exactly one of output_size or output_ratio must be defined.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the size of the window to take a max over.'),\n",
       " Document(metadata={}, page_content='Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)  output_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the target output size of the image of the form oT x oH x oW .\\nCan be a tuple (oT, oH, oW) or a single number oH for a square image oH x oH x oH  output_ratio ( Union  [  float  ,  Tuple  [  float  ,  float  ,  float  ]  ] ) – If one wants to have an output size as a ratio of the input size, this option can be given.'),\n",
       " Document(metadata={}, page_content='This has to be a number or tuple in the range (0, 1)  return_indices ( bool ) – if True , will return the indices along with the outputs.'),\n",
       " Document(metadata={}, page_content='Useful to pass to nn.MaxUnpool3d() . Default: False  Shape:  Input:(  N  ,  C  ,  T  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, T_{in}, H_{in}, W_{in})(N,C,Tin\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  T  i  n  ,  H  i  n  ,  W  i  n  )  (C, T_{in}, H_{in}, W_{in})(C,Tin\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  T  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, T_{out}, H_{out}, W_{out})(N,C,Tout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  T  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, T_{out}, H_{out}, W_{out})(C,Tout\\u200b,Hout\\u200b,Wout\\u200b),'),\n",
       " Document(metadata={}, page_content='T_{out}, H_{out}, W_{out})(C,Tout\\u200b,Hout\\u200b,Wout\\u200b), where(  T  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  =  output_size  (T_{out}, H_{out}, W_{out})=\\\\text{output\\\\_size}(Tout\\u200b,Hout\\u200b,Wout\\u200b)=output_sizeor(  T  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  =  output_ratio  ×  (  T  i  n  ,  H  i  n  ,  W  i  n  )  (T_{out}, H_{out}, W_{out})=\\\\text{output\\\\_ratio} \\\\times (T_{in}, H_{in}, W_{in})(Tout\\u200b,Hout\\u200b,Wout\\u200b)=output_ratio×(Tin\\u200b,Hin\\u200b,Win\\u200b)  Examples  >>># pool of cubic window of size=3, and target'),\n",
       " Document(metadata={}, page_content='>>># pool of cubic window of size=3, and target output size 13x12x11>>>m=nn.FractionalMaxPool3d(3,output_size=(13,12,11))>>># pool of cubic window and target output size being half of input size>>>m=nn.FractionalMaxPool3d(3,output_ratio=(0.5,0.5,0.5))>>>input=torch.randn(20,16,50,32,16)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html#fractionalmaxpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d'),\n",
       " Document(metadata={}, page_content='LPPool1d [LINK_1]  class torch.nn.LPPool1d( norm_type , kernel_size , stride=None , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 1D power-average pooling over an input signal composed of several input planes.  On each window, the function computed is:  f  (  X  )  =  ∑  x  ∈  X  x  p  p  f(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}f(X)=p\\u200bx∈X∑\\u200bxp\\u200b  At p =∞  \\\\infty∞, one gets Max Pooling  At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)  Note  If the sum to the power'),\n",
       " Document(metadata={}, page_content='Average Pooling)  Note  If the sum to the power of p is zero, the gradient of this function is'),\n",
       " Document(metadata={}, page_content='not defined. This implementation will set the gradient to zero in this case.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – a single int, the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ]  ] ) – a single int, the stride of the window. Default value is kernel_size  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C,'),\n",
       " Document(metadata={}, page_content='C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), where  L  o  u  t  =  ⌊  L  i  n  −  kernel_size  stride  +  1  ⌋  L_{out} = \\\\left\\\\lfloor\\\\frac{L_{in} - \\\\text{kernel\\\\_size}}{\\\\text{stride}} + 1\\\\right\\\\rfloorLout\\u200b=⌊strideLin\\u200b−kernel_size\\u200b+1⌋  Examples::  >>># power-2 pool of window of length 3, with stride'),\n",
       " Document(metadata={}, page_content='power-2 pool of window of length 3, with stride 2.>>>m=nn.LPPool1d(2,3,stride=2)>>>input=torch.randn(20,16,50)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html#lppool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d'),\n",
       " Document(metadata={}, page_content='LPPool2d [LINK_1]  class torch.nn.LPPool2d( norm_type , kernel_size , stride=None , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 2D power-average pooling over an input signal composed of several input planes.  On each window, the function computed is:  f  (  X  )  =  ∑  x  ∈  X  x  p  p  f(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}f(X)=p\\u200bx∈X∑\\u200bxp\\u200b  At p =∞  \\\\infty∞, one gets Max Pooling  At p = 1, one gets Sum Pooling (which is proportional to average pooling)  The parameters kernel_size ,'),\n",
       " Document(metadata={}, page_content='to average pooling)  The parameters kernel_size , stride can either be:  a single int – in which case the same value is used for the height and width dimension  a tuple of two ints – in which case, the first int is used for the height dimension,'),\n",
       " Document(metadata={}, page_content='and the second int for the width dimension  Note  If the sum to the power of p is zero, the gradient of this function is'),\n",
       " Document(metadata={}, page_content='not defined. This implementation will set the gradient to zero in this case.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W'),\n",
       " Document(metadata={}, page_content='W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  ⌊  H  i  n  −  kernel_size  [  0  ]  stride  [  0  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} - \\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[0]Hin\\u200b−kernel_size[0]\\u200b+1⌋  W  o  u  t  ='),\n",
       " Document(metadata={}, page_content='W  o  u  t  =  ⌊  W  i  n  −  kernel_size  [  1  ]  stride  [  1  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} - \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[1]Win\\u200b−kernel_size[1]\\u200b+1⌋  Examples:  >>># power-2 pool of square window of size=3, stride=2>>>m=nn.LPPool2d(2,3,stride=2)>>># pool of non-square window of power 1.2>>>m=nn.LPPool2d(1.2,(3,2),stride=(2,1))>>>input=torch.randn(20,16,50,32)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html#lppool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d'),\n",
       " Document(metadata={}, page_content='LPPool3d [LINK_1]  class torch.nn.LPPool3d( norm_type , kernel_size , stride=None , ceil_mode=False ) [source]  [source]  [LINK_2]  Applies a 3D power-average pooling over an input signal composed of several input planes.  On each window, the function computed is:  f  (  X  )  =  ∑  x  ∈  X  x  p  p  f(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}f(X)=p\\u200bx∈X∑\\u200bxp\\u200b  At p =∞  \\\\infty∞, one gets Max Pooling  At p = 1, one gets Sum Pooling (which is proportional to average pooling)  The parameters kernel_size ,'),\n",
       " Document(metadata={}, page_content='to average pooling)  The parameters kernel_size , stride can either be:  a single int – in which case the same value is used for the height, width and depth dimension  a tuple of three ints – in which case, the first int is used for the depth dimension,'),\n",
       " Document(metadata={}, page_content='the second int for the height dimension and the third int for the width dimension  Note  If the sum to the power of p is zero, the gradient of this function is'),\n",
       " Document(metadata={}, page_content='not defined. This implementation will set the gradient to zero in this case.  Parameters  kernel_size ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the size of the window  stride ( Union  [  int  ,  Tuple  [  int  ,  int  ,  int  ]  ] ) – the stride of the window. Default value is kernel_size  ceil_mode ( bool ) – when True, will use ceil instead of floor to compute the output shape  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in},'),\n",
       " Document(metadata={}, page_content=',  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  ⌊  D  i  n  −  kernel_size  [  0  ]  stride  [  0  ]  +  1  ⌋  D_{out} ='),\n",
       " Document(metadata={}, page_content='[  0  ]  stride  [  0  ]  +  1  ⌋  D_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} - \\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloorDout\\u200b=⌊stride[0]Din\\u200b−kernel_size[0]\\u200b+1⌋  H  o  u  t  =  ⌊  H  i  n  −  kernel_size  [  1  ]  stride  [  1  ]  +  1  ⌋  H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} - \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloorHout\\u200b=⌊stride[1]Hin\\u200b−kernel_size[1]\\u200b+1⌋  W  o  u  t  =  ⌊  W  i  n  −  kernel_size  [  2  ]  stride  [  2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}'),\n",
       " Document(metadata={}, page_content='2  ]  +  1  ⌋  W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} - \\\\text{kernel\\\\_size}[2]}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloorWout\\u200b=⌊stride[2]Win\\u200b−kernel_size[2]\\u200b+1⌋  Examples:  >>># power-2 pool of square window of size=3, stride=2>>>m=nn.LPPool3d(2,3,stride=2)>>># pool of non-square window of power 1.2>>>m=nn.LPPool3d(1.2,(3,2,2),stride=(2,1,2))>>>input=torch.randn(20,16,50,44,31)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html#lppool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LPPool3d.html#torch.nn.LPPool3d'),\n",
       " Document(metadata={}, page_content='AdaptiveMaxPool1d [LINK_1]  class torch.nn.AdaptiveMaxPool1d( output_size , return_indices=False ) [source]  [source]  [LINK_2]  Applies a 1D adaptive max pooling over an input signal composed of several input planes.  The output size isL  o  u  t  L_{out}Lout\\u200b, for any input size.'),\n",
       " Document(metadata={}, page_content='The number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – the target output sizeL  o  u  t  L_{out}Lout\\u200b.  return_indices ( bool ) – if True , will return the indices along with the outputs.'),\n",
       " Document(metadata={}, page_content='Useful to pass to nn.MaxUnpool1d. Default: False  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), whereL  o  u  t  =  output_size  L_{out}=\\\\text{output\\\\_size}Lout\\u200b=output_size.  Examples  >>># target output size of 5>>>m=nn.AdaptiveMaxPool1d(5)>>>input=torch.randn(1,64,8)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html#adaptivemaxpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d'),\n",
       " Document(metadata={}, page_content='AdaptiveMaxPool2d [LINK_1]  class torch.nn.AdaptiveMaxPool2d( output_size , return_indices=False ) [source]  [source]  [LINK_2]  Applies a 2D adaptive max pooling over an input signal composed of several input planes.  The output is of sizeH  o  u  t  ×  W  o  u  t  H_{out} \\\\times W_{out}Hout\\u200b×Wout\\u200b, for any input size.'),\n",
       " Document(metadata={}, page_content='The number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  None  ,  Tuple  [  Optional  [  int  ]  ,  Optional  [  int  ]  ]  ] ) – the target output size of the image of the formH  o  u  t  ×  W  o  u  t  H_{out} \\\\times W_{out}Hout\\u200b×Wout\\u200b.\\nCan be a tuple(  H  o  u  t  ,  W  o  u  t  )  (H_{out}, W_{out})(Hout\\u200b,Wout\\u200b)or a singleH  o  u  t  H_{out}Hout\\u200bfor a'),\n",
       " Document(metadata={}, page_content='square imageH  o  u  t  ×  H  o  u  t  H_{out} \\\\times H_{out}Hout\\u200b×Hout\\u200b.H  o  u  t  H_{out}Hout\\u200bandW  o  u  t  W_{out}Wout\\u200bcan be either a int , or None which means the size will be the same as that\\nof the input.  return_indices ( bool ) – if True , will return the indices along with the outputs.'),\n",
       " Document(metadata={}, page_content='Useful to pass to nn.MaxUnpool2d. Default: False  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where(  H  o  u  t  ,  W  o  u  t  )  =  output_size  (H_{out},'),\n",
       " Document(metadata={}, page_content='u  t  ,  W  o  u  t  )  =  output_size  (H_{out}, W_{out})=\\\\text{output\\\\_size}(Hout\\u200b,Wout\\u200b)=output_size.  Examples  >>># target output size of 5x7>>>m=nn.AdaptiveMaxPool2d((5,7))>>>input=torch.randn(1,64,8,9)>>>output=m(input)>>># target output size of 7x7 (square)>>>m=nn.AdaptiveMaxPool2d(7)>>>input=torch.randn(1,64,10,9)>>>output=m(input)>>># target output size of 10x7>>>m=nn.AdaptiveMaxPool2d((None,7))>>>input=torch.randn(1,64,10,9)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#adaptivemaxpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d'),\n",
       " Document(metadata={}, page_content='AdaptiveMaxPool3d [LINK_1]  class torch.nn.AdaptiveMaxPool3d( output_size , return_indices=False ) [source]  [source]  [LINK_2]  Applies a 3D adaptive max pooling over an input signal composed of several input planes.  The output is of sizeD  o  u  t  ×  H  o  u  t  ×  W  o  u  t  D_{out} \\\\times H_{out} \\\\times W_{out}Dout\\u200b×Hout\\u200b×Wout\\u200b, for any input size.'),\n",
       " Document(metadata={}, page_content='The number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  None  ,  Tuple  [  Optional  [  int  ]  ,  Optional  [  int  ]  ,  Optional  [  int  ]  ]  ] ) – the target output size of the image of the formD  o  u  t  ×  H  o  u  t  ×  W  o  u  t  D_{out} \\\\times H_{out} \\\\times W_{out}Dout\\u200b×Hout\\u200b×Wout\\u200b.'),\n",
       " Document(metadata={}, page_content='Can be a tuple(  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (D_{out}, H_{out}, W_{out})(Dout\\u200b,Hout\\u200b,Wout\\u200b)or a singleD  o  u  t  D_{out}Dout\\u200bfor a cubeD  o  u  t  ×  D  o  u  t  ×  D  o  u  t  D_{out} \\\\times D_{out} \\\\times D_{out}Dout\\u200b×Dout\\u200b×Dout\\u200b.D  o  u  t  D_{out}Dout\\u200b,H  o  u  t  H_{out}Hout\\u200bandW  o  u  t  W_{out}Wout\\u200bcan be either a int , or None which means the size will be the same as that of the input.  return_indices ( bool ) – if True , will return the indices along with the'),\n",
       " Document(metadata={}, page_content='if True , will return the indices along with the outputs.'),\n",
       " Document(metadata={}, page_content='Useful to pass to nn.MaxUnpool3d. Default: False  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b),'),\n",
       " Document(metadata={}, page_content='where(  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  =  output_size  (D_{out}, H_{out}, W_{out})=\\\\text{output\\\\_size}(Dout\\u200b,Hout\\u200b,Wout\\u200b)=output_size.  Examples  >>># target output size of 5x7x9>>>m=nn.AdaptiveMaxPool3d((5,7,9))>>>input=torch.randn(1,64,8,9,10)>>>output=m(input)>>># target output size of 7x7x7 (cube)>>>m=nn.AdaptiveMaxPool3d(7)>>>input=torch.randn(1,64,10,9,8)>>>output=m(input)>>># target output size of'),\n",
       " Document(metadata={}, page_content='target output size of 7x9x8>>>m=nn.AdaptiveMaxPool3d((7,None,None))>>>input=torch.randn(1,64,10,9,8)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html#adaptivemaxpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d'),\n",
       " Document(metadata={}, page_content='AdaptiveAvgPool1d [LINK_1]  class torch.nn.AdaptiveAvgPool1d( output_size ) [source]  [source]  [LINK_2]  Applies a 1D adaptive average pooling over an input signal composed of several input planes.  The output size isL  o  u  t  L_{out}Lout\\u200b, for any input size.'),\n",
       " Document(metadata={}, page_content='The number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  Tuple  [  int  ]  ] ) – the target output sizeL  o  u  t  L_{out}Lout\\u200b.  Shape:  Input:(  N  ,  C  ,  L  i  n  )  (N, C, L_{in})(N,C,Lin\\u200b)or(  C  ,  L  i  n  )  (C, L_{in})(C,Lin\\u200b).  Output:(  N  ,  C  ,  L  o  u  t  )  (N, C, L_{out})(N,C,Lout\\u200b)or(  C  ,  L  o  u  t  )  (C, L_{out})(C,Lout\\u200b), whereL  o  u  t  =  output_size  L_{out}=\\\\text{output\\\\_size}Lout\\u200b=output_size.  Examples'),\n",
       " Document(metadata={}, page_content='Examples  >>># target output size of 5>>>m=nn.AdaptiveAvgPool1d(5)>>>input=torch.randn(1,64,8)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html#adaptiveavgpool1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d'),\n",
       " Document(metadata={}, page_content='AdaptiveAvgPool2d [LINK_1]  class torch.nn.AdaptiveAvgPool2d( output_size ) [source]  [source]  [LINK_2]  Applies a 2D adaptive average pooling over an input signal composed of several input planes.  The output is of size H x W, for any input size.\\nThe number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  None  ,  Tuple  [  Optional  [  int  ]  ,  Optional  [  int  ]  ]  ] ) – the target output size of the image of the form H x W.'),\n",
       " Document(metadata={}, page_content='Can be a tuple (H, W) or a single H for a square image H x H.\\nH and W can be either a int , or None which means the size will'),\n",
       " Document(metadata={}, page_content='be the same as that of the input.  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  S  0  ,  S  1  )  (N, C, S_{0}, S_{1})(N,C,S0\\u200b,S1\\u200b)or(  C  ,  S  0  ,  S  1  )  (C, S_{0}, S_{1})(C,S0\\u200b,S1\\u200b), whereS  =  output_size  S=\\\\text{output\\\\_size}S=output_size.  Examples  >>># target output size of'),\n",
       " Document(metadata={}, page_content='Examples  >>># target output size of 5x7>>>m=nn.AdaptiveAvgPool2d((5,7))>>>input=torch.randn(1,64,8,9)>>>output=m(input)>>># target output size of 7x7 (square)>>>m=nn.AdaptiveAvgPool2d(7)>>>input=torch.randn(1,64,10,9)>>>output=m(input)>>># target output size of 10x7>>>m=nn.AdaptiveAvgPool2d((None,7))>>>input=torch.randn(1,64,10,9)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#adaptiveavgpool2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d'),\n",
       " Document(metadata={}, page_content='AdaptiveAvgPool3d [LINK_1]  class torch.nn.AdaptiveAvgPool3d( output_size ) [source]  [source]  [LINK_2]  Applies a 3D adaptive average pooling over an input signal composed of several input planes.  The output is of size D x H x W, for any input size.'),\n",
       " Document(metadata={}, page_content='The number of output features is equal to the number of input planes.  Parameters  output_size ( Union  [  int  ,  None  ,  Tuple  [  Optional  [  int  ]  ,  Optional  [  int  ]  ,  Optional  [  int  ]  ]  ] ) – the target output size of the form D x H x W.\\nCan be a tuple (D, H, W) or a single number D for a cube D x D x D.\\nD, H and W can be either a int , or None which means the size will'),\n",
       " Document(metadata={}, page_content='be the same as that of the input.  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  S  0  ,  S  1  ,  S  2  )  (N, C, S_{0}, S_{1}, S_{2})(N,C,S0\\u200b,S1\\u200b,S2\\u200b)or(  C  ,  S  0  ,  S  1  ,  S  2  )  (C, S_{0}, S_{1}, S_{2})(C,S0\\u200b,S1\\u200b,S2\\u200b),'),\n",
       " Document(metadata={}, page_content='whereS  =  output_size  S=\\\\text{output\\\\_size}S=output_size.  Examples  >>># target output size of 5x7x9>>>m=nn.AdaptiveAvgPool3d((5,7,9))>>>input=torch.randn(1,64,8,9,10)>>>output=m(input)>>># target output size of 7x7x7 (cube)>>>m=nn.AdaptiveAvgPool3d(7)>>>input=torch.randn(1,64,10,9,8)>>>output=m(input)>>># target output size of 7x9x8>>>m=nn.AdaptiveAvgPool3d((7,None,None))>>>input=torch.randn(1,64,10,9,8)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html#adaptiveavgpool3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d'),\n",
       " Document(metadata={}, page_content='ReflectionPad1d [LINK_1]  class torch.nn.ReflectionPad1d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using the reflection of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 2- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ReflectionPad1d(2)>>>input=torch.arange(8,dtype=torch.float).reshape(1,2,4)>>>inputtensor([[[0., 1., 2., 3.],[4., 5., 6., 7.]]])>>>m(input)tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],[6., 5., 4., 5., 6., 7., 6., 5.]]])>>># using different paddings for different sides>>>m=nn.ReflectionPad1d((3,1))>>>m(input)tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],[7., 6., 5., 4., 5., 6., 7., 6.]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html#reflectionpad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d'),\n",
       " Document(metadata={}, page_content='ReflectionPad2d [LINK_1]  class torch.nn.ReflectionPad2d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using the reflection of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same'),\n",
       " Document(metadata={}, page_content='padding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)'),\n",
       " Document(metadata={}, page_content='Note that padding size should be less than the corresponding input dimension.  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b)where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} +'),\n",
       " Document(metadata={}, page_content='+  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ReflectionPad2d(2)>>>input=torch.arange(9,dtype=torch.float).reshape(1,1,3,3)>>>inputtensor([[[[0., 1., 2.],[3., 4., 5.],[6., 7., 8.]]]])>>>m(input)tensor([[[[8., 7., 6., 7., 8., 7., 6.],[5., 4.,'),\n",
       " Document(metadata={}, page_content='7., 6., 7., 8., 7., 6.],[5., 4., 3., 4., 5., 4., 3.],[2., 1., 0., 1., 2., 1., 0.],[5., 4., 3., 4., 5., 4., 3.],[8., 7., 6., 7., 8., 7., 6.],[5., 4., 3., 4., 5., 4., 3.],[2., 1., 0., 1., 2., 1., 0.]]]])>>># using different paddings for different sides>>>m=nn.ReflectionPad2d((1,1,2,0))>>>m(input)tensor([[[[7., 6., 7., 8., 7.],[4., 3., 4., 5., 4.],[1., 0., 1., 2., 1.],[4., 3., 4., 5., 4.],[7., 6., 7., 8., 7.]]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html#reflectionpad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d'),\n",
       " Document(metadata={}, page_content='ReflectionPad3d [LINK_1]  class torch.nn.ReflectionPad3d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using the reflection of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in},'),\n",
       " Document(metadata={}, page_content='n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b),'),\n",
       " Document(metadata={}, page_content='where  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ReflectionPad3d(1)>>>input=torch.arange(8,dtype=torch.float).reshape(1,1,2,2,2)>>>m(input)tensor([[[[[7., 6., 7., 6.],[5., 4., 5., 4.],[7., 6., 7., 6.],[5., 4., 5., 4.]],[[3., 2., 3., 2.],[1., 0., 1., 0.],[3., 2., 3., 2.],[1., 0., 1., 0.]],[[7., 6., 7., 6.],[5., 4., 5., 4.],[7., 6., 7., 6.],[5., 4., 5., 4.]],[[3., 2., 3., 2.],[1., 0., 1., 0.],[3., 2., 3., 2.],[1., 0., 1., 0.]]]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html#reflectionpad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d'),\n",
       " Document(metadata={}, page_content='ReplicationPad1d [LINK_1]  class torch.nn.ReplicationPad1d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using replication of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 2- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ReplicationPad1d(2)>>>input=torch.arange(8,dtype=torch.float).reshape(1,2,4)>>>inputtensor([[[0., 1., 2., 3.],[4., 5., 6., 7.]]])>>>m(input)tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],[4., 4., 4., 5., 6., 7., 7., 7.]]])>>># using different paddings for different sides>>>m=nn.ReplicationPad1d((3,1))>>>m(input)tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],[4., 4., 4., 4., 5., 6., 7., 7.]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html#replicationpad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d'),\n",
       " Document(metadata={}, page_content='ReplicationPad2d [LINK_1]  class torch.nn.ReplicationPad2d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using replication of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same'),\n",
       " Document(metadata={}, page_content='padding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out},'),\n",
       " Document(metadata={}, page_content=',  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ReplicationPad2d(2)>>>input=torch.arange(9,dtype=torch.float).reshape(1,1,3,3)>>>inputtensor([[[[0., 1., 2.],[3., 4., 5.],[6., 7., 8.]]]])>>>m(input)tensor([[[[0., 0., 0., 1., 2., 2., 2.],[0., 0., 0., 1., 2., 2., 2.],[0., 0., 0., 1., 2., 2., 2.],[3., 3., 3., 4., 5., 5., 5.],[6., 6., 6., 7., 8., 8., 8.],[6., 6., 6., 7., 8., 8., 8.],[6., 6., 6., 7., 8., 8., 8.]]]])>>># using different paddings for different sides>>>m=nn.ReplicationPad2d((1,1,2,0))>>>m(input)tensor([[[[0., 0.,'),\n",
       " Document(metadata={}, page_content='0., 1., 2., 2.],[0., 0., 1., 2., 2.],[0., 0., 1., 2., 2.],[3., 3., 4., 5., 5.],[6., 6., 7., 8., 8.]]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#replicationpad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d'),\n",
       " Document(metadata={}, page_content='ReplicationPad3d [LINK_1]  class torch.nn.ReplicationPad3d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using replication of the input boundary.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in},'),\n",
       " Document(metadata={}, page_content='n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b),'),\n",
       " Document(metadata={}, page_content='where  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ReplicationPad3d(3)>>>input=torch.randn(16,3,8,320,480)>>>output=m(input)>>># using different paddings for different sides>>>m=nn.ReplicationPad3d((3,3,6,6,1,1))>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html#replicationpad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d'),\n",
       " Document(metadata={}, page_content='ZeroPad1d [LINK_1]  class torch.nn.ZeroPad1d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with zero.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in both boundaries. If a 2- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ZeroPad1d(2)>>>input=torch.randn(1,2,4)>>>inputtensor([[[-1.0491, -0.7152, -0.0749,  0.8530],[-1.3287,  1.8966,  0.1466, -0.2771]]])>>>m(input)tensor([[[ 0.0000,  0.0000, -1.0491, -0.7152, -0.0749,  0.8530,  0.0000,0.0000],[ 0.0000,  0.0000, -1.3287,  1.8966,  0.1466, -0.2771,  0.0000,0.0000]]])>>>m=nn.ZeroPad1d(2)>>>input=torch.randn(1,2,3)>>>inputtensor([[[ 1.6616,  1.4523, -1.1255],[-3.6372,  0.1182, -1.8652]]])>>>m(input)tensor([[[ 0.0000,  0.0000,  1.6616,  1.4523,'),\n",
       " Document(metadata={}, page_content='0.0000,  0.0000,  1.6616,  1.4523, -1.1255,  0.0000,  0.0000],[ 0.0000,  0.0000, -3.6372,  0.1182, -1.8652,  0.0000,  0.0000]]])>>># using different paddings for different sides>>>m=nn.ZeroPad1d((3,1))>>>m(input)tensor([[[ 0.0000,  0.0000,  0.0000,  1.6616,  1.4523, -1.1255,  0.0000],[ 0.0000,  0.0000,  0.0000, -3.6372,  0.1182, -1.8652,  0.0000]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html#zeropad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d'),\n",
       " Document(metadata={}, page_content='ZeroPad2d [LINK_1]  class torch.nn.ZeroPad2d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with zero.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same'),\n",
       " Document(metadata={}, page_content='padding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out},'),\n",
       " Document(metadata={}, page_content=',  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ZeroPad2d(2)>>>input=torch.randn(1,1,3,3)>>>inputtensor([[[[-0.1678, -0.4418,  1.9466],[ 0.9604, -0.4219, -0.5241],[-0.9162, -0.5436, -0.6446]]]])>>>m(input)tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],[ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,'),\n",
       " Document(metadata={}, page_content='0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])>>># using different paddings for different sides>>>m=nn.ZeroPad2d((1,1,2,0))>>>m(input)tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],[ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],[ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],[ 0.0000, -0.9162, -0.5436,'),\n",
       " Document(metadata={}, page_content='-0.5241,  0.0000],[ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html#zeropad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d'),\n",
       " Document(metadata={}, page_content='ZeroPad3d [LINK_1]  class torch.nn.ZeroPad3d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with zero.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in},'),\n",
       " Document(metadata={}, page_content='n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i'),\n",
       " Document(metadata={}, page_content='H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ZeroPad3d(3)>>>input=torch.randn(16,3,10,20,30)>>>output=m(input)>>># using different paddings for different'),\n",
       " Document(metadata={}, page_content='using different paddings for different sides>>>m=nn.ZeroPad3d((3,3,6,6,0,1))>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html#zeropad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d'),\n",
       " Document(metadata={}, page_content='ConstantPad1d [LINK_1]  class torch.nn.ConstantPad1d( padding , value ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with a constant value.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in both boundaries. If a 2- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ConstantPad1d(2,3.5)>>>input=torch.randn(1,2,4)>>>inputtensor([[[-1.0491, -0.7152, -0.0749,  0.8530],[-1.3287,  1.8966,  0.1466, -0.2771]]])>>>m(input)tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,3.5000],[ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,3.5000]]])>>>m=nn.ConstantPad1d(2,3.5)>>>input=torch.randn(1,2,3)>>>inputtensor([[[ 1.6616,  1.4523, -1.1255],[-3.6372,  0.1182, -1.8652]]])>>>m(input)tensor([[[ 3.5000,  3.5000,'),\n",
       " Document(metadata={}, page_content='3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],[ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])>>># using different paddings for different sides>>>m=nn.ConstantPad1d((3,1),3.5)>>>m(input)tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],[ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html#constantpad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d'),\n",
       " Document(metadata={}, page_content='ConstantPad2d [LINK_1]  class torch.nn.ConstantPad2d( padding , value ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with a constant value.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same'),\n",
       " Document(metadata={}, page_content='padding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out},'),\n",
       " Document(metadata={}, page_content=',  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.ConstantPad2d(2,3.5)>>>input=torch.randn(1,2,2)>>>inputtensor([[[ 1.6585,  0.4320],[-0.8701, -0.4649]]])>>>m(input)tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],[ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])>>># using different'),\n",
       " Document(metadata={}, page_content='3.5000,  3.5000,  3.5000]]])>>># using different paddings for different sides>>>m=nn.ConstantPad2d((3,0,2,1),3.5)>>>m(input)tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],[ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],[ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html#constantpad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d'),\n",
       " Document(metadata={}, page_content='ConstantPad3d [LINK_1]  class torch.nn.ConstantPad3d( padding , value ) [source]  [source]  [LINK_2]  Pads the input tensor boundaries with a constant value.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in},'),\n",
       " Document(metadata={}, page_content='n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b), where  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i'),\n",
       " Document(metadata={}, page_content='H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:  >>>m=nn.ConstantPad3d(3,3.5)>>>input=torch.randn(16,3,10,20,30)>>>output=m(input)>>># using different paddings for different'),\n",
       " Document(metadata={}, page_content='using different paddings for different sides>>>m=nn.ConstantPad3d((3,3,6,6,0,1),3.5)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html#constantpad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d'),\n",
       " Document(metadata={}, page_content='CircularPad1d [LINK_1]  class torch.nn.CircularPad1d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using circular padding of the input boundary.  Tensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is'),\n",
       " Document(metadata={}, page_content='applied then the ends of the tensor get removed.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 2- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right)  Shape:  Input:(  C  ,  W  i  n  )  (C, W_{in})(C,Win\\u200b)or(  N  ,  C  ,  W  i  n  )  (N, C, W_{in})(N,C,Win\\u200b).  Output:(  C  ,  W  o  u  t  )  (C, W_{out})(C,Wout\\u200b)or(  N  ,  C  ,  W  o  u  t  )  (N, C, W_{out})(N,C,Wout\\u200b), where  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.CircularPad1d(2)>>>input=torch.arange(8,dtype=torch.float).reshape(1,2,4)>>>inputtensor([[[0., 1., 2., 3.],[4., 5., 6., 7.]]])>>>m(input)tensor([[[2., 3., 0., 1., 2., 3., 0., 1.],[6., 7., 4., 5., 6., 7., 4., 5.]]])>>># using different paddings for different sides>>>m=nn.CircularPad1d((3,1))>>>m(input)tensor([[[1., 2., 3., 0., 1., 2., 3., 0.],[5., 6., 7., 4., 5., 6., 7., 4.]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html#circularpad1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad1d.html#torch.nn.CircularPad1d'),\n",
       " Document(metadata={}, page_content='CircularPad2d [LINK_1]  class torch.nn.CircularPad2d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using circular padding of the input boundary.  Tensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is'),\n",
       " Document(metadata={}, page_content='applied then the ends of the tensor get removed.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same'),\n",
       " Document(metadata={}, page_content='padding in all boundaries. If a 4- tuple , uses (padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom)  Shape:  Input:(  N  ,  C  ,  H  i  n  ,  W  i  n  )  (N, C, H_{in}, W_{in})(N,C,Hin\\u200b,Win\\u200b)or(  C  ,  H  i  n  ,  W  i  n  )  (C, H_{in}, W_{in})(C,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out},'),\n",
       " Document(metadata={}, page_content=',  H  o  u  t  ,  W  o  u  t  )  (N, C, H_{out}, W_{out})(N,C,Hout\\u200b,Wout\\u200b)or(  C  ,  H  o  u  t  ,  W  o  u  t  )  (C, H_{out}, W_{out})(C,Hout\\u200b,Wout\\u200b), where  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.CircularPad2d(2)>>>input=torch.arange(9,dtype=torch.float).reshape(1,1,3,3)>>>inputtensor([[[[0., 1., 2.],[3., 4., 5.],[6., 7., 8.]]]])>>>m(input)tensor([[[[4., 5., 3., 4., 5., 3., 4.],[7., 8., 6., 7., 8., 6., 7.],[1., 2., 0., 1., 2., 0., 1.],[4., 5., 3., 4., 5., 3., 4.],[7., 8., 6., 7., 8., 6., 7.],[1., 2., 0., 1., 2., 0., 1.],[4., 5., 3., 4., 5., 3., 4.]]]])>>># using different paddings for different sides>>>m=nn.CircularPad2d((1,1,2,0))>>>m(input)tensor([[[[5., 3., 4.,'),\n",
       " Document(metadata={}, page_content='3., 4., 5., 3.],[8., 6., 7., 8., 6.],[2., 0., 1., 2., 0.],[5., 3., 4., 5., 3.],[8., 6., 7., 8., 6.]]]])'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html#circularpad2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad2d.html#torch.nn.CircularPad2d'),\n",
       " Document(metadata={}, page_content='CircularPad3d [LINK_1]  class torch.nn.CircularPad3d( padding ) [source]  [source]  [LINK_2]  Pads the input tensor using circular padding of the input boundary.  Tensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is'),\n",
       " Document(metadata={}, page_content='applied then the ends of the tensor get removed.  For N -dimensional padding, use torch.nn.functional.pad() .  Parameters  padding ( int  ,  tuple ) – the size of the padding. If is int , uses the same\\npadding in all boundaries. If a 6- tuple , uses'),\n",
       " Document(metadata={}, page_content='(padding_left  \\\\text{padding\\\\_left}padding_left,padding_right  \\\\text{padding\\\\_right}padding_right,padding_top  \\\\text{padding\\\\_top}padding_top,padding_bottom  \\\\text{padding\\\\_bottom}padding_bottom,padding_front  \\\\text{padding\\\\_front}padding_front,padding_back  \\\\text{padding\\\\_back}padding_back)  Shape:  Input:(  N  ,  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (N, C, D_{in}, H_{in}, W_{in})(N,C,Din\\u200b,Hin\\u200b,Win\\u200b)or(  C  ,  D  i  n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in},'),\n",
       " Document(metadata={}, page_content='n  ,  H  i  n  ,  W  i  n  )  (C, D_{in}, H_{in}, W_{in})(C,Din\\u200b,Hin\\u200b,Win\\u200b).  Output:(  N  ,  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\\u200b,Hout\\u200b,Wout\\u200b)or(  C  ,  D  o  u  t  ,  H  o  u  t  ,  W  o  u  t  )  (C, D_{out}, H_{out}, W_{out})(C,Dout\\u200b,Hout\\u200b,Wout\\u200b),'),\n",
       " Document(metadata={}, page_content='where  D  o  u  t  =  D  i  n  +  padding_front  +  padding_back  D_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}Dout\\u200b=Din\\u200b+padding_front+padding_back  H  o  u  t  =  H  i  n  +  padding_top  +  padding_bottom  H_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}Hout\\u200b=Hin\\u200b+padding_top+padding_bottom  W  o  u  t  =  W  i  n  +  padding_left  +  padding_right  W_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}Wout\\u200b=Win\\u200b+padding_left+padding_right  Examples:'),\n",
       " Document(metadata={}, page_content='Examples:  >>>m=nn.CircularPad3d(3)>>>input=torch.randn(16,3,8,320,480)>>>output=m(input)>>># using different paddings for different sides>>>m=nn.CircularPad3d((3,3,6,6,1,1))>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html#circularpad3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CircularPad3d.html#torch.nn.CircularPad3d'),\n",
       " Document(metadata={}, page_content='ELU [LINK_1]  class torch.nn.ELU( alpha=1.0 , inplace=False ) [source]  [source]  [LINK_2]  Applies the Exponential Linear Unit (ELU) function, element-wise.  Method described in the paper: Fast and Accurate Deep Network Learning by Exponential Linear\\nUnits (ELUs) .  ELU is defined as:  ELU  (  x  )  =  {  x  ,  if  x  >  0  α  ∗  (  exp  \\u2061  (  x  )  −  1  )  ,  if  x  ≤  0  \\\\text{ELU}(x) = \\\\begin{cases}\\nx, & \\\\text{ if } x > 0\\\\\\\\\\n\\\\alpha * (\\\\exp(x) - 1), & \\\\text{ if } x \\\\leq 0'),\n",
       " Document(metadata={}, page_content='\\\\alpha * (\\\\exp(x) - 1), & \\\\text{ if } x \\\\leq 0\\n\\\\end{cases}ELU(x)={x,α∗(exp(x)−1),\\u200bifx>0ifx≤0\\u200b  Parameters  alpha ( float ) – theα  \\\\alphaαvalue for the ELU formulation. Default: 1.0  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.ELU()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#elu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU'),\n",
       " Document(metadata={}, page_content='Hardshrink [LINK_1]  class torch.nn.Hardshrink( lambd=0.5 ) [source]  [source]  [LINK_2]  Applies the Hard Shrinkage (Hardshrink) function element-wise.  Hardshrink is defined as:  HardShrink  (  x  )  =  {  x  ,  if  x  >  λ  x  ,  if  x  <  −  λ  0  ,  otherwise  \\\\text{HardShrink}(x) =\\n\\\\begin{cases}\\nx, & \\\\text{ if } x > \\\\lambda \\\\\\\\\\nx, & \\\\text{ if } x < -\\\\lambda \\\\\\\\\\n0, & \\\\text{ otherwise }'),\n",
       " Document(metadata={}, page_content='0, & \\\\text{ otherwise }\\n\\\\end{cases}HardShrink(x)=⎩⎨⎧\\u200bx,x,0,\\u200bifx>λifx<−λotherwise\\u200b  Parameters  lambd ( float ) – theλ  \\\\lambdaλvalue for the Hardshrink formulation. Default: 0.5  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Hardshrink()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html#hardshrink\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink'),\n",
       " Document(metadata={}, page_content='Hardsigmoid [LINK_1]  class torch.nn.Hardsigmoid( inplace=False ) [source]  [source]  [LINK_2]  Applies the Hardsigmoid function element-wise.  Hardsigmoid is defined as:  Hardsigmoid  (  x  )  =  {  0  if  x  ≤  −  3  ,  1  if  x  ≥  +  3  ,  x  /  6  +  1  /  2  otherwise  \\\\text{Hardsigmoid}(x) = \\\\begin{cases}\\n    0 & \\\\text{if~} x \\\\le -3, \\\\\\\\\\n    1 & \\\\text{if~} x \\\\ge +3, \\\\\\\\\\n    x / 6 + 1 / 2 & \\\\text{otherwise}'),\n",
       " Document(metadata={}, page_content='x / 6 + 1 / 2 & \\\\text{otherwise}\\n\\\\end{cases}Hardsigmoid(x)=⎩⎨⎧\\u200b01x/6+1/2\\u200bifx≤−3,ifx≥+3,otherwise\\u200b  Parameters  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Hardsigmoid()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html#hardsigmoid\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid'),\n",
       " Document(metadata={}, page_content='Hardtanh [LINK_1]  class torch.nn.Hardtanh( min_val=-1.0 , max_val=1.0 , inplace=False , min_value=None , max_value=None ) [source]  [source]  [LINK_2]  Applies the HardTanh function element-wise.  HardTanh is defined as:  HardTanh  (  x  )  =  {  max_val  if  x  >  max_val  min_val  if  x  <  min_val  x  otherwise  \\\\text{HardTanh}(x) = \\\\begin{cases}\\n    \\\\text{max\\\\_val} & \\\\text{ if } x > \\\\text{ max\\\\_val } \\\\\\\\\\n    \\\\text{min\\\\_val} & \\\\text{ if } x < \\\\text{ min\\\\_val } \\\\\\\\\\n    x & \\\\text{ otherwise } \\\\\\\\'),\n",
       " Document(metadata={}, page_content='\\\\end{cases}HardTanh(x)=⎩⎨⎧\\u200bmax_valmin_valx\\u200bifx>max_valifx<min_valotherwise\\u200b  Parameters  min_val ( float ) – minimum value of the linear region range. Default: -1  max_val ( float ) – maximum value of the linear region range. Default: 1  inplace ( bool ) – can optionally do the operation in-place. Default: False  Keyword arguments min_value and max_value have been deprecated in favor of min_val and max_val .  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗'),\n",
       " Document(metadata={}, page_content='*∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Hardtanh(-2,2)>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html#hardtanh\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh'),\n",
       " Document(metadata={}, page_content='Hardswish [LINK_1]  class torch.nn.Hardswish( inplace=False ) [source]  [source]  [LINK_2]  Applies the Hardswish function, element-wise.  Method described in the paper: Searching for MobileNetV3 .  Hardswish is defined as:  Hardswish  (  x  )  =  {  0  if  x  ≤  −  3  ,  x  if  x  ≥  +  3  ,  x  ⋅  (  x  +  3  )  /  6  otherwise  \\\\text{Hardswish}(x) = \\\\begin{cases}\\n    0 & \\\\text{if~} x \\\\le -3, \\\\\\\\\\n    x & \\\\text{if~} x \\\\ge +3, \\\\\\\\\\n    x \\\\cdot (x + 3) /6 & \\\\text{otherwise}'),\n",
       " Document(metadata={}, page_content='x \\\\cdot (x + 3) /6 & \\\\text{otherwise}\\n\\\\end{cases}Hardswish(x)=⎩⎨⎧\\u200b0xx⋅(x+3)/6\\u200bifx≤−3,ifx≥+3,otherwise\\u200b  Parameters  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Hardswish()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html#hardswish\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html#torch.nn.Hardswish'),\n",
       " Document(metadata={}, page_content='LeakyReLU [LINK_1]  class torch.nn.LeakyReLU( negative_slope=0.01 , inplace=False ) [source]  [source]  [LINK_2]  Applies the LeakyReLU function element-wise.  LeakyReLU  (  x  )  =  max  \\u2061  (  0  ,  x  )  +  negative_slope  ∗  min  \\u2061  (  0  ,  x  )  \\\\text{LeakyReLU}(x) = \\\\max(0, x) + \\\\text{negative\\\\_slope} * \\\\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)  or  LeakyReLU  (  x  )  =  {  x  ,  if  x  ≥  0  negative_slope  ×  x  ,  otherwise  \\\\text{LeakyReLU}(x) =\\n\\\\begin{cases}'),\n",
       " Document(metadata={}, page_content='\\\\begin{cases}\\nx, & \\\\text{ if } x \\\\geq 0 \\\\\\\\\\n\\\\text{negative\\\\_slope} \\\\times x, & \\\\text{ otherwise }\\n\\\\end{cases}LeakyReLU(x)={x,negative_slope×x,\\u200bifx≥0otherwise\\u200b  Parameters  negative_slope ( float ) – Controls the angle of the negative slope (which is used for\\nnegative input values). Default: 1e-2  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗)where * means, any number of additional'),\n",
       " Document(metadata={}, page_content='dimensions  Output:(  ∗  )  (*)(∗), same shape as the input  Examples:  >>>m=nn.LeakyReLU(0.1)>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#leakyrelu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU'),\n",
       " Document(metadata={}, page_content='LogSigmoid [LINK_1]  class torch.nn.LogSigmoid( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the Logsigmoid function element-wise.  LogSigmoid  (  x  )  =  log  \\u2061  (  1  1  +  exp  \\u2061  (  −  x  )  )  \\\\text{LogSigmoid}(x) = \\\\log\\\\left(\\\\frac{ 1 }{ 1 + \\\\exp(-x)}\\\\right)LogSigmoid(x)=log(1+exp(−x)1\\u200b)  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:'),\n",
       " Document(metadata={}, page_content=')  (*)(∗), same shape as the input.  Examples:  >>>m=nn.LogSigmoid()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#logsigmoid\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid'),\n",
       " Document(metadata={}, page_content='MultiheadAttention [LINK_1]  class torch.nn.MultiheadAttention( embed_dim , num_heads , dropout=0.0 , bias=True , add_bias_kv=False , add_zero_attn=False , kdim=None , vdim=None , batch_first=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Allows the model to jointly attend to information from different representation subspaces.  Note  See this tutorial for an in depth discussion of the performant building blocks PyTorch offers for building your own'),\n",
       " Document(metadata={}, page_content='transformer layers.  Method described in the paper: Attention Is All You Need .  Multi-Head Attention is defined as:  MultiHead  (  Q  ,  K  ,  V  )  =  Concat  (  head  1  ,  …  ,  head  h  )  W  O  \\\\text{MultiHead}(Q, K, V) = \\\\text{Concat}(\\\\text{head}_1,\\\\dots,\\\\text{head}_h)W^OMultiHead(Q,K,V)=Concat(head1\\u200b,…,headh\\u200b)WO  wherehead  i  =  Attention  (  Q  W  i  Q  ,  K  W  i  K  ,  V  W  i  V  )  \\\\text{head}_i = \\\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)headi\\u200b=Attention(QWiQ\\u200b,KWiK\\u200b,VWiV\\u200b).'),\n",
       " Document(metadata={}, page_content='VW_i^V)headi\\u200b=Attention(QWiQ\\u200b,KWiK\\u200b,VWiV\\u200b).  nn.MultiheadAttention will use the optimized implementations of scaled_dot_product_attention() when possible.  In addition to support for the new scaled_dot_product_attention() function, for speeding up Inference, MHA will use'),\n",
       " Document(metadata={}, page_content='fastpath inference with support for Nested Tensors, iff:  self attention is being computed (i.e., query , key , and value are the same tensor).  inputs are batched (3D) with batch_first==True  Either autograd is disabled (using torch.inference_mode or torch.no_grad ) or no tensor argument requires_grad  training is disabled (using .eval() )  add_bias_kv is False  add_zero_attn is False  kdim and vdim are equal to embed_dim  if a NestedTensor is passed, neither key_padding_mask nor attn_mask is'),\n",
       " Document(metadata={}, page_content='passed, neither key_padding_mask nor attn_mask is passed  autocast is disabled  If the optimized inference fastpath implementation is in use, a NestedTensor can be passed for query / key / value to represent padding more efficiently than using a'),\n",
       " Document(metadata={}, page_content='padding mask. In this case, a NestedTensor will be returned, and an additional speedup proportional to the fraction of the input\\nthat is padding can be expected.  Parameters  embed_dim – Total dimension of the model.  num_heads – Number of parallel attention heads. Note that embed_dim will be split'),\n",
       " Document(metadata={}, page_content='across num_heads (i.e. each head will have dimension embed_dim//num_heads ).  dropout – Dropout probability on attn_output_weights . Default: 0.0 (no dropout).  bias – If specified, adds bias to input / output projection layers. Default: True .  add_bias_kv – If specified, adds bias to the key and value sequences at dim=0. Default: False .  add_zero_attn – If specified, adds a new batch of zeros to the key and value sequences at dim=1.'),\n",
       " Document(metadata={}, page_content='Default: False .  kdim – Total number of features for keys. Default: None (uses kdim=embed_dim ).  vdim – Total number of features for values. Default: None (uses vdim=embed_dim ).  batch_first – If True , then the input and output tensors are provided'),\n",
       " Document(metadata={}, page_content='as (batch, seq, feature). Default: False (seq, batch, feature).  Examples:  >>>multihead_attn=nn.MultiheadAttention(embed_dim,num_heads)>>>attn_output,attn_output_weights=multihead_attn(query,key,value)  forward( query , key , value , key_padding_mask=None , need_weights=True , attn_mask=None , average_attn_weights=True , is_causal=False ) [source]  [source]  [LINK_3]  Compute attention outputs using query, key, and value embeddings.  Supports optional parameters for padding, masks and'),\n",
       " Document(metadata={}, page_content='optional parameters for padding, masks and attention weights.  Parameters  query ( Tensor ) – Query embeddings of shape(  L  ,  E  q  )  (L, E_q)(L,Eq\\u200b)for unbatched input,(  L  ,  N  ,  E  q  )  (L, N, E_q)(L,N,Eq\\u200b)when batch_first=False or(  N  ,  L  ,  E  q  )  (N, L, E_q)(N,L,Eq\\u200b)when batch_first=True , whereL  LLis the target sequence length,N  NNis the batch size, andE  q  E_qEq\\u200bis the query embedding dimension embed_dim .'),\n",
       " Document(metadata={}, page_content='Queries are compared against key-value pairs to produce the output.\\nSee “Attention Is All You Need” for more details.  key ( Tensor ) – Key embeddings of shape(  S  ,  E  k  )  (S, E_k)(S,Ek\\u200b)for unbatched input,(  S  ,  N  ,  E  k  )  (S, N, E_k)(S,N,Ek\\u200b)when batch_first=False or(  N  ,  S  ,  E  k  )  (N, S, E_k)(N,S,Ek\\u200b)when batch_first=True , whereS  SSis the source sequence length,N  NNis the batch size, andE  k  E_kEk\\u200bis the key embedding dimension kdim .'),\n",
       " Document(metadata={}, page_content='See “Attention Is All You Need” for more details.  value ( Tensor ) – Value embeddings of shape(  S  ,  E  v  )  (S, E_v)(S,Ev\\u200b)for unbatched input,(  S  ,  N  ,  E  v  )  (S, N, E_v)(S,N,Ev\\u200b)when batch_first=False or(  N  ,  S  ,  E  v  )  (N, S, E_v)(N,S,Ev\\u200b)when batch_first=True , whereS  SSis the source\\nsequence length,N  NNis the batch size, andE  v  E_vEv\\u200bis the value embedding dimension vdim .'),\n",
       " Document(metadata={}, page_content='See “Attention Is All You Need” for more details.  key_padding_mask ( Optional  [  Tensor  ] ) – If specified, a mask of shape(  N  ,  S  )  (N, S)(N,S)indicating which elements within key to ignore for the purpose of attention (i.e. treat as “padding”). For unbatched query , shape should be(  S  )  (S)(S).\\nBinary and float masks are supported.\\nFor a binary mask, a True value indicates that the corresponding key value will be ignored for'),\n",
       " Document(metadata={}, page_content='the purpose of attention. For a float mask, it will be directly added to the corresponding key value.  need_weights ( bool ) – If specified, returns attn_output_weights in addition to attn_outputs .\\nSet need_weights=False to use the optimized scaled_dot_product_attention and achieve the best performance for MHA.'),\n",
       " Document(metadata={}, page_content='Default: True .  attn_mask ( Optional  [  Tensor  ] ) – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape(  L  ,  S  )  (L, S)(L,S)or(  N  ⋅  num_heads  ,  L  ,  S  )  (N\\\\cdot\\\\text{num\\\\_heads}, L, S)(N⋅num_heads,L,S), whereN  NNis the batch size,L  LLis the target sequence length, andS  SSis the source sequence length. A 2D mask will be\\nbroadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.'),\n",
       " Document(metadata={}, page_content='Binary and float masks are supported. For a binary mask, a True value indicates that the\\ncorresponding position is not allowed to attend. For a float mask, the mask values will be added to\\nthe attention weight.\\nIf both attn_mask and key_padding_mask are supplied, their types should match.  average_attn_weights ( bool ) – If true, indicates that the returned attn_weights should be averaged across\\nheads. Otherwise, attn_weights are provided separately per head. Note that this flag only has an'),\n",
       " Document(metadata={}, page_content='effect when need_weights=True . Default: True (i.e. average weights across heads)  is_causal ( bool ) – If specified, applies a causal mask as attention mask.\\nDefault: False .\\nWarning: is_causal provides a hint that attn_mask is the\\ncausal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward'),\n",
       " Document(metadata={}, page_content='compatibility.  Return type  Tuple [ Tensor , Optional [ Tensor ]]  Outputs:  attn_output - Attention outputs of shape(  L  ,  E  )  (L, E)(L,E)when input is unbatched,(  L  ,  N  ,  E  )  (L, N, E)(L,N,E)when batch_first=False or(  N  ,  L  ,  E  )  (N, L, E)(N,L,E)when batch_first=True ,\\nwhereL  LLis the target sequence length,N  NNis the batch size, andE  EEis the\\nembedding dimension embed_dim .  attn_output_weights - Only returned when need_weights=True . If average_attn_weights=True ,'),\n",
       " Document(metadata={}, page_content='returns attention weights averaged across heads of shape(  L  ,  S  )  (L, S)(L,S)when input is unbatched or(  N  ,  L  ,  S  )  (N, L, S)(N,L,S), whereN  NNis the batch size,L  LLis the target sequence length, andS  SSis the source sequence length. If average_attn_weights=False , returns attention weights per'),\n",
       " Document(metadata={}, page_content='head of shape(  num_heads  ,  L  ,  S  )  (\\\\text{num\\\\_heads}, L, S)(num_heads,L,S)when input is unbatched or(  N  ,  num_heads  ,  L  ,  S  )  (N, \\\\text{num\\\\_heads}, L, S)(N,num_heads,L,S).  Note  batch_first argument is ignored for unbatched inputs.  merge_masks( attn_mask , key_padding_mask , query ) [source]  [source]  [LINK_4]  Determine mask type and combine masks if necessary.  If only one mask is provided, that mask'),\n",
       " Document(metadata={}, page_content='and the corresponding mask type will be returned. If both masks are provided, they will be both\\nexpanded to shape (batch_size,num_heads,seq_len,seq_len) , combined with logical or and mask type 2 will be returned\\n:param attn_mask: attention mask of shape (seq_len,seq_len) , mask type 0\\n:param key_padding_mask: padding mask of shape (batch_size,seq_len) , mask type 1\\n:param query: query embeddings of shape (batch_size,seq_len,embed_dim)  Returns  merged mask'),\n",
       " Document(metadata={}, page_content='mask_type: merged mask type (0, 1, or 2)  Return type  merged_mask\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#multiheadattention\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward\\n [LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.merge_masks'),\n",
       " Document(metadata={}, page_content='PReLU [LINK_1]  class torch.nn.PReLU( num_parameters=1 , init=0.25 , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies the element-wise PReLU function.  PReLU  (  x  )  =  max  \\u2061  (  0  ,  x  )  +  a  ∗  min  \\u2061  (  0  ,  x  )  \\\\text{PReLU}(x) = \\\\max(0,x) + a * \\\\min(0,x)PReLU(x)=max(0,x)+a∗min(0,x)  or  PReLU  (  x  )  =  {  x  ,  if  x  ≥  0  a  x  ,  otherwise  \\\\text{PReLU}(x) =\\n\\\\begin{cases}\\nx, & \\\\text{ if } x \\\\ge 0 \\\\\\\\\\nax, & \\\\text{ otherwise }'),\n",
       " Document(metadata={}, page_content='ax, & \\\\text{ otherwise }\\n\\\\end{cases}PReLU(x)={x,ax,\\u200bifx≥0otherwise\\u200b  Herea  aais a learnable parameter. When called without arguments, nn.PReLU() uses a single\\nparametera  aaacross all input channels. If called with nn.PReLU(nChannels) ,\\na separatea  aais used for each input channel.  Note  weight decay should not be used when learninga  aafor good performance.  Note  Channel dim is the 2nd dim of input. When input has dims < 2, then there is'),\n",
       " Document(metadata={}, page_content='no channel dim and the number of channels = 1.  Parameters  num_parameters ( int ) – number ofa  aato learn.\\nAlthough it takes an int as input, there is only two values are legitimate:\\n1, or the number of channels at input. Default: 1  init ( float ) – the initial value ofa  aa. Default: 0.25  Shape:  Input:(  ∗  )  ( *)(∗)where * means, any number of additional'),\n",
       " Document(metadata={}, page_content='dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Variables  weight ( Tensor ) – the learnable weights of shape ( num_parameters ).  Examples:  >>>m=nn.PReLU()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#prelu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#torch.nn.PReLU'),\n",
       " Document(metadata={}, page_content='ReLU [LINK_1]  class torch.nn.ReLU( inplace=False ) [source]  [source]  [LINK_2]  Applies the rectified linear unit function element-wise.  ReLU  (  x  )  =  (  x  )  +  =  max  \\u2061  (  0  ,  x  )  \\\\text{ReLU}(x) = (x)^+ = \\\\max(0, x)ReLU(x)=(x)+=max(0,x)  Parameters  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:'),\n",
       " Document(metadata={}, page_content=')  (*)(∗), same shape as the input.  Examples:  >>>m=nn.ReLU()>>>input=torch.randn(2)>>>output=m(input)AnimplementationofCReLU-https://arxiv.org/abs/1603.05201>>>m=nn.ReLU()>>>input=torch.randn(2).unsqueeze(0)>>>output=torch.cat((m(input),m(-input)))'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU'),\n",
       " Document(metadata={}, page_content='ReLU6 [LINK_1]  class torch.nn.ReLU6( inplace=False ) [source]  [source]  [LINK_2]  Applies the ReLU6 function element-wise.  ReLU6  (  x  )  =  min  \\u2061  (  max  \\u2061  (  0  ,  x  )  ,  6  )  \\\\text{ReLU6}(x) = \\\\min(\\\\max(0,x), 6)ReLU6(x)=min(max(0,x),6)  Parameters  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:'),\n",
       " Document(metadata={}, page_content=')  (*)(∗), same shape as the input.  Examples:  >>>m=nn.ReLU6()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#relu6\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#torch.nn.ReLU6'),\n",
       " Document(metadata={}, page_content='RReLU [LINK_1]  class torch.nn.RReLU( lower=0.125 , upper=0.3333333333333333 , inplace=False ) [source]  [source]  [LINK_2]  Applies the randomized leaky rectified linear unit function, element-wise.  Method described in the paper: Empirical Evaluation of Rectified Activations in Convolutional Network .  The function is defined as:  RReLU  (  x  )  =  {  x  if  x  ≥  0  a  x  otherwise  \\\\text{RReLU}(x) =\\n\\\\begin{cases}\\n    x & \\\\text{if } x \\\\geq 0 \\\\\\\\\\n    ax & \\\\text{ otherwise }'),\n",
       " Document(metadata={}, page_content='ax & \\\\text{ otherwise }\\n\\\\end{cases}RReLU(x)={xax\\u200bifx≥0otherwise\\u200b  wherea  aais randomly sampled from uniform distributionU  (  lower  ,  upper  )  \\\\mathcal{U}(\\\\text{lower}, \\\\text{upper})U(lower,upper)during training while during'),\n",
       " Document(metadata={}, page_content='evaluationa  aais fixed witha  =  lower  +  upper  2  a = \\\\frac{\\\\text{lower} + \\\\text{upper}}{2}a=2lower+upper\\u200b.  Parameters  lower ( float ) – lower bound of the uniform distribution. Default:1  8  \\\\frac{1}{8}81\\u200b  upper ( float ) – upper bound of the uniform distribution. Default:1  3  \\\\frac{1}{3}31\\u200b  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the'),\n",
       " Document(metadata={}, page_content='Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.RReLU(0.1,0.3)>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#rrelu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#torch.nn.RReLU'),\n",
       " Document(metadata={}, page_content='SELU [LINK_1]  class torch.nn.SELU( inplace=False ) [source]  [source]  [LINK_2]  Applies the SELU function element-wise.  SELU  (  x  )  =  scale  ∗  (  max  \\u2061  (  0  ,  x  )  +  min  \\u2061  (  0  ,  α  ∗  (  exp  \\u2061  (  x  )  −  1  )  )  )  \\\\text{SELU}(x) = \\\\text{scale} * (\\\\max(0,x) + \\\\min(0, \\\\alpha * (\\\\exp(x) - 1)))SELU(x)=scale∗(max(0,x)+min(0,α∗(exp(x)−1)))  withα  =  1.6732632423543772848170429916717  \\\\alpha = 1.6732632423543772848170429916717α=1.6732632423543772848170429916717andscale  ='),\n",
       " Document(metadata={}, page_content=\"=  1.0507009873554804934193349852946  \\\\text{scale} = 1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.  Warning  When using kaiming_normal or kaiming_normal_ for initialisation, nonlinearity='linear' should be used instead of nonlinearity='selu' in order to get Self-Normalizing Neural Networks .\"),\n",
       " Document(metadata={}, page_content='See torch.nn.init.calculate_gain() for more information.  More details can be found in the paper Self-Normalizing Neural Networks .  Parameters  inplace ( bool  ,  optional ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.SELU()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#selu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#torch.nn.SELU'),\n",
       " Document(metadata={}, page_content='CELU [LINK_1]  class torch.nn.CELU( alpha=1.0 , inplace=False ) [source]  [source]  [LINK_2]  Applies the CELU function element-wise.  CELU  (  x  )  =  max  \\u2061  (  0  ,  x  )  +  min  \\u2061  (  0  ,  α  ∗  (  exp  \\u2061  (  x  /  α  )  −  1  )  )  \\\\text{CELU}(x) = \\\\max(0,x) + \\\\min(0, \\\\alpha * (\\\\exp(x/\\\\alpha) - 1))CELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1))  More details can be found in the paper Continuously Differentiable Exponential Linear Units .  Parameters  alpha ( float ) – theα  \\\\alphaαvalue for the'),\n",
       " Document(metadata={}, page_content='alpha ( float ) – theα  \\\\alphaαvalue for the CELU formulation. Default: 1.0  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.CELU()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#celu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU'),\n",
       " Document(metadata={}, page_content=\"GELU [LINK_1]  class torch.nn.GELU( approximate='none' ) [source]  [source]  [LINK_2]  Applies the Gaussian Error Linear Units function.  GELU  (  x  )  =  x  ∗  Φ  (  x  )  \\\\text{GELU}(x) = x * \\\\Phi(x)GELU(x)=x∗Φ(x)  whereΦ  (  x  )  \\\\Phi(x)Φ(x)is the Cumulative Distribution Function for Gaussian Distribution.  When the approximate argument is ‘tanh’, Gelu is estimated with:  GELU  (  x  )  =  0.5  ∗  x  ∗  (  1  +  Tanh  (  2  /  π  ∗  (  x  +  0.044715  ∗  x  3  )  )  )  \\\\text{GELU}(x) = 0.5\"),\n",
       " Document(metadata={}, page_content=\"0.044715  ∗  x  3  )  )  )  \\\\text{GELU}(x) = 0.5 * x * (1 + \\\\text{Tanh}(\\\\sqrt{2 / \\\\pi} * (x + 0.044715 * x^3)))GELU(x)=0.5∗x∗(1+Tanh(2/π\\u200b∗(x+0.044715∗x3)))  Parameters  approximate ( str  ,  optional ) – the gelu approximation algorithm to use: 'none' | 'tanh' . Default: 'none'  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.GELU()>>>input=torch.randn(2)>>>output=m(input)\"),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#gelu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU'),\n",
       " Document(metadata={}, page_content='Sigmoid [LINK_1]  class torch.nn.Sigmoid( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the Sigmoid function element-wise.  Sigmoid  (  x  )  =  σ  (  x  )  =  1  1  +  exp  \\u2061  (  −  x  )  \\\\text{Sigmoid}(x) = \\\\sigma(x) = \\\\frac{1}{1 + \\\\exp(-x)}Sigmoid(x)=σ(x)=1+exp(−x)1\\u200b  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Sigmoid()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#sigmoid\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid'),\n",
       " Document(metadata={}, page_content='SiLU [LINK_1]  class torch.nn.SiLU( inplace=False ) [source]  [source]  [LINK_2]  Applies the Sigmoid Linear Unit (SiLU) function, element-wise.  The SiLU function is also known as the swish function.  silu  (  x  )  =  x  ∗  σ  (  x  )  ,  where  σ  (  x  )  is\\xa0the\\xa0logistic\\xa0sigmoid.  \\\\text{silu}(x) = x * \\\\sigma(x), \\\\text{where } \\\\sigma(x) \\\\text{ is the logistic sigmoid.}silu(x)=x∗σ(x),whereσ(x)is\\xa0the\\xa0logistic\\xa0sigmoid.  Note  See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear'),\n",
       " Document(metadata={}, page_content='Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation'),\n",
       " Document(metadata={}, page_content='in Reinforcement Learning and Swish:\\na Self-Gated Activation Function where the SiLU was experimented with later.  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.SiLU()>>>input=torch.randn(2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);'),\n",
       " Document(metadata={}, page_content='var url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#silu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#torch.nn.SiLU'),\n",
       " Document(metadata={}, page_content='Mish [LINK_1]  class torch.nn.Mish( inplace=False ) [source]  [source]  [LINK_2]  Applies the Mish function, element-wise.  Mish: A Self Regularized Non-Monotonic Neural Activation Function.  Mish  (  x  )  =  x  ∗  Tanh  (  Softplus  (  x  )  )  \\\\text{Mish}(x) = x * \\\\text{Tanh}(\\\\text{Softplus}(x))Mish(x)=x∗Tanh(Softplus(x))  Note  See Mish: A Self Regularized Non-Monotonic Neural Activation Function  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )'),\n",
       " Document(metadata={}, page_content='any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Mish()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#mish\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#torch.nn.Mish'),\n",
       " Document(metadata={}, page_content='Softplus [LINK_1]  class torch.nn.Softplus( beta=1.0 , threshold=20.0 ) [source]  [source]  [LINK_2]  Applies the Softplus function element-wise.  Softplus  (  x  )  =  1  β  ∗  log  \\u2061  (  1  +  exp  \\u2061  (  β  ∗  x  )  )  \\\\text{Softplus}(x) = \\\\frac{1}{\\\\beta} * \\\\log(1 + \\\\exp(\\\\beta * x))Softplus(x)=β1\\u200b∗log(1+exp(β∗x))  SoftPlus is a smooth approximation to the ReLU function and can be used'),\n",
       " Document(metadata={}, page_content='to constrain the output of a machine to always be positive.  For numerical stability the implementation reverts to the linear function'),\n",
       " Document(metadata={}, page_content='wheni  n  p  u  t  ×  β  >  t  h  r  e  s  h  o  l  d  input \\\\times \\\\beta > thresholdinput×β>threshold.  Parameters  beta ( float ) – theβ  \\\\betaβvalue for the Softplus formulation. Default: 1  threshold ( float ) – values above this revert to a linear function. Default: 20  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Softplus()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#softplus\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#torch.nn.Softplus'),\n",
       " Document(metadata={}, page_content='Softshrink [LINK_1]  class torch.nn.Softshrink( lambd=0.5 ) [source]  [source]  [LINK_2]  Applies the soft shrinkage function element-wise.  SoftShrinkage  (  x  )  =  {  x  −  λ  ,  if  x  >  λ  x  +  λ  ,  if  x  <  −  λ  0  ,  otherwise  \\\\text{SoftShrinkage}(x) =\\n\\\\begin{cases}\\nx - \\\\lambda, & \\\\text{ if } x > \\\\lambda \\\\\\\\\\nx + \\\\lambda, & \\\\text{ if } x < -\\\\lambda \\\\\\\\\\n0, & \\\\text{ otherwise }'),\n",
       " Document(metadata={}, page_content='0, & \\\\text{ otherwise }\\n\\\\end{cases}SoftShrinkage(x)=⎩⎨⎧\\u200bx−λ,x+λ,0,\\u200bifx>λifx<−λotherwise\\u200b  Parameters  lambd ( float ) – theλ  \\\\lambdaλ(must be no less than zero) value for the Softshrink formulation. Default: 0.5  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Softshrink()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html#softshrink\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html#torch.nn.Softshrink'),\n",
       " Document(metadata={}, page_content='Softsign [LINK_1]  class torch.nn.Softsign( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the element-wise Softsign function.  SoftSign  (  x  )  =  x  1  +  ∣  x  ∣  \\\\text{SoftSign}(x) = \\\\frac{x}{ 1 + |x|}SoftSign(x)=1+∣x∣x\\u200b  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Softsign()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html#softsign\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html#torch.nn.Softsign'),\n",
       " Document(metadata={}, page_content='Tanh [LINK_1]  class torch.nn.Tanh( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the Hyperbolic Tangent (Tanh) function element-wise.  Tanh is defined as:  Tanh  (  x  )  =  tanh  \\u2061  (  x  )  =  exp  \\u2061  (  x  )  −  exp  \\u2061  (  −  x  )  exp  \\u2061  (  x  )  +  exp  \\u2061  (  −  x  )  \\\\text{Tanh}(x) = \\\\tanh(x) = \\\\frac{\\\\exp(x) - \\\\exp(-x)} {\\\\exp(x) + \\\\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(−x)exp(x)−exp(−x)\\u200b  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )'),\n",
       " Document(metadata={}, page_content='any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Tanh()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#tanh\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh'),\n",
       " Document(metadata={}, page_content='Tanhshrink [LINK_1]  class torch.nn.Tanhshrink( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies the element-wise Tanhshrink function.  Tanhshrink  (  x  )  =  x  −  tanh  \\u2061  (  x  )  \\\\text{Tanhshrink}(x) = x - \\\\tanh(x)Tanhshrink(x)=x−tanh(x)  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Tanhshrink()>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html#tanhshrink\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink'),\n",
       " Document(metadata={}, page_content='Threshold [LINK_1]  class torch.nn.Threshold( threshold , value , inplace=False ) [source]  [source]  [LINK_2]  Thresholds each element of the input Tensor.  Threshold is defined as:  y  =  {  x  ,  if  x  >  threshold  value  ,  otherwise  y =\\n\\\\begin{cases}\\nx, &\\\\text{ if } x > \\\\text{threshold} \\\\\\\\\\n\\\\text{value}, &\\\\text{ otherwise }'),\n",
       " Document(metadata={}, page_content='\\\\text{value}, &\\\\text{ otherwise }\\n\\\\end{cases}y={x,value,\\u200bifx>thresholdotherwise\\u200b  Parameters  threshold ( float ) – The value to threshold at  value ( float ) – The value to replace with  inplace ( bool ) – can optionally do the operation in-place. Default: False  Shape:  Input:(  ∗  )  (*)(∗), where∗  *∗means any number of dimensions.  Output:(  ∗  )  (*)(∗), same shape as the input.  Examples:  >>>m=nn.Threshold(0.1,20)>>>input=torch.randn(2)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#threshold\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#torch.nn.Threshold'),\n",
       " Document(metadata={}, page_content='GLU [LINK_1]  class torch.nn.GLU( dim=-1 ) [source]  [source]  [LINK_2]  Applies the gated linear unit function.  G  L  U  (  a  ,  b  )  =  a  ⊗  σ  (  b  )  {GLU}(a, b)= a \\\\otimes \\\\sigma(b)GLU(a,b)=a⊗σ(b)wherea  aais the first half\\nof the input matrices andb  bbis the second half.  Parameters  dim ( int ) – the dimension on which to split the input. Default: -1  Shape:  Input:(  ∗  1  ,  N  ,  ∗  2  )  (\\\\ast_1, N, \\\\ast_2)(∗1\\u200b,N,∗2\\u200b)where * means, any number of additional'),\n",
       " Document(metadata={}, page_content='dimensions  Output:(  ∗  1  ,  M  ,  ∗  2  )  (\\\\ast_1, M, \\\\ast_2)(∗1\\u200b,M,∗2\\u200b)whereM  =  N  /  2  M=N/2M=N/2  Examples:  >>>m=nn.GLU()>>>input=torch.randn(4,2)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GLU.html#glu\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GLU.html#torch.nn.GLU'),\n",
       " Document(metadata={}, page_content='Softmin [LINK_1]  class torch.nn.Softmin( dim=None ) [source]  [source]  [LINK_2]  Applies the Softmin function to an n-dimensional input Tensor.  Rescales them so that the elements of the n-dimensional output Tensor'),\n",
       " Document(metadata={}, page_content='lie in the range [0, 1] and sum to 1.  Softmin is defined as:  Softmin  (  x  i  )  =  exp  \\u2061  (  −  x  i  )  ∑  j  exp  \\u2061  (  −  x  j  )  \\\\text{Softmin}(x_{i}) = \\\\frac{\\\\exp(-x_i)}{\\\\sum_j \\\\exp(-x_j)}Softmin(xi\\u200b)=∑j\\u200bexp(−xj\\u200b)exp(−xi\\u200b)\\u200b  Shape:  Input:(  ∗  )  (*)(∗)where * means, any number of additional\\ndimensions  Output:(  ∗  )  (*)(∗), same shape as the input  Parameters  dim ( int ) – A dimension along which Softmin will be computed (so every slice'),\n",
       " Document(metadata={}, page_content='along dim will sum to 1).  Returns  a Tensor of the same dimension and shape as the input, with\\nvalues in the range [0, 1]  Return type  None  Examples:  >>>m=nn.Softmin(dim=1)>>>input=torch.randn(2,3)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html#softmin\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html#torch.nn.Softmin'),\n",
       " Document(metadata={}, page_content='Softmax [LINK_1]  class torch.nn.Softmax( dim=None ) [source]  [source]  [LINK_2]  Applies the Softmax function to an n-dimensional input Tensor.  Rescales them so that the elements of the n-dimensional output Tensor'),\n",
       " Document(metadata={}, page_content='lie in the range [0,1] and sum to 1.  Softmax is defined as:  Softmax  (  x  i  )  =  exp  \\u2061  (  x  i  )  ∑  j  exp  \\u2061  (  x  j  )  \\\\text{Softmax}(x_{i}) = \\\\frac{\\\\exp(x_i)}{\\\\sum_j \\\\exp(x_j)}Softmax(xi\\u200b)=∑j\\u200bexp(xj\\u200b)exp(xi\\u200b)\\u200b  When the input Tensor is a sparse tensor then the unspecified\\nvalues are treated as -inf .  Shape:  Input:(  ∗  )  (*)(∗)where * means, any number of additional'),\n",
       " Document(metadata={}, page_content='dimensions  Output:(  ∗  )  (*)(∗), same shape as the input  Returns  a Tensor of the same dimension and shape as the input with\\nvalues in the range [0, 1]  Parameters  dim ( int ) – A dimension along which Softmax will be computed (so every slice\\nalong dim will sum to 1).  Return type  None  Note  This module doesn’t work directly with NLLLoss,\\nwhich expects the Log to be computed between the Softmax and itself.'),\n",
       " Document(metadata={}, page_content='Use LogSoftmax instead (it’s faster and has better numerical properties).  Examples:  >>>m=nn.Softmax(dim=1)>>>input=torch.randn(2,3)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#softmax\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax'),\n",
       " Document(metadata={}, page_content='Softmax2d [LINK_1]  class torch.nn.Softmax2d( *args , **kwargs ) [source]  [source]  [LINK_2]  Applies SoftMax over features to each spatial location.  When given an image of ChannelsxHeightxWidth , it will'),\n",
       " Document(metadata={}, page_content='apply Softmax to each location(  C  h  a  n  n  e  l  s  ,  h  i  ,  w  j  )  (Channels, h_i, w_j)(Channels,hi\\u200b,wj\\u200b)  Shape:  Input:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W).  Output:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)or(  C  ,  H  ,  W  )  (C, H, W)(C,H,W)(same shape as input)  Returns  a Tensor of the same dimension and shape as the input with'),\n",
       " Document(metadata={}, page_content='values in the range [0, 1]  Return type  None  Examples:  >>>m=nn.Softmax2d()>>># you softmax over the 2nd dimension>>>input=torch.randn(2,3,12,13)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html#softmax2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d'),\n",
       " Document(metadata={}, page_content='LogSoftmax [LINK_1]  class torch.nn.LogSoftmax( dim=None ) [source]  [source]  [LINK_2]  Applies thelog  \\u2061  (  Softmax  (  x  )  )  \\\\log(\\\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor.  The LogSoftmax formulation can be simplified as:  LogSoftmax  (  x  i  )  =  log  \\u2061  (  exp  \\u2061  (  x  i  )  ∑  j  exp  \\u2061  (  x  j  )  )  \\\\text{LogSoftmax}(x_{i}) = \\\\log\\\\left(\\\\frac{\\\\exp(x_i) }{ \\\\sum_j \\\\exp(x_j)} \\\\right)LogSoftmax(xi\\u200b)=log(∑j\\u200bexp(xj\\u200b)exp(xi\\u200b)\\u200b)  Shape:  Input:(  ∗  )'),\n",
       " Document(metadata={}, page_content='Shape:  Input:(  ∗  )  (*)(∗)where * means, any number of additional'),\n",
       " Document(metadata={}, page_content='dimensions  Output:(  ∗  )  (*)(∗), same shape as the input  Parameters  dim ( int ) – A dimension along which LogSoftmax will be computed.  Returns  a Tensor of the same dimension and shape as the input with\\nvalues in the range [-inf, 0)  Return type  None  Examples:  >>>m=nn.LogSoftmax(dim=1)>>>input=torch.randn(2,3)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#logsoftmax\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax'),\n",
       " Document(metadata={}, page_content='AdaptiveLogSoftmaxWithLoss [LINK_1]  class torch.nn.AdaptiveLogSoftmaxWithLoss( in_features , n_classes , cutoffs , div_value=4.0 , head_bias=False , device=None , dtype=None ) [source]  [source]  [LINK_2]  Efficient softmax approximation.  As described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin,\\nMoustapha Cissé, David Grangier, and Hervé Jégou .  Adaptive softmax is an approximate strategy for training models with large'),\n",
       " Document(metadata={}, page_content='output spaces. It is most effective when the label distribution is highly\\nimbalanced, for example in natural language modelling, where the word\\nfrequency distribution approximately follows the Zipf’s law .  Adaptive softmax partitions the labels into several clusters, according to\\ntheir frequency. These clusters may contain different number of targets\\neach.\\nAdditionally, clusters containing less frequent labels assign lower'),\n",
       " Document(metadata={}, page_content='dimensional embeddings to those labels, which speeds up the computation.\\nFor each minibatch, only clusters for which at least one target is\\npresent are evaluated.  The idea is that the clusters which are accessed frequently\\n(like the first one, containing most frequent labels), should also be cheap\\nto compute – that is, contain a small number of assigned labels.  We highly recommend taking a look at the original paper for more details.  cutoffs should be an ordered Sequence of integers sorted'),\n",
       " Document(metadata={}, page_content='in the increasing order.\\nIt controls number of clusters and the partitioning of targets into\\nclusters. For example setting cutoffs=[10,100,1000] means that first 10 targets will be assigned\\nto the ‘head’ of the adaptive softmax, targets 11, 12, …, 100 will be\\nassigned to the first cluster, and targets 101, 102, …, 1000 will be\\nassigned to the second cluster, while targets 1001, 1002, …, n_classes - 1 will be assigned'),\n",
       " Document(metadata={}, page_content='to the last, third cluster.  div_value is used to compute the size of each additional cluster,\\nwhich is given as⌊  in_features  div_value  i  d  x  ⌋  \\\\left\\\\lfloor\\\\frac{\\\\texttt{in\\\\_features}}{\\\\texttt{div\\\\_value}^{idx}}\\\\right\\\\rfloor⌊div_valueidxin_features\\u200b⌋,\\nwherei  d  x  idxidxis the cluster index (with clusters\\nfor less frequent words having larger indices,\\nand indices starting from1  11).  head_bias if set to True, adds a bias term to the ‘head’ of the'),\n",
       " Document(metadata={}, page_content='adaptive softmax. See paper for details. Set to False in the official\\nimplementation.  Warning  Labels passed as inputs to this module should be sorted according to\\ntheir frequency. This means that the most frequent label should be\\nrepresented by the index 0 , and the least frequent'),\n",
       " Document(metadata={}, page_content='label should be represented by the index n_classes - 1 .  Note  This module returns a NamedTuple with output and loss fields. See further documentation for details.  Note  To compute log-probabilities for all classes, the log_prob method can be used.  Parameters  in_features ( int ) – Number of features in the input tensor  n_classes ( int ) – Number of classes in the dataset  cutoffs ( Sequence ) – Cutoffs used to assign targets to their buckets  div_value ( float  ,  optional ) – value used'),\n",
       " Document(metadata={}, page_content='div_value ( float  ,  optional ) – value used as an exponent to compute sizes'),\n",
       " Document(metadata={}, page_content='of the clusters. Default: 4.0  head_bias ( bool  ,  optional ) – If True , adds a bias term to the ‘head’ of the\\nadaptive softmax. Default: False  Returns  output is a Tensor of size N containing computed target\\nlog probabilities for each example  loss is a Scalar representing the computed negative'),\n",
       " Document(metadata={}, page_content='log likelihood loss  Return type  NamedTuple with output and loss fields  Shape:  input:(  N  ,  in_features  )  (N, \\\\texttt{in\\\\_features})(N,in_features)or(  in_features  )  (\\\\texttt{in\\\\_features})(in_features)  target:(  N  )  (N)(N)or(  )  ()()where each value satisfies0  <  =  target[i]  <  =  n_classes  0 <= \\\\texttt{target[i]} <= \\\\texttt{n\\\\_classes}0<=target[i]<=n_classes  output1:(  N  )  (N)(N)or(  )  ()()  output2: Scalar  log_prob( input ) [source]  [source]  [LINK_3]  Compute log'),\n",
       " Document(metadata={}, page_content='input ) [source]  [source]  [LINK_3]  Compute log probabilities for alln_classes  \\\\texttt{n\\\\_classes}n_classes.  Parameters  input ( Tensor ) – a minibatch of examples  Returns  log-probabilities of for each classc  ccin range0  <  =  c  <  =  n_classes  0 <= c <= \\\\texttt{n\\\\_classes}0<=c<=n_classes, wheren_classes  \\\\texttt{n\\\\_classes}n_classesis a'),\n",
       " Document(metadata={}, page_content='parameter passed to AdaptiveLogSoftmaxWithLoss constructor.  Return type  Tensor  Shape:  Input:(  N  ,  in_features  )  (N, \\\\texttt{in\\\\_features})(N,in_features)  Output:(  N  ,  n_classes  )  (N, \\\\texttt{n\\\\_classes})(N,n_classes)  predict( input ) [source]  [source]  [LINK_4]  Return the class with the highest probability for each example in the input minibatch.  This is equivalent to self.log_prob(input).argmax(dim=1) , but is more efficient in some cases.  Parameters  input ( Tensor ) – a'),\n",
       " Document(metadata={}, page_content='in some cases.  Parameters  input ( Tensor ) – a minibatch of examples  Returns  a class with the highest probability for each example  Return type  output ( Tensor )  Shape:  Input:(  N  ,  in_features  )  (N, \\\\texttt{in\\\\_features})(N,in_features)  Output:(  N  )  (N)(N)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#adaptivelogsoftmaxwithloss\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob'),\n",
       " Document(metadata={}, page_content='[LINK_4]  : https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.predict'),\n",
       " Document(metadata={}, page_content='BatchNorm1d [LINK_1]  class torch.nn.BatchNorm1d( num_features , eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Batch Normalization over a 2D or 3D input.  Method described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing'),\n",
       " Document(metadata={}, page_content='Internal Covariate Shift .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{\\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension over\\nthe mini-batches andγ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the number of features or channels of the input). By default, the\\nelements ofγ  \\\\gammaγare set to 1 and the elements ofβ  \\\\betaβare set to 0.'),\n",
       " Document(metadata={}, page_content='At train time in the forward pass, the variance is calculated via the biased estimator,\\nequivalent to torch.var(input,unbiased=False) . However, the value stored in the\\nmoving average of the variance is calculated via the unbiased  estimator, equivalent to torch.var(input,unbiased=True) .  Also by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during'),\n",
       " Document(metadata={}, page_content='evaluation. The running estimates are kept with a default momentum of 0.1.  If track_running_stats is set to False , this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the'),\n",
       " Document(metadata={}, page_content='update rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Because the Batch Normalization is done over the C dimension, computing statistics'),\n",
       " Document(metadata={}, page_content='on (N, L) slices, it’s common terminology to call this Temporal Batch Normalization.  Parameters  num_features ( int ) – number of features or channelsC  CCof the input  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average'),\n",
       " Document(metadata={}, page_content='(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.'),\n",
       " Document(metadata={}, page_content='in both training and eval modes. Default: True  Shape:  Input:(  N  ,  C  )  (N, C)(N,C)or(  N  ,  C  ,  L  )  (N, C, L)(N,C,L), whereN  NNis the batch size,C  CCis the number of features or channels, andL  LLis the sequence length  Output:(  N  ,  C  )  (N, C)(N,C)or(  N  ,  C  ,  L  )  (N, C, L)(N,C,L)(same shape as input)  Examples:  >>># With Learnable Parameters>>>m=nn.BatchNorm1d(100)>>># Without Learnable'),\n",
       " Document(metadata={}, page_content='Without Learnable Parameters>>>m=nn.BatchNorm1d(100,affine=False)>>>input=torch.randn(20,100)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d'),\n",
       " Document(metadata={}, page_content='BatchNorm2d [LINK_1]  class torch.nn.BatchNorm2d( num_features , eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Batch Normalization over a 4D input.  4D is a mini-batch of 2D inputs\\nwith additional channel dimension. Method described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing'),\n",
       " Document(metadata={}, page_content='Internal Covariate Shift .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension over\\nthe mini-batches andγ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the input size). By default, the elements ofγ  \\\\gammaγare set'),\n",
       " Document(metadata={}, page_content='to 1 and the elements ofβ  \\\\betaβare set to 0. At train time in the forward pass, the\\nstandard-deviation is calculated via the biased estimator, equivalent to torch.var(input,unbiased=False) . However, the value stored in the moving average of the\\nstandard-deviation is calculated via the unbiased  estimator, equivalent to torch.var(input,unbiased=True) .  Also by default, during training this layer keeps running estimates of its'),\n",
       " Document(metadata={}, page_content='computed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default momentum of 0.1.  If track_running_stats is set to False , this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the'),\n",
       " Document(metadata={}, page_content='update rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Because the Batch Normalization is done over the C dimension, computing statistics'),\n",
       " Document(metadata={}, page_content='on (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.  Parameters  num_features ( int ) –C  CCfrom an expected input of size(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average'),\n",
       " Document(metadata={}, page_content='(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.'),\n",
       " Document(metadata={}, page_content='in both training and eval modes. Default: True  Shape:  Input:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)  Output:(  N  ,  C  ,  H  ,  W  )  (N, C, H, W)(N,C,H,W)(same shape as input)  Examples:  >>># With Learnable Parameters>>>m=nn.BatchNorm2d(100)>>># Without Learnable Parameters>>>m=nn.BatchNorm2d(100,affine=False)>>>input=torch.randn(20,100,35,45)>>>output=m(input)\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .'),\n",
       " Document(metadata={}, page_content='var match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#batchnorm2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d'),\n",
       " Document(metadata={}, page_content='BatchNorm3d [LINK_1]  class torch.nn.BatchNorm3d( num_features , eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Batch Normalization over a 5D input.  5D is a mini-batch of 3D inputs with additional channel dimension as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing'),\n",
       " Document(metadata={}, page_content='Internal Covariate Shift .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension over\\nthe mini-batches andγ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors\\nof size C (where C is the input size). By default, the elements ofγ  \\\\gammaγare set'),\n",
       " Document(metadata={}, page_content='to 1 and the elements ofβ  \\\\betaβare set to 0. At train time in the forward pass, the\\nstandard-deviation is calculated via the biased estimator, equivalent to torch.var(input,unbiased=False) . However, the value stored in the moving average of the\\nstandard-deviation is calculated via the unbiased  estimator, equivalent to torch.var(input,unbiased=True) .  Also by default, during training this layer keeps running estimates of its'),\n",
       " Document(metadata={}, page_content='computed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default momentum of 0.1.  If track_running_stats is set to False , this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.  Note  This momentum argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the'),\n",
       " Document(metadata={}, page_content='update rule for running statistics here isx  ^  new  =  (  1  −  momentum  )  ×  x  ^  +  momentum  ×  x  t  \\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_tx^new\\u200b=(1−momentum)×x^+momentum×xt\\u200b,\\nwherex  ^  \\\\hat{x}x^is the estimated statistic andx  t  x_txt\\u200bis the\\nnew observed value.  Because the Batch Normalization is done over the C dimension, computing statistics\\non (N, D, H, W) slices, it’s common terminology to call this Volumetric Batch Normalization'),\n",
       " Document(metadata={}, page_content='or Spatio-temporal Batch Normalization.  Parameters  num_features ( int ) –C  CCfrom an expected input of size(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average'),\n",
       " Document(metadata={}, page_content='(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.'),\n",
       " Document(metadata={}, page_content='in both training and eval modes. Default: True  Shape:  Input:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)  Output:(  N  ,  C  ,  D  ,  H  ,  W  )  (N, C, D, H, W)(N,C,D,H,W)(same shape as input)  Examples:  >>># With Learnable Parameters>>>m=nn.BatchNorm3d(100)>>># Without Learnable Parameters>>>m=nn.BatchNorm3d(100,affine=False)>>>input=torch.randn(20,100,35,45,10)>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#batchnorm3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d'),\n",
       " Document(metadata={}, page_content='LazyBatchNorm1d [LINK_1]  class torch.nn.LazyBatchNorm1d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.BatchNorm1d module with lazy initialization.  Lazy initialization based on the num_features argument of the BatchNorm1d that is inferred\\nfrom the input.size(1) .'),\n",
       " Document(metadata={}, page_content='from the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average'),\n",
       " Document(metadata={}, page_content='(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.'),\n",
       " Document(metadata={}, page_content='in both training and eval modes. Default: True  cls_to_become [source]  [LINK_3]  alias of BatchNorm1d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#lazybatchnorm1d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d.cls_to_become'),\n",
       " Document(metadata={}, page_content='LazyBatchNorm2d [LINK_1]  class torch.nn.LazyBatchNorm2d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.BatchNorm2d module with lazy initialization.  Lazy initialization is done for the num_features argument of the BatchNorm2d that is inferred\\nfrom the input.size(1) .'),\n",
       " Document(metadata={}, page_content='from the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average'),\n",
       " Document(metadata={}, page_content='(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.'),\n",
       " Document(metadata={}, page_content='in both training and eval modes. Default: True  cls_to_become [source]  [LINK_3]  alias of BatchNorm2d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#lazybatchnorm2d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d.cls_to_become'),\n",
       " Document(metadata={}, page_content='LazyBatchNorm3d [LINK_1]  class torch.nn.LazyBatchNorm3d( eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  A torch.nn.BatchNorm3d module with lazy initialization.  Lazy initialization is done for the num_features argument of the BatchNorm3d that is inferred\\nfrom the input.size(1) .'),\n",
       " Document(metadata={}, page_content='from the input.size(1) .\\nThe attributes that will be lazily initialized are weight , bias , running_mean and running_var .  Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation\\non lazy modules and their limitations.  Parameters  eps ( float ) – a value added to the denominator for numerical stability.\\nDefault: 1e-5  momentum ( Optional  [  float  ] ) – the value used for the running_mean and running_var\\ncomputation. Can be set to None for cumulative moving average'),\n",
       " Document(metadata={}, page_content='(i.e. simple average). Default: 0.1  affine ( bool ) – a boolean value that when set to True , this module has\\nlearnable affine parameters. Default: True  track_running_stats ( bool ) – a boolean value that when set to True , this\\nmodule tracks the running mean and variance, and when set to False ,\\nthis module does not track such statistics, and initializes statistics\\nbuffers running_mean and running_var as None .\\nWhen these buffers are None , this module always uses batch statistics.'),\n",
       " Document(metadata={}, page_content='in both training and eval modes. Default: True  cls_to_become [source]  [LINK_3]  alias of BatchNorm3d\\n Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#lazybatchnorm3d\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d\\n [LINK_3]  : https://pytorch.org/docs/stable/generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d.cls_to_become'),\n",
       " Document(metadata={}, page_content='GroupNorm [LINK_1]  class torch.nn.GroupNorm( num_groups , num_channels , eps=1e-05 , affine=True , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Group Normalization over a mini-batch of inputs.  This layer implements the operation as described in'),\n",
       " Document(metadata={}, page_content='the paper Group Normalization  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The input channels are separated into num_groups groups, each containing num_channels/num_groups channels. num_channels must be divisible by num_groups . The mean and standard-deviation are calculated\\nseparately over the each group.γ  \\\\gammaγandβ  \\\\betaβare learnable'),\n",
       " Document(metadata={}, page_content='per-channel affine transform parameter vectors of size num_channels if affine is True .\\nThe variance is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False) .  This layer uses statistics computed from input data in both training and'),\n",
       " Document(metadata={}, page_content='evaluation modes.  Parameters  num_groups ( int ) – number of groups to separate the channels into  num_channels ( int ) – number of channels expected in input  eps ( float ) – a value added to the denominator for numerical stability. Default: 1e-5  affine ( bool ) – a boolean value that when set to True , this module\\nhas learnable per-channel affine parameters initialized to ones (for weights)'),\n",
       " Document(metadata={}, page_content='and zeros (for biases). Default: True .  Shape:  Input:(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)whereC  =  num_channels  C=\\\\text{num\\\\_channels}C=num_channels  Output:(  N  ,  C  ,  ∗  )  (N, C, *)(N,C,∗)(same shape as input)  Examples:  >>>input=torch.randn(20,6,10,10)>>># Separate 6 channels into 3 groups>>>m=nn.GroupNorm(3,6)>>># Separate 6 channels into 6 groups (equivalent with InstanceNorm)>>>m=nn.GroupNorm(6,6)>>># Put all 6 channels into a single group (equivalent with'),\n",
       " Document(metadata={}, page_content='6 channels into a single group (equivalent with LayerNorm)>>>m=nn.GroupNorm(1,6)>>># Activating the module>>>output=m(input)'),\n",
       " Document(metadata={}, page_content='Next  Previous  © Copyright 2024, PyTorch Contributors.  Built with Sphinx using a theme provided by Read the Docs .\\nvar match = window.location.href.match(/\\\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\\nvar url = window.location.href.lastIndexOf(match[match.length-1]);'),\n",
       " Document(metadata={}, page_content='if (url)\\n  {\\n    var div = \\'<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>\\'\\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML(\\'afterBegin\\', div)\\n  }'),\n",
       " Document(metadata={}, page_content='[LINK_1]  : https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html#groupnorm\\n [LINK_2]  : https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm'),\n",
       " Document(metadata={}, page_content='SyncBatchNorm [LINK_1]  class torch.nn.SyncBatchNorm( num_features , eps=1e-05 , momentum=0.1 , affine=True , track_running_stats=True , process_group=None , device=None , dtype=None ) [source]  [source]  [LINK_2]  Applies Batch Normalization over a N-Dimensional input.  The N-D input is a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing'),\n",
       " Document(metadata={}, page_content='Internal Covariate Shift .  y  =  x  −  E  [  x  ]  V  a  r  [  x  ]  +  ϵ  ∗  γ  +  β  y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\betay=Var[x]+ϵ\\u200bx−E[x]\\u200b∗γ+β  The mean and standard-deviation are calculated per-dimension over all\\nmini-batches of the same process groups.γ  \\\\gammaγandβ  \\\\betaβare learnable parameter vectors of size C (where C is the input size).'),\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kuber\\anaconda3\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\kuber\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kuber\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Generate embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Checkpoint for Vector Store\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "vectorstore.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved Vector Store\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embedding_model,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
