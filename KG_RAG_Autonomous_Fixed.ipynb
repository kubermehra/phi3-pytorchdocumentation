{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cb01ab",
   "metadata": {},
   "source": [
    "# KG-RAG: Knowledge Graph + Vector Store + LLM (Autonomous RAG)\n",
    "This notebook shows how to:\n",
    "- Extract entity triples from text using an LLM\n",
    "- Store them in a knowledge graph (Neo4j or in-memory)\n",
    "- Store document chunks in a vector database (FAISS)\n",
    "- Query both sources using a user prompt\n",
    "- Feed facts + documents to LLM for contextual answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6d7f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.69.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp313-cp313-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting neo4j\n",
      "  Downloading neo4j-5.28.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain)\n",
      "  Downloading langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.19-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.0-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\kuber\\anaconda3\\envs\\rag\\lib\\site-packages (from openai) (4.12.2)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\kuber\\anaconda3\\envs\\rag\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting pytz (from neo4j)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.16-cp313-cp313-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp313-cp313-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp313-cp313-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kuber\\anaconda3\\envs\\rag\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 14.9 MB/s eta 0:00:00\n",
      "Downloading openai-1.69.0-py3-none-any.whl (599 kB)\n",
      "   ---------------------------------------- 0.0/599.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 599.1/599.1 kB 15.7 MB/s eta 0:00:00\n",
      "Downloading faiss_cpu-1.10.0-cp313-cp313-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 7.3/13.7 MB 35.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.7 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 26.9 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.9.0-cp313-cp313-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 894.7/894.7 kB 15.1 MB/s eta 0:00:00\n",
      "Downloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp313-cp313-win_amd64.whl (204 kB)\n",
      "Downloading langchain_core-0.3.49-py3-none-any.whl (420 kB)\n",
      "Downloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.19-py3-none-any.whl (351 kB)\n",
      "Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.8/12.6 MB 33.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.6 MB 30.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 25.4 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.0-py3-none-any.whl (442 kB)\n",
      "Downloading pydantic_core-2.33.0-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 19.9 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 23.7 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Downloading greenlet-3.1.1-cp313-cp313-win_amd64.whl (299 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.16-cp313-cp313-win_amd64.whl (133 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading zstandard-0.23.0-cp313-cp313-win_amd64.whl (495 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: pytz, zstandard, urllib3, typing-inspection, tqdm, tenacity, sniffio, regex, PyYAML, pydantic-core, orjson, numpy, networkx, neo4j, jsonpointer, jiter, idna, h11, greenlet, distro, charset-normalizer, certifi, annotated-types, SQLAlchemy, requests, pydantic, jsonpatch, httpcore, faiss-cpu, anyio, tiktoken, requests-toolbelt, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.40 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 charset-normalizer-3.4.1 distro-1.9.0 faiss-cpu-1.10.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.9.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.21 langchain-core-0.3.49 langchain-text-splitters-0.3.7 langsmith-0.3.19 neo4j-5.28.1 networkx-3.4.2 numpy-2.2.4 openai-1.69.0 orjson-3.10.16 pydantic-2.11.0 pydantic-core-2.33.0 pytz-2025.2 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 tiktoken-0.9.0 tqdm-4.67.1 typing-inspection-0.4.0 urllib3-2.3.0 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (if running in Colab)\n",
    "!pip install langchain openai faiss-cpu tiktoken neo4j networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9cd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"phi3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae94a4",
   "metadata": {},
   "source": [
    "## Step 1: Simulate Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"title\": \"Spring Fling 10K Overview\",\n",
    "        \"url\": \"https://runclub.com/spring-fling\",\n",
    "        \"content\": \"The Spring Fling 10K is led by Alan and takes place in Prospect Park.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Alan's Profile\",\n",
    "        \"url\": \"https://runclub.com/alan\",\n",
    "        \"content\": \"Alan is a runner and site leader for Spring Fling 10K. See details here: https://runclub.com/spring-fling\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f20a61",
   "metadata": {},
   "source": [
    "## Step 2: Extract Triples from Documents Using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "triple_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "Extract factual triples from the following text.\n",
    "Return as a list of JSON dicts: [{'subject': ..., 'predicate': ..., 'object': ...}]\n",
    "\n",
    "Text: {text}\n",
    "Triples:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=triple_extraction_prompt)\n",
    "\n",
    "all_triples = []\n",
    "for doc in documents:\n",
    "    result = chain.run(text=doc['content'])\n",
    "    print(f\"Triples from {doc['title']}:\\n{result}\\n\")\n",
    "    all_triples.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f475a",
   "metadata": {},
   "source": [
    "## Step 3: (Optional) Store Triples in Graph Database (Neo4j or NetworkX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for Neo4j (optional)\n",
    "# graph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=\"your-password\")\n",
    "# graph.query(\"Cypher query here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d43c96",
   "metadata": {},
   "source": [
    "## Step 4: Vector Store (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b93b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_documents = [\n",
    "    Document(\n",
    "        page_content=doc['content'],\n",
    "        metadata={\"title\": doc['title'], \"url\": doc['url']}\n",
    "    ) for doc in documents\n",
    "]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(lc_documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "vectorstore.save_local(\"faiss_kg_rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479de84",
   "metadata": {},
   "source": [
    "## Step 5: Final Query - Combine KG and Vector Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where does Alan lead events?\"\n",
    "\n",
    "# Simulated KG result\n",
    "facts = [\n",
    "    \"Alan leads the Spring Fling 10K\",\n",
    "    \"Spring Fling 10K takes place in Prospect Park\"\n",
    "]\n",
    "\n",
    "# Vector DB search\n",
    "results = vectorstore.similarity_search(\"Spring Fling 10K\", k=2)\n",
    "\n",
    "print(\"\\nGraph Facts:\")\n",
    "for f in facts:\n",
    "    print(\"-\", f)\n",
    "\n",
    "print(\"\\nVector Search Results:\")\n",
    "for r in results:\n",
    "    print(f\"[{r.metadata['title']}]: {r.page_content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
